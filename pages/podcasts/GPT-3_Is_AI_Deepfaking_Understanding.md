Unknown 0:04
Welcome, everybody. We've got Joshua back back here today just about a very bright person. And I've interviewed in a number of times all the way back in 2012. And in Oxford, it's been eight years since we did our first interview. But then we did a few others. While Joshua was in Melbourne for edge Chi in 2000, late 2017, I think it was. So there's been some excitement around GPT. Three, and it's it's astonishing ability to produce human light text. So today, we'll be discussing some of the fundamental concepts around AI that haven't been solved yet. Like, what does it mean to understand? Does GPT three, understand? Where does symbol grounding fit in? What about causation and concept formation and category learning? So if you liked this podcast, please remember to subscribe and share if you enjoy this. So just a bit of a background, your show is an AI researchers worked and published about cognitive architectures, mental representation, emotion, social model, modeling, and multi agent systems. It's also got a PhD in cognitive science at the University of awesome Brooke, Germany, if I said that correctly. And you've also written a book called Seven Principles of synthetic intelligence. So I'll link to that in the description.

Joscha Bach 1:23
Thank you very much, considering the situation 2020 And all relatively well. We are in beautiful California, it's eternal spring, it's standing we cannot see anybody face to face while living a few steps from Stanford, which is a real waste. And I think my family finds it difficult to be insulated from other family and the kids cannot see friends but kids enjoy not being in school. And yeah, it's by and large for beginning of the end of the world. It's pretty good.

Unknown 2:01
Yes, we're in pretty severe lockdown in Melbourne as well. You're not allowed outside curfew at 8pm. And you can actually go outside as long as you're shopping, ordering essentials, maybe some exercising only allowed out like one hour a day. And certainly one person per household is allowed out, which is weird. So yeah, at each guy in Melbourne, we we discussed commercial success in AI. And that while there has been progress in AI, that hasn't necessarily been revolutionary, conceptual progress in AI, if you were to put your crystal ball on and were to make some informed speculations, what are the missing puzzle pieces? What's missing in AI? The moment

Joscha Bach 2:47
I think it's mostly a unified coherent model of reality, that's, in some sense, the big unanswered question. This means that, when we learn into our own mind, we don't learn into separate classifiers that we use to trigger separate policies. But instead, we learn everything into a unified function that is meant to explain all our sensory data, mostly by predicting them. And this unified model. That is what we call the universe and what we experience as reality that we are in it's basically a virtual reality that is constantly created by our mind. And it's updated to predict the next batch of sensory data. And the mental representations that have this property that their best predict the next batch of sensory data is what our mind labels is real. Right. And so we would need to have a model that is gives us the situatedness, this constant online property, and the ability to observe ourselves while we perform actions on the environment and why we make perceptions and why we optimize our models using attention in order to become sentient. So sentience, for me is this property that you are aware of your own existence in the universe and of your own policies of the fact that you are an agent that you pay attention. And so in some sense that you know what you're doing. And there is no AI system that I'm aware of that is aware of what it's doing.

Unknown 4:15
There's AI systems that look like they're aware of what they're doing, in a sense, like GTP, three people have got very excited about, but what's missing with GTP? Three

Joscha Bach 4:27
tippity three is able to behave as if in the sense that you can set it up in such a way using a prompt that you can ask it, what would you do or what texts would you produce about a situation in which super intelligent AI would be communicating with an outside environment under the following circumstances? And then it's able to give you that text, and it's pretty good, but it doesn't really update its model in the sense that it's really changing the relationship. We're about it has Learn already as a result of completely novel situations, for instance, and by itself, it doesn't care about anything. So the system itself is, in some sense, not too dissimilar to the autocomplete function on your phone. It's only the statistics that it has over words and language are much, much more elaborate and much more detailed. And the context that it uses to predict the next word is larger than the autocomplete in your phone, that's typically only going to use a couple of words to predict the next one. So it's a much much more elaborate context that it's going to use. And to get this to become like us, it would probably have to have an unbounded context, there are things in your life, that only makes sense over a very long range of a very long time span. So our context is not restricted to the last 2000 tokens or something like this, to predict the next action. But it's something that goes much more in the past and into the future. Also, we don't just predict from the past, we also predict from the future, which means we want to achieve certain states and avoid other states. And that also informs our context. And you can tip it sets up super data three in such a way that it can do this to some extent. But it's basically a trick that you are applying on to it. There's also the fact that GPT three is not multimodal, it's able to form embeddings over language and do that very well. And it also learns many irregularities in the language. So it's, for instance, able to perform simple arithmetic, legit arithmetic, but it's not as good as doing arithmetic as a machine learning system that has been trained to get things right. Instead, it's going to produce arithmetic that looks more like what it's seen so far in texts on the internet. Yeah, which means it doesn't do very large numbers. And it's mostly doing things that it's seen before. And

Unknown 7:00
it's confused by like dollar signs and commas, like for instance, yeah,

Joscha Bach 7:05
yeah. But you could also say it's not confused, it just is doesn't know that it's expected to say something about numbers, it's just continuing text, right? In a sense, it doesn't know what it's doing, you're just going to continue the most likely text. And so you would have to trick the introduction of the text to make correct numerical calculations more likely, in the presence of strange inputs, or strange things happening in the text. So for instance, you can give it a session where it's correcting inputs. And you start its outputs, for instance, by saying the following is a set of corrections over faulty inputs. And then you give it a handful of examples. And then you can continue giving it sample to faulty inputs over say, formulas that you want to enter correctly, and it's going to do its best to correct them.

Unknown 7:58
I mean, it's really interesting how convincing This is, like how convincing some of the text is, I've seen sort of writing by the GPT three modeling. And it looks pretty interesting. But I think people are selecting for the interesting output that it produces for one, that the other thing is, it does look as though it's a human writing, because it's sampling from what humans do that humans do symbol grounding, I guess you could say and have understanding and they produced produced text. And that's being used as the database the sample data for GPT. Three, to learn from, as far as I understand, it doesn't have simple grounding, or any form of understanding embedded in it. But I guess

Joscha Bach 8:42
I would say it grounds its symbols into a relational model of language. And the meaning is the relationships that all the tokens have with each other. And the grounding doesn't reach all the way into the representation in a three dimensional world is moving objects. So it would have difficulty to render what it's seeing, but it's grounded in stories. And the argument that you could basically the best explanation of all the stories for all the grounding eventually ends when you will continue training this will lead to inferences about the three dimensional world. And this is a big speculation, we don't know whether that happened, but we do know is that the learning is hasn't exhausted itself, yet we the learning curves do not yet bend. So you could probably train this up, there's at least one more magnitude of data and discover further regularities. What we also know is that you can take GPT three and train it for instance, on music. So for instance, you translate music into MIDI files, which is text, and then it's able to discover style and music and structure and music. And what you can also do is you can feed it images, only very short images, because the current context is only something like 2048 tokens, but it's 2004 The tokens can also be pixels and colors. Right? So for GBT three, this is text. And so it's not to be T three, this model that is using this on text image DPT, but it's not different algorithms, the same algorithm, just use on images. And this algorithm is able to learn the structure of images. So when you want to do your continuation you do instead of feeding text, and you feed it a few lines of pixels. And this thing is going to continue the few lines of pixels. For instance, if the few lines of excellence contain a cat image, it's going to give you a cat, it's going to be a different cat every time and every time is going to look like a real cat. So we now have a general algorithm that is able to also learn the structure of images. And there is no reason why an algorithm like this, except for resource constraints is not going to be able to do the same thing for video. Yeah. So once you are able to predict the next frame in video, over the past frames in video that you've seen. And you will now imagine that the video input comes from the sensors of a robot. And this robot is navigating the environment and you will make the navigation signals part of the input. And

Unknown 11:14
kind of strange like if GPT three generated a cat pet for you, that every time you saw it, it looked different. Like at first it was a tabby and that was a black and white. And then it ended up being like, you know, a purple cat for some reason. It seems to struggle with maintaining consistency over long periods. Yes,

Joscha Bach 11:31
but it's not because of the variance, there were variances introduced with some degree of deliberation. It's not that GBTC is by itself in deterministic, it is a parameter temperature. And if you set the temperature to zero, it's basically giving you the same result every time it's just going to come up as the most likely response. And this tends to be also the most boring response. And so the temperature introduces a little bit of noise in the higher levels of the prediction. So it gives you the less obvious things, if you set the temperature very high, then you'll often get very loopy results are gibberish. And so when you generate outputs, people try to find a temperature that is suitable for the task, which means if you try to say translate programming code from problem one problem language into another one, then you're going to set the temperature extremely low, and extra zero. And if you want to create an interesting page in a novel, then say you want to have some kind of thriller novel or some romantic novel and so on, you're going to set the temperature to some medium level where it's not going to go off the rails that easily, but it's going to be interesting to read. And there is not that much cherry picking that you need to do, which means it's more GBTC is able to come up with interesting stuff most of the time, it's just not going to the best stuff. And since you get sometimes a better version, just by hitting repeat, you just roll the dice a few times until you think oh my god, this is really good, I would have never come up with this, or only in the long amount of time. And so just for and tool for idea generation, it's quite powerful. And there are some people that use it for creative writing and find that it increases their productivity not by a factor of 10, but by a factor of two to three, which means you do have to edit. One person that I follow in federal liminal worms who use it to write romance novels visit describes it's like flat fill on a page. And sometimes it's going to introduce a few pixels that you don't want to have. So he accepts them and replace them by something else. And it's going to continue from the prompt they gave it, it's also able to do pretty decent business writing like you can for instance out of an email, you just give it a few bullet points, and it writes your the email around this. But you have to read carefully what it does, because everything that it doesn't know or doesn't have the most likely completion at ready, it's going to confabulate. So you always have to make sure that you know what you're doing. Also, when you write complex programming code visit, it can maybe introduce now help can help you with writing algorithms, especially boilerplate can generate regular expressions, which can be a pain in the neck if you do this by hand, or SQL queries and so on. But you still need to know what your tables are called in your actual database and balance make sense?

Unknown 14:29
Yeah, well, I mean, like, it's certainly generated a sort of a wow response to a lot of people because they can. The text generation is so convincing. And it has been able to produce interesting programming examples and all that sort of thing. That people are thinking that it's going to sort of spawn a singularity like you know, GTP N some GTP 21 in the future will gain sentience and catalyze an intelligence explosion. What are your thoughts?

Joscha Bach 15:00
We don't know. So we don't know what needs to be added to GPT three of whether the ideas that you're currently having to add are sufficient. At the moment, it seems to me that the million dollar question is, how can you make this algorithm more efficient? So it's able to learn faster with less data? How can you introduce a loss function that is not just going to go for a probability to look like the data source that you have. So at the moment, it's going to produce text that's been written by a human or looks like this and is indistinguishable from it, it doesn't try to give you the correct solution. Right, so it's going to try to find the correct solution in a situation where a human will try to find the correct solution. But if you try, if you ask it to solve a complicated integral, it's probably not going to be able to do so. And it's not because the resources of the system are insufficient, there are deep learning systems, very few are resources that have learned to solve quite complicated mathematical equations. But it's not what the learning algorithm has asked itself for when the system was trained. So by changing the loss function of this thing, you could shift it towards something that is less human like, and more super human. And, but the biggest thing that we would need to change is the length of the input context.

Unknown 16:24
Right? So needs to like do more with less. Yes,

Joscha Bach 16:30
it's basically to get an idea of what it's doing. I remember in the 90s, I was working in New Zealand, I was attending Waikato University, and took classes, this invitation about text compression. And I was bored in class. And he noticed and took me out of his class and put me into his lap and asked me to do stuff that was more interesting. And so the the task that he gave me was to identify grammatical structure and language without knowing what language I was looking at. So I compiled a large corpus of hundreds of millions of words, and I got the largest machine at the university, which had two gigabytes of RAM, which was amazing. And then I wrote a C code that would use that RAM as efficiently as I could. So I really split split up my input at the level of less than a bite to squeeze as much into memory as I could. And then I parsed everything and translated this into tokens. And throughout all the tokens are very rare. And then I did statistics over these tokens. And instead of just using engrams, which look at adjacent words, I looked at pairs of words that could be could have quite a distance from each other. And I would try to find out how well verse words predict each other pairwise. So in both directions if you have a pair of words in the same sentence, or paragraph. And then I would try to construct a tree like structure that would put the words in the order that allow you to make the best prediction over the text, is it the next best text compression. And it turns out that this tree that you'll find over the structure in the sentence is capturing semantic relationships. And it's capturing grammatical structure. And it's going to capture the different grammatical structure for different language. Okay, now, the next step is that you by doing these co occurrence statistics, you will split these different similarities or CO occurrences of words along different dimensions. And this allows you to create space in which you will embed the words so every word is going to have a position in certain space. And it turns out that the directions dimensions of that space correspond to semantic relationships. For instance, there is going to be a vector that you can construct between the words king and queen, that is very similar in distance and direction to the vector between the words man and woman. So basically, in this high dimensional space, there is going to be one direction that captures grammatical relate or semantic relationship that is, much pretty much gender. Right? So you can use this to make inferences. And when I did these things, I found that it was very hard computationally and for the algorithms that I was using to go higher than pairs of words. And I wanted to go in basically arbitrary levels of relationships, maybe fourth degree relationships, and so on. And I wanted to go beyond the context of a sentence. And this was very difficult because the algorithms that I implemented and tried, and I thought if I had a much, much larger machine and could work in parallel, maybe I could use a large context and basically test the relationships of everything was everything in that context, and do my statistics over there. And when I read about what the transformer was doing in GPT, three, this is pretty much the solution that they're looking for. Right? It's so they are looking for a particular way to find the relationships Between all the elements in your input data, and this relationship that you discover what you need to pay attention to, to map it to each other, that's what they call attention in the thing. So you basically have an attentional system that tracks how the semantic relationships between different tokens in the text layout, which means if you have a long text at the beginning of the text, you can have the word scientist, and then a little bit later in the text, you could have the word he or she, and then later on, you would have the name. And this attentional system is able to discover the relationship between these tokens and realize that they refer to the same concept. And so it's able to create structure in the text that you could not create before because the algorithms are just not able to make these statistics. In some sense, GBT three, this transformer algorithm is a tool that has been developed for making automatic translations between different languages. So it's going to discover the structures language and a similar embedding between the words, it does discover similar conceptual neighborhoods. And by mapping these spaces to each other, it's able to automatically discover which parts of of the language relate to the other. And, of course, it's easy. When you have related text and you know, this text is the translation of the other text, then it's as a good chance to discover which words correspond to which other words, right if you do this for a lot of text,

Joscha Bach 21:27
and it's not the way humans do it, it's basically brute forcing this problem, if you would try to do this with a human being, you would start out with a very small context, then you learn to read with start with very short sentences, or half sentences, even with a controlled small vocabulary. And only after we exhaustively are able to deal with this vocabulary and grammatical relationships, we extend the scope. And we do this step by step. And at some point, we basically move into a space where we understand the lay of the land. And we do not just understand the relationships between things that you have seen. But you're also able to predict this high confidence of relationships between things that you haven't seen yet, which is called Zero shot learning. And this zero short learning is something that GPT three is able to do under some circumstances. So it's often able to predict things that it hasn't seen before. So what we want to do now is to what people will, I think are working on frantically at the moment is how can we change this arbitrary 2000 for the eight, target chocolate context, and extend this into something that is much larger, that is unbounded, basically. So we start out with a small one, and we feed in more and make it as large as we want. So at the moment, when this thing reads a book, it's only parsing like three pages at once. And everything else, it only knows as the memory that it has learned into the relational structure of the general model, it doesn't keep this as context anymore.

Unknown 22:49
If it gets everything three pages earlier, or

Joscha Bach 22:52
it's forgetting that it has seen this as a context, it's only going to store this as a general statistics over language after it has learned them. And it's not doing online learning. So during the training process is going to gradually learn more more and more statistics into this. And these are the same statistics that you learn to your own mind in a way when you understand the statistics of your own language or have your own knowledge base, that you no longer recall where you learned that particular thing. It's also not important where you learned it. And it's probably not related to the current context, the current context is just your working memory. And the working memory of TPTs V is relatively short. And it's our working memory is probably shorter than the working memory of TPTs. For you. It's smaller, but it's smaller, but not shorter. Our working memory can integrate over a very long time, Spence basically can keep crucial elements of the beginning of a book that you read in your mind, and carry them forward.

Unknown 23:48
It's also something that salient concepts hopefully, yes.

Joscha Bach 23:51
And so we basically have better criteria for saliency. And it's an obvious extension that you can hack into GPMC. Right now on without changing the code base, you just by the way you use it, you can ask it to make excerpts of the text that you're currently looking at. So if you want to pretty sweet write a novel, for instance, right now, that's very difficult to do, because it's not going to remember what it wrote 10 pages ago, it's only going to remember the things that happened during the last few pages. So it's even the last paragraphs if they're long paragraphs, and as a result, it's going to veer off track at some point. And so a trick that you could be using is that you will sum up the general contents of the book. They're just in a few lines before you go into the details. So you basically make some kind of book abstract, and then the chapter abstract, and then you feed them into the current context. And these book abstracts and chapter abstracts can be generated by GBT suites able to produce abstracts that are quite plausible, right? Of course, the issue is that you cannot yet tell them in some kind of meta language, what you're really interested in. So we also want to have a different way to address the concepts in it just not by a stream of things, but by something where we can also predict from the middle or from the future backwards. And eventually this thing needs to become multimodal. So it's not just going to work on one modality. But it's going to learn at which point the modalities of vision, audio, proprioception, touch, and text are fusing at which level to the point to the same world.

Unknown 25:27
Interesting, you know, so many things you brought up there reminded me of stuff I used to do. When I was at uni with friends. I'd create a an email, and then I push it through some really bad sort of translation system. I think it might have been AltaVista or some Google Translate thing, like early days or probably before Google. And then do that over and over again. So English to Russian, Russian to Chinese, Chinese to Indonesian, and then back to English. And the results are sometimes quite hilarious.

Joscha Bach 25:59
Yeah. But at the moment, I am totally engrossed this, playing this GPT three, and my kids too. So I wrote a universal chatbot for you. basically specify in a couple lines, whom you want to talk to. And then it's going to do its very best to enable the conversation. So my son was able to talk to God and ask God about the lyrics of the next published album of its favorite rap artist. It was very explicit, so I'm not going to recite it here. But it was really cool and my daughter got it to sing. God got to tell her lyrics about unknown Harry Potter songs. Her I have found a way to talk to Johann Wolfgang Ghouta he speaks German. Me, Johan, could you please translate the sentence into German? The computer is the best model of reality. This was my only prompt these two lines that I just gave you. And then it started to generate the infamous Tsion stanovaya got me what does that mean? Good. Information is the new god. Me? Can you explain this? Continues? No, the informatio and Canon seven. Didn't give a very good and I'll talk to you later. It's even me. Only information can help us to overcome the overwhelming impression of emptiness. I see. Guys, what Ghouta in German, about the infamous Tsuna de la Agnon because that's on Isla Agon, and Wanda, but information has its own laws and its own miracles. So Johan, what do you think of my sentence? That's just any Kutuzova a buddy that is Toyland wouldn't me that's a good thing but the world is full of good things good. Contest us poor blame the economist Allah guten Zakon endeavored I found escaped Sofia for in pure Muslims have only given up showing me and that is the problem. We can experience all of the good things in the world. There are too many of them, we must limit ourselves to a few greater, and we have to win if you haven't the best and Zack and GoPhone admit incoherent. So that's the other under than forgetting me. And we found the best things and competed with them. So we forgot all about it. So we forgot about all the others. Because of your candidacy, Burstein. persons were very touched me so we can only survive what overwhelms us go to in German, the infamous Yan externado got me. Information is the new god, Johan, I think you're right. The world is full of good things, and we can experience them all. We have to choose a few choose a few. And the problem is that we chose the wrong ones. But then again, it's not our fault. We had no other choice. Greta was conveyed again to me What can we do about it? Greta? They can not equal to alpha and on me, you can only change the culture. Johanna agree you the most important thing in the world is culture. Because only with a good culture, can we enjoy all of the good things that are available to us? Good time that students begin and annoy Cotuit schaffen. Me, then that's begin to create a new culture. And then the next line is the conversation ends here and the book begins. And so this is basically a completely autogenerated after having cut in a couple of lines and the question, what does it mean for me? And it sort of took both sides in the conversation it did completely fluent translation from German into English. And it did not use completely modern German. So it basically took slightly antiquated German from Buddhist time. And it also captured the gist or the style of the way and the cultural references that somebody like Gouda might use. And that's quite typical for what GPT three is doing. And this particular conversation was not cherry picked. It was just the first thing that came out of it. And I'm sure if you would tweak this a little or if you would roll the dice quite a few times, you could say more particularly what you want to have, or maybe let it make even more sense. But is this a big thing for creative writing, for journalism, for technical writing, and so on?

Unknown 30:14
Absolutely. Yeah. Do you think that there's gradients of understanding? And if there is a gradient from very whatever primitive understanding in its most primitive form would be? Would you say that GPT three has it?

Joscha Bach 30:32
What does understanding mean, I think in some sense, it means that you map a domain onto something that you can already compute, which means you have some model that tells you about the causal dynamics of a system. And then you take another system and map it onto that, or decompose it into things that you can simulate. And there's also slightly different tools. So a simulation is a simulacrum. A simulacrum is something like an actor that you dress up in your own mind, for instance, as a machine or as a physical process. And then these actors produces the desired behavior, without necessarily implementing the required internal structure for doing so. Right. So imagine that, you want to understand how it is that the sun is shining, you can make a mental model that you basically take some kind of a universal actor, and dress it up as the sun and that tell it to shine and it sits there and shines. But you still don't know anything about its internal dynamics, it's just you can map the inputs and outputs of the sun and lead to produce a certain behavior. And to understand it goes a step beyond that, it basically means that you will have something like a mechanical explanation that you understand the causal dynamics at a deeper level. And then that's the difference between the understanding of a relationship where you understand one function, and the difference of a domain where you understand the entire parameter space of the function. So you understand what happens in the entire room, you basically click on the light, and you'll see the relationships between between the objects in the entire room, including some of the objects that you haven't seen so far. So you discover the hidden symmetries. And this is a crucial part of deep understanding.

Unknown 32:20
So do you think then, like, I mean, at the moment, GTP three doesn't look. So it's programmed to causation. It's not identifying causes. It's just, as far as I understand, it's brute forcing across big data sets, lots of correlations. Oh, am I wrong? Is there some sort of mechanism there for it to identify causes?

Joscha Bach 32:42
Now, the thing is that correlation and causation are strongly correlated, right. And so it's able to discover something that is often indistinguishable from causation. And what makes this discussion difficult is that not everybody agrees what we mean by causality. But often people take this as a given, especially when they want to criticize a system that they think is inferior, or that they don't like. So they assume that the words mean what they want them to mean. But many people just use a simulacrum of causality when they try to try to refer to it right. So we need a functional understanding of causality. And my own functional understanding of causality is that it's an artifact not of the world itself. But of the way that we describe the world in certain contexts, the universe might probably be best understood as some kind of unified state vector that progresses from state to state according to the laws of physics. Basically, physics is explaining the function or trying to find the function that correlates adjacent universe states. And at the level of this unified model, where everything is connected to everything in a way, you don't have causality. And causality only emerges when you separate the world into into things. Because we cannot measure the entire universe at once and cannot observe it all at once, there is no hope that you can model the universe as a unified state progression. So we Yes, so what we do is we take out different parts of the universe and treat them as separate objects. And the separate objects are all described by their own state space and their own transition function that moves them in that state space. And these objects might share interface, which means they share states, but one object can influence the states of another. And so the state progression is in one object can influence the state evolution and another object. And this is what we would call a causal interaction. Right? So for instance, your computer is a thing that is by itself pretty much deterministic and is following its own evolutionary role. It state evolution, and you can press a key and that might change everything that happens on the computer, right? To a certain degree, it's going to change the evolution of the computer. And as you Press the key very, very hard, it's going to still be your computer. If you press it too hard, it might break. But as long as it doesn't break, as long as it doesn't leave the state space that still lets it be a computer and your own model, it's going to change its evolution in a certain way. And if you manage to change the evolution of the computer by interacting visit, that is the causal interaction. So it's in some sense describes the progression of information through objects and traits, provided it changes relevant states in the objects. And now we could look at GPT three and ask ourselves, Is it doing that? And I think it does. So you can give it a context in which it's being asked to generalize over interactions between symbols. So for instance, you can ask it to give you the emotional valence of a paragraph. And you give it a handful of examples where you give it different paragraphs of text and tell it what emotion is dominant just in one word, and then you give it new paragraphs, and it's going to tell you the dominant emotion, it's works pretty well. Right? This is, in some sense, a causal relationship between the text and its output that it has learned to deduce from your previous inputs. It understands that the output is an object, it understands that the paragraph is an object, it understands that there is a relationship between them, it understands that the relationship is here, which means it has a functional model that is actually implemented in this and you can test this by new function that is being instantiated from the context, right? In some sense, TPT. Three is a compiler that takes the context to compile a certain model that is context dependent, to produce the new output. And it does compile this based on invariances that it has learned before. But it doesn't just learn this because it has seen this exact paragraph somewhere in its training data and the matching that it will do to this paragraph, it is able to discover this relationship between the paragraph and some kind of emotion. And so I did test this, for instance, by giving it as an example. See.

Unknown 37:21
But when you say it, it won't even have any understanding that the concepts or emotions or that they represent, they're not like, do they owe at the end the reference, in a sense,

Joscha Bach 37:32
it doesn't understand the concept of emotion in the sense as we do it. So it does is not able to experience any emotion. But it's if you can ask it. If there was an agent that had this kind of emotion, what would this agent likely do under these in these circumstances, then it's going to create your story that is quite plausible. And if you could use the context to describe the situation that you are in, or that itself is in or a robot is in that is controlled by the output of GPT. Three, then GPUs, we would be able to produce plots of the behavior for that robot. Right? So it's a very interesting thing, what's the difference between making a decision and predicting what decision an agent would make? If that agent is in your situation? Right, what does it need to be in your situation?

Unknown 38:20
What instead of standing agent would would make

Joscha Bach 38:23
right so it can make something that is basically a deep fake, of, of agentic behavior?

Unknown 38:32
So I mean, is it okay that we're developing AI that's basically creating deep fakes of everything, instead of genuinely understanding? Do we want an AI to genuinely understand that think of the the ethical scenario because sometimes it gets things wrong, because it's not really understanding exactly what what we want or what we mean or what our intentions are, it's just parsing text. So if we did have a computer that doesn't really understand what it's doing, and what what we are doing or what we want, then we give it a command, and it interprets it in a very, in a way that we didn't intend, or in a very unwise way. So it seems to so if we really do want to develop strong artificial intelligence or artificial like AGI or a very powerful form of artificial intelligence, we're going to depend highly on we'd want to try and get it to understand so that a, it can understand what our real intentions are, and also in the larger context, understand whether our intentions are wise. So it's not to execute blindly on and on bad commands. It's kind of like the old fable where Midas asked the genie to have everything turned to gold once he touches it, but he doesn't really think about things very wisely and he touches his family. mean data to gold and all the food? And disaster happens. But yeah,

Joscha Bach 40:04
okay, relevant metaphor is the Golem. golems to exist, right, we build machines that we don't know how to stop because they become stronger than us. And the biggest Golem in this regard, I think is the technological civilization. And we started to go down the route of technology, we set something in motion that we could no longer stop. A good example of one of these aspects is probably global warming. The first predictions that global warming would happen, happened in the I think, late 1800s. And then in 1930, there are pretty good descriptions with pretty good predictions about the actual changes in temperature. And then, by the 1970s oil, although oil companies did internal simulations and got pretty good results, and they went to their governments and told the governments that something needed to be done, but it was not politically feasible. Because the overall dynamic of the system was stronger than the government's, right, we built machines that were stronger than the parts that we could control. And since then, a lot of people weren't as tech, these climate researchers started to tell the public in the 70s and 80s, what the score was, and that people would need to switch before the date 80s to prevent catastrophic results. And we didn't change it. And instead, what happened is that people started to pay attention to this, and they went into denial, or they thought that they need to have very strong emotions about this, and that if the result was not manifesting itself, they would need to have put me to have even stronger emotions about this. But regardless of what emotions we have about global warming, if you didn't do anything, the things that you've done is pretty much what you would have would have done otherwise and resolved, for instance, to increasing oil prices. And eventually, we came up with the Paris accord. And the Paris accord, if you follow up on it is not going to make a big dent in carbon dioxide budget. And it's not going to have a big influence on the climate because the tipping points have mostly been last century, I think. So now we have to deal with the situation that our planet is going to change in interesting ways. It's not going to extinguish life, but it is probably going to extinguish the present form of civilization that we have, or make it much different from what it is right now. And I think you could argue that this creation of a technological civilization is such a machine that you couldn't stop. And so GPT three by itself, I think is totally fine. It's, it's works best for the human in the loop. And when you put the human in the loop, it's it's really it empowers that human to be way more productive in many circumstances. And if you put this thing in charge, then you are probably in trouble. And humans also make difficult, difficult mistakes when they are in charge, and they don't understand something and act anyway or they're fully incentivized. But I see a big danger if you have a system that is faking, understanding so well, that almost nobody understands what's going on.

Unknown 43:08
And you can't provide intelligible explanations about what it's really doing underneath. No, it

Joscha Bach 43:13
can that is the terrible thing. It can explain deep fakes of intelligible explanations. Okay, you can ask it give me an explanation of the following unintelligible this thing is going to give you an intelligible explanation or something that sounds and indistinguishable, right? It's like a consultant, that is completely irresponsible.

Unknown 43:32
Because we won't be able to tell the difference between an intelligible explanation that actually is an explanation of something that does understand and something that doesn't.

Joscha Bach 43:43
So you can also ask you to do that. So it's able to respond to things that it clearly doesn't understand or the probability is low with the answer that it doesn't understand. It's bad. For instance, the difference between the things that understands and doesn't understand it's not going to be able to explain this very well to you. At least in the examples that I've seen, maybe there's a way to prompt GPT three and to completely sane behavior, but I haven't seen it so far. Typically, the way to make it sane is that you understand the domain so well, that you can combine it with some kind of filter that is good to test the output, for instance, against the CRM prover or against the set of allowed parameters. And you roll the dice for as long as you need to get an answer that pass that passes the test. If you don't have that, then you need some thing that has that understanding. And I found that GBTC is basic babbling and it's babbling at a level that is better than the babbling of human non experts. So it's not going to produce something that is as good as something that an expert on good to could produce. But it's better than something that a non German speaker could produce or somebody who is poor right or could produce about Curtis output to simulate that right so That is something that makes it dangerous because it can, in some sense, replace the output by incompetent people by something that looks fairly competent. If you don't have a good critic, sitting there who understands what's doing what this thing's doing. And so to me, the main issue, again, is sentience. And it's also the same issue as our civilization, our civilization is actually not sentient, it doesn't know what it is, it doesn't know what it's doing. It's not planning ahead. A civilization that was sentient would be planning ahead for many generations. But our civilization is not able to act on a plan that reaches 200 years in the future. If you cannot act on such a plan that reaches at least 200 years in the future, you cannot deal with resource depletion or global warming, right? Because it's, there's nothing you can do about this. It's like your child was attention deficit, asked to do its homework. So it's going to get a good grade at the end of the year, it's not going to happen. Right? So we are in a very weird situation that our models of reality change faster than our understanding that the future changes faster than our models. And so we can no longer track it.

Unknown 46:08
Yeah, well, it's certainly changing faster. Now. I mean, we came from a time where the future did was relatively slow, but it's speeding up, isn't it? So it's very hard for us to keep track GTP, three uses 175 billion parameters, or Catan. Right? Do you think it would, it would make market improvements if it were given like the ability to consume a billion parameters, 5 billion parameters? What do you think there's fundamental limitations to its structure, which, you know, you won't be able to get like, I guess great improvements from adding heaps of more parameter inputs.

Joscha Bach 46:47
So the parameters are what is learnt into the model, the structure in the model, and typically is v is basically a continuous Hopfield model. Network, it's very old data structure, if you look at, it's basically like a fully connected network where all the parameters to point to all the other parameters and constrain them for simpler computable relationships. And the old models couldn't be trained. So in some sense, the model structure that we need is pretty much the theoretically optimal model structure. But the encoding and retrieval from that model are not optimal yet, and they need to be improved. And sometimes GPT GeoPDF V is an extremely simple idea as simple algorithm for training it that has been scaled up, there are a number of very clever ideas to formulate the input, so it would work. But there are hacks, and we need to change these texts, the way the content is fed into this, for instance. And we already know Google has trained a model I think, was about 600 billion parameters. And it didn't talk very much about the performance of that model, it was just a paper that came out that nobody seemed to pay a lot of attention to. Yeah, we did that too. And we did it with a larger model, it is a few times larger than deputies three. And Google is probably not going to make this model public because they are their own customer. And they mostly demonstrated to themselves and maybe others that they could do this the scales even further. And expect that Amazon who can basically take its own AWS instances for free on the spot market is able to do this on the site and make a large, much larger model. So I expect that in the next few months, we will see a number of experiments. Also the data set that is being used for training GPT. Three is free, mostly the common core, which is summary of everything that is freely available on the internet over a certain time span. And in the case of GBTC, it ends in October 2019, which is fascinating. This thing doesn't know anything about COVID, or George Floyd, it's basically an artifact of a completely different era. It's before everything started coming.

Unknown 48:53
Yeah. Wow. mentioned what it was what it would output if it had her the most up to date. model of the world. Yeah. Geez. So yeah, I said, one to 5 billion, I actually meant trillion parameters. Yeah. Yeah. Fascinating. So I mean, in order for an algorithm to ideally ground, I'm not sure if you believe humans, ideally, ground symbols. But I guess you know, Are we far off achieving that? Is that is it when are the people consider this a big problem in AI, it's not really properly doing grounding. But before thinking when you were discussing GPT, three, doing some form of grounding. I'm just wondering, like, Could an AI in a completely virtual environment if it was fed, many different modalities of media like that could be video texts, images, tactile data, like sense data, data was collected through people touching things and or some sort of simulation of data and olfactory data, maybe even in something else like that? The sense that we don't have echolocation or whatever, do you think it would be able to ground things in a much more, I guess, amazing way than we do. Can I say

Joscha Bach 50:15
it might scale better than we do. I think that cats have proper grounding. Myself proper grounding in some sense. And the world that the ground things in is different than our world, because they have different affordances, they are entangled with that world in a slightly different way. And GPS Fi is not grounding its behavior or its output, or its models in the same world as we do, the world that it lives in as a result of textual tokens. And the context is a set set of tokens. And the action that it performs using the models that it's grounded in is producing more of these tokens. And you could say that, in some sense, that's similar to what we do we have strings that come in the strings that come in are the patterns on our retina, and cochlea, and our skin and an proprioception and vestibular system, they're all presented to the brain or the thalamus. So the brain is, in some sense, busy predicting the next set of patterns on the thalamus. And many of the patterns on the Talon was generated by the brain itself, for instance, by the control architecture by the motivational system of the brain and by the attentional system of the brain. And by monitoring those as well, if this thing is going to make a model of itself as a control system, right, this is what our mind is doing. It's learning to observe its own interaction with the world and tell a story about it. And we live inside of that story. Right? As conscious agents, we are not in the physical universe, we are inside of that story that the brain generates, about itself and the interaction with the world and the world. And there is no reason why a system like GPT three with some extensions and real time behavior should not be able to do that. To make it really interesting, it should have the capacity for online learning. So it should be able to change its behavior in real time and change its its models in real time. It's upstate, it's entire things in real time. And so it has attentional learning, that happens in real time, that implies that it would be able to reason and learn from the things that they did reasoned about, and make this these inferences that performed in any given situation available in future situations. Right, this is an extension that would need to happen to GPT. Three to make it more like us. But the most important thing is, it would need to be connected to a universe that is more similar to ours. And it would need to be connected into it in real time. So it's going to predict the next set of patterns based on some kind of real time input to make it really interesting.

Unknown 52:56
So the virtual reality would have to be rich enough that it's in or it would just have to have sensors and actuators connected to the real world, like we are.

Joscha Bach 53:06
Yeah, but for us, the crucial thing is not so much that our sensors and actuators are connected to the real world, but that they work and our mental simulations and the mental simulations are connected and tracking the things that happen in the real world. Right, you start out with a system that has some simple cybernetic control. And this system already pretty much knows how to walk and how to grasp and how to fixate things, it's learned that but there's relatively simple learning functions that are hard coded in the brain training functions are built into the brain. And once the basic training has kicked in, you have an attention agent that lives inside of the system, and observes all the underlying structures and learns what will relate to what to improve the behavior of the system. And the protocol of this attention agent is what we can perceive as our own conscious contents. Right? It's this attentional protocol, what whatever captures our attention, what we would get into focus of the attentional system might enter a protocol memory. And that includes the activity of the attentional system itself. So for instance, order to pay attention, we need to check bet back on the fact that we are currently paying attention and just not drifting off in some kind of transfer. We think that we are paying attention but we actually have fallen asleep already into this property that we go back and check. Have I fallen asleep, or am I still paying attention? Am I still tracking a consistent reality is necessary for the type of attention that we have. And that makes our attention reflexive. And it's very tempting to think about what system look like that is producing general embeddings and acts in real time on an agent that can improve its performance based on the models that the system distills.

Unknown 54:51
Interesting, I think Oakland Cobb was tapping a form of micro sigh in order to sort of do some sort of attempt at attend All thing. Are you still working on Microsoft?

Joscha Bach 55:03
No, not much. At the moment, I'm thinking about making the next edition of Microsoft to distill some of the ideas that currently have into a new architecture. The previous edition of Microsoft is still being used by Microsoft industries, which is a startup in Berlin that some former students of mine have founded and my co founder in this, and it's being used as a basically a framework to control industrial robots and learn hand eye coordination for robots. But when I think about microsite, as I started conceiving of it, it was a tool for me to test my own understanding and build it up. And it's related to a cognitive model by the German psychologist Dietrich Turner, who came up with a theory that he called the size theory SE is not an acronym for anything, it just stands for psychology, this Greek letter. And I thought that if I write a book about the theory, maybe I make need to make it an acronym, so I called it principles for synthetic intelligence. And I tried to distill the ideas that Dietrich Turner develop in the course of his professional life are those that he documented in publications into concise form, and try to implement as much of it as I could. And so we had some deep insights about the nature of representation, representation hierarchies. spreading activation is message passing, inference, object representation, abstraction, causal reasoning, agency, action control, motivation, and emotion. And I thought, this is an extremely powerful way to think about the nature of artificial intelligence and to make sense of it. It's not necessarily the best way to build such a system, because it's something that allows you to build your own constructive understanding in your own mind. And biological system is not constructed, it's grown, which means it's constructing itself. So you need to move one level higher, you need to build a system that wants to grow into the shape that solves the problems that you want to have, instead of learning, you think about meta learning, you don't think about how can I learn this particular domain? But how can I set up a system that is that wants to learn this particular domain is able to figure out how to do this to self organize?

Unknown 57:23
Wow, fantastic. The concept of computational ism, it's got its historical meanings. But when people like Stephen Howard, say that computers can't do things that we do, because they're computing. It's like a sort of mismatch and people's intended meanings behind the word computation and computational ism here.

Joscha Bach 57:48
Because there isn't some kind of difference in what he means by computation, and what for instance, Turing meant by computation, and others that and so maybe we need to take a step back and talk about what we actually mean by this. Exactly. And, to me this, it's very useful to think about the relationship between computation and mathematics. Mathematics is, I think, best understood as the domain of all languages. Basically, you have some kind of notation, or some kind of system that you use to talk about relationships about other things. A language is a thing that allows you to talk about other things that makes symbols have meaning, for instance. And this is true, whether you're doing geometry, or whether you're doing algebra, or calculus, and so on, and it mathematics consists out of functions that describe relationships in a way. And the difference between computation and classical mathematics is that basically, mathematics prefers representations that are stateless. That allows you to do things like looking at infinitely many elements in one step. So you can describe a function that would compute pi to the last digit. And it's something that you cannot do step by step you, it's something that you could only do if you would have a system that is able to process infinitely many steps all at once, right. And so if you have a stateless system, and you can come up with a specification for the process that would have to happen in these infinitely many steps. fantastical mathematics is your friend. And computation only does things step by step. So no computational system is ever going to know the last digit of pi, or perform operation that would require knowing the last digit of pi, because you can only do a finite number of steps in any finite number of time, and in fact, if any finite amount of time and the interesting thing was that Google has shown that the assumption that we can define functions in such a way that the number of steps doesn't matter leads into contradictions. This is the true point, I think of both the halting problem and the child's problem that Turing describes, and of the issue in girdles proof, where you use a self reference. So whenever you have the situation that you are, the algorithm doesn't terminate, but you don't get a result after a finite number of steps, it is not meaningful to say that you have that result. And this means that you have to redefine the nature of truth. Because in classical mathematics and in griddled online tools is something that exists somewhat independently of the process that is used to determine truth. computational system tools would be the output of a function. And if the function is not done yet, the function needs to be computed, it's meaningless to say what the truth is. Which means instead of something being true or false, you might have the situation where you just don't know yet. And then you may never know. And this notion exists in mathematics. It's basically constructive mathematics or intuition is mathematics has that already. And computation is constructive mathematics. It's an obscure subfield of mathematics, though you need to construct everything before you can claim that it has a value from the constructive perspective, pi is not really a value. It's a set of functions. And you can take one of these functions and plug it into your local sign, and then you give as many digits as you can afford to. And this is it. And so this doesn't change mathematics very much, because it doesn't take anything away from the mathematicians, they could never compute uncomputable things before. But for a lot of mathematicians, it's disturbing. If you tell them no, you have to constrain yourself to the things that you can actually compute. And there's this issue that physicists borrow the codebase from the off the mathematicians before Google was done. And you were forced to become a constructivist. Instead, they basically, some physics relies on having known the last digit of pi.

Joscha Bach 1:02:07
And this leads to some weird theories about how the universe works and some weird intuitions, right? So even today, many philosophers believe that Google has proven that mathematics is important to describe reality. And this gives the advantage to philosophers who don't know mathematics somehow. But it's not what I think what Google did was, he forced us to accept that everything that can be modeled, and that can be understood, has to be done. So with a finite state machine. And so we basically now have a certain idea of how systems operate, that have the capacity of forming a language that can be implemented and whether drugs can mean anything. And this narrows everything down to computation. So basically, every understanding of a system that can be implemented, is going to be computational. Now think to for something to exist, it must be implemented somehow, right? Something must do the work, something must realize it. And to be implemented, it needs to be computable.

Unknown 1:03:05
It's interesting, like, it's a big discussion about whether the universe is continuous or granular. And I wonder, like, if we did find a galactic expanding civilization, creating mega structures, whether it would be trying to realize platonic sort of ideals, by plugging, you know, trying to get the last digit of pi by plug it into, you know, a galaxy and computing, or into a black hole editor? Yeah. So how

Joscha Bach 1:03:33
would you read this out? So imagine you had such a system that is able to perform some kind of blackbox system? How would you measure the value that it's gotten? Right? How can you do this, there is no way

Unknown 1:03:48
I don't know like pipe into an ideal universe where you can do that instead of this.

Joscha Bach 1:03:55
Infinity infinities work to some degree when you postulate them as given, which means you basically create a virtual black box and say, this box is able to deal with the following types of infinities and perform the following operations over them. And then you can reason over that box. But I think in reality, what happens we have never had such a box. And if it did exist, we would not be able to recognize it or describe it. And the assumptions that we can describe it exhaustively by defining a process that can be implemented results in solving this infinity leads into contradictions, which means the avert infinity does not actually mean anything. It can only be used in a system of language that is inconsistent, which means it's it's not useful to ascribe a reality that is consistent. And so if you want to describe a consistent reality, you need to find a different word that does the same book. And if you look at it, in which context is did we use infinity, and we typically use infinity in the context where we have too much stuff to count. And when you look at the physics around Stress the environment that is mostly made of too many things to count, right? Although all these molecules, you cannot count them. So you have to look at the dynamics of many molecules in the aggregate and the limit. And this this limit is, is if you had infinitely many, right. And so geometry is the behavior of too many things to count is exactly those operators, that give you pretty much the same result if you have a few trillion parameters, or a few quadrillion parameters, as opposed to a few dozen parameters. And when the difference between a few trillions and a few quadrillion parameters is smaller than the difference between a few trillion to a few dozen parameters, then you are looking at things that you have convergent operations, and you cannot complete the main able to compute them, but you can decently approximate them. So if you have an effectively computable geometry, and this is the kind of model that our own mind does when it deals with the world around us. First things look to us, if there are infinitely many result that infinity find resolved, you'll just look at behavior of stuff in the limit. Fantastic. So to get back to Stephen, hairnet, his understanding of computation is grounded in a different discourse. And it's the question, can you use something that is a formal language that is grammatical that uses basically a low dimensional discrete representation to describe reality? And so basically, you can you use something like a natural language that we use in a very strict way. And basically that Vidkun Stein envisioned it an ideal language for philosophy and use this to, in some sense, build a complete model of reality? Or do you need to use something that is more messy, something like geometry. And it turns out that all the geometry that we can do, we can only do this by defining something that you mentioned learning system that is approximating the dynamics over these functions in the limit, right. So if you can train a system that is able to perform the geometrical operations is an arbitrary degree of accuracy. And this representations of that system on your computer are all done done with finite automata. It's finite state machine. And these are discrete automata. So this is sufficient to do it. And you could in principle, also translate translate this into a natural language or Boolean logic, everything in the computer is expressed with NAND gates. So it's even simpler than Boolean logic. And you can write it down. But of course, no human being could read it. And so the misunderstanding is, can we use a language that fits into human minds? And that is discrete? Or do we have to use a different language that would not fit into a discretized verbal form, and low dimensional form into human minds. So it's sometimes it's about the difference between the languages in between two reasoning and analytic operations and language on one hand, on the other hand, the language in which people perception, this is this geometrical language. And I think that Hornets argument makes more sense when you say, Okay, we need to ground our linguistic representations, in a model of reality that is essentially geometric and has the properties that it can track what happens on your retina. And that can track your interactions with the world that you build to predict what happens on your retina and thereby explain your own agency. And that makes total sense. But the idea that one is a computer and the other one is not as very misleading. In order to perform deterministic controlled behavior, they both need to be computers. So in some sense, the there is no difference between the cybernetic system and the computer or dynamical system and the computer in my view. Okay.

Unknown 1:08:33
I want to talk about category learning and category perception. But do you think deep learning is able to be I guess, adjusted so that it takes into account more of the causal stuff? I mean, you're sure Bendigo thinks that the next big sort of progress in AI is causal. Guess understanding in order to he said, in order to for deep learning to make progress, it must, I guess, replicate true human intelligence. They should be able to make sense of causes and be able to answer questions of why and provide explanations. And do you think causation is important for that?

Joscha Bach 1:09:14
Well, I think to answer why questions per definition, you need to do some causal modeling, or something that is indistinguishable from causal modeling under all circumstances. And if you will define causal modeling properly, I do think that GPT three is able to do cause semantics as I tried to explain before because it's able to learn transformations. And it's not. It's also been able to distinguish which transformation it's has to use in a certain context because it could do other transformations as well. In an example, that where I did the sentiment extraction, I asked it to explain the emotion mentioned the sentence and I said when I woke up and looked out of of the window, the flowers are radiating in the sun. And the emotion that they found was happy. And when, when I looked out at the windows and the flowers, my gardens were suddenly replaced by giraffes, the emotion was surprised. And when I looked out of the garden and the flowers were all rotten and dying, the emotion was discussed. And when I looked out of the windows, and all the flowers had been stolen by my disgusting neighbor, the emotion was anger. And so it didn't match disgusting neighbor to disgust, right, it understood that I was actually angry and use disgust to characterize my neighbor in anger. And it was able to just emigrate this. And that's not that surprising to me at this point, because I saw what DBT is we could do. But it would have been very surprising to me, if I had tried it the first time on that thing, and I'd never heard about its capabilities, right? Because it seems that to a very casual observer, the semantic distance to of that sentence that actually actually matters mentioned that the neighbor is disgusting to discuss disorder than to anger.

Unknown 1:11:14
So I mean, like, could you achieve an equivalent of an understanding of causation just by brute forcing enough correlation? Or do you think there's there's got to be like fundamental like, I guess, more algorithmic pro progress,

Joscha Bach 1:11:28
that basically means you have to understand when something is the case, and when something is not the case. And once you are able to understand or to to model to which degree, a certain phenomenon predicts another phenomenon. And the absence of that phenomenon predicts that there is no change going to happen in the evolution of that system is requires that you understand, how would this system that you are affecting evolve normally, what internal structure does it have? What components would interact with what you do to that system? Right? How do you know that you pressing a key on the computer changed an outcome on the screen? You might not know right? It could be that the computer is currently in a demo mode. And they are constantly things happening and your key press didn't actually change anything. Maybe this would have happened anyway. And you have to have a model that allows you to predict what happens in the computer, and you need to be able to predict what happens in the absence and you need to have a certain way to test that. And this is going to increase your confidence that your model of the dynamics is correct. And there is no reason why a deep learning system cannot do that. In my view, deep learning simply means compositional function approximation, it doesn't doesn't mean that you have to use a certain way of representing things and a certain way of learning it. The reason why we use stochastic gradient descent with backpropagation in the layered architecture is not because that's the only way to do deep learning, but we use it because it works so well. And GPT three is another type of deep learning, and we can conceive of many other ways. So you could conceive of a system where every new one is actually a smart local entity that is a reinforcement learning agent and tries to get things right and has a little bit of memory and able to predict the future. And they are linked up in such a way that the right behavior is emergent. And it would still be a deep learning system if it's able to learn compositional functions, I think. So the difference between deep learning and shallow learning is that for instance, a shallow learning system could be something like linear regression. That is relating the inputs Wallis directly to the outputs. And if you want to use hidden states, that things that you cannot directly observe some internal structure, that it's difficult to guess the internal structure, right. And worst case, you could just try for instance, you could use some kind of evolutionary process that is just throwing internal structure at the problem. And linking inputs and outputs until the internal structure is giving you a good approximation between the inputs and the outputs. And this actually works. So for simple problems, you can just use some evolutionary learning to basic place neurons and links in the hidden layers, until your agent is able to play a certain game on the screen. But it's not working very well, for complicated problems. For instance, this approach is not going to learn the difference between cats and dogs very well. And for this, you need to have a way that decomposes the issue of learning cats and dogs in such a way that you can gradually improve on your progress. And do this as many many small changes that you perform on the model. And one of the best algorithms that works on this is stochastic gradient descent was back propagation. But there are other ways you could just look at core currencies between bits and then build up from there and you could have self organizing things and you can have some energy minimizing things and so on eventually the question is just do you find a way to gradually improve on finding the truth based on the current output,

Unknown 1:14:56
if we could run current deep learning on a major refresh your brain around, like our Sun. Do you think it'd be able to behave like a human? Or like would would? Would it? Wake up? Would it be able to sort of find sentience, so something goes there something,

Joscha Bach 1:15:15
it would definitely not behave like a human. Because imagine that you would basically be spread out around the sun and interact with the sun. At cosmic timescales, you would experience yourself as something that is very, very different, right? Even if you retain your capacity for sentience and making sense of reality, and you just scale it up, you will be completely in human. And this human identity is the important question. Why not? I suspect that sentience is the result of some kind of control problem that requires to understand your relationship with the environment. So if you need to optimize yourself, and the way that in which you discover policies and apply them to actions, and this problem is learnable. And you can make observations about your own behavior and use that for learning, you will, at some level form a model of the system interacting with environment. And when the contents of that model are driving the behavior of that system, including the self report of that system, then the system will tell itself and possibly others that it's sentient. Right? And is this something that we this is the reality that we observe in ourselves, it's, we observe an agent that is interacting with its environment, we observe the best model of that reality that our brain can can others. And we ask our mind, am I real? And that thing says, Yes, it's the best approximation of reality that I found, right? So yes, you're real. And, of course, you're still completely virtual. This wouldn't change. For me, the more interesting question is not so much. What happens to the AI systems that we built? Is this particular algorithm going to cut it we don't know yet which algorithms are going to be general enough. But I suspect that sentience is a general property of an information processing system with sufficient complexity, and time and resources for self organizing itself. So I suspect that, if you will, that the plan sit alone for long enough, I don't see a reason why it shouldn't become sentient. The difference between plants and animals is plants don't have a nervous system and the sensors we do. But so they have to compute by sending signals to adjacent cells. And basically, all cells in our body have the ability to send messages that can be encoded as different chemicals. So different message types, and to learn how to react to them, there's an evolutionary process. And this evolutionary process can also affect temporary state changes in the current cells. So this current cell can learn how to react to certain messages in a given context, according to the constraints that it has evolved, which means that the cell in the plant can exchange measures in almost the same way as a brain can. The difference is that the message exchange is going to be much, much slower, because it can only progress a few cells per second, the chemical gradients in the plant, which means for the most part, the cent plant is going to be very, very slow. And for the plant, we're going to be some kind of a blur. It also means that relative to us, the plant cannot collect that much training data, it's going to be relatively young when it dies. And the more interesting level of intelligence for a plant based system is going to be more on the level of forests or ecosystems. And how would you know that an ecosystem is sentient.

Joscha Bach 1:18:30
And I wonder if the old moose of fairies, which are basically the spirits, the operating systems of ecosystems, where one day in Fairyland equals seven years in human land, if this has some kind of relationships, and where people get this amazing idea, and they came up with the stories, also took me a long time to realize what spirit actually means. In our culture, we think that it's a superstition, but our culture is like 400 years old and emerged after we broke free from the Catholic cult that has completely obscured our epistemology. And I think that the word spirit actually means just operating system for an autonomous agent. And entertainment as word was invented, the only known autonomous agents were people and plants and ecosystems and cities, and nation states and so on, and they all have spirits, right, they all have an operating system that makes them coherent. And to be coherent, they also need to have some kind of agency and the coherence enables that agency in turn. So the plant needs to be coherent with itself. Otherwise, it's not going to work the parts at the top of the plant need to communicate with the parts of the root of a plant in a particular way the water that is pumped into the roots needs to reach the crown of a tree and so on right and if the water demands of the crown of the tree change, then this needs to affect the woods. So there needs to be all this information processing which needs to be coherent right. And this is in some senses operating system is the spirit It's a software. And software is not physical in the sense that it's realized as atoms, it's or that it is in space. It's a model that you impose on reality, it's a particular way to understand reality, to project it onto macro states, you could say, software is very, very specific physical laws, the software in your computer is just a physical law, it doesn't have an identity. It just means if you arrange matter in this in this famous these in these constraints, regardless of a role in the universe, as long as the space is these, these properties, the following behavior is going to ensue. Right? Does this what software is doing, it's just a physical law that exists in in the eye of an observer that tries to make sense of what's happening. And in order to make sense of what's happening in the plant, in the absence of the ability to observe all its molecules, you need to have the software description that describes causal structure at an abstract level. And people have a spirit in the sense and plants have a spirit. And the question is, can we give AI a spirit, a coherent soul, so to speak, a thing that is relating all its activities in a coherent way, and turn it into a coherent agent. And you could also ask yourself, can a civilization have a spirit that turns into a coherent agent? I think if a civilization had a spirit, it would mean that it's planning very far ahead. And this thing, which allows civilization to plan very far ahead is a culture. And in the absence of culture, you get some kind of spiritualism, that is a phantom limb by this super superstitious spiritualism that Well, lots of people in our culture, let's civilization have that. Because our civilization has lost the plot. It does not plan ahead anymore, right? It does not have this coherent idea of what it is and what it should be doing. Instead, people are grasping in the air, and they have ideas about moral behavior and moral impulse that we'll be having that are not really grounded in the outcomes of the actions in the long run, right? Nobody has moral intuitions that when you would act on them, due to the survival of the species and the conditions that we find palatable. It's a very weird situation where our preferences seem to be incompatible with what would be necessary for our survival. Yeah, strange. So the only thing that is left to us is some superstitious notion of spiritualism wherever the greater hole gets replaced by something that is a fairy tale, some spiritual entity outside of this universe that cares about us, and that wants to save us and gives us an afterlife. That gives us meaning if you perform certain rituals, and can trigger certain emotions visited and degree of reliability, if you are adhering to a certain card and do the right things for the card. But this is true superstition, I don't think that it really is to its spirit superstitious.

Unknown 1:22:45
It really does sound interesting to explore the idea of some form of empirically informed spirituality, where we

Joscha Bach 1:22:52
could Yeah, but it's practically very simple, it means that the dynamics of the parts of a system can be described by a shared purpose. And it's if the shared purpose describes an agent that has intentions and goals and is able to act on them, then you could say that it has a spirit in some sense, but you have a next level agent. And the same way as the organism doesn't actually exist. Physically, the organism is a function that describes the interaction of cells. These cells exist, in some sense physically, but the interaction between the cells is virtual, it's this is the description of this organization that makes the hole bigger than its parts, because it tells you which cells are good ones, which are not, which should be attacked by the organism in which should be strengthened in their function, and nurtured and so on. This is what the organism is about. And the same thing is true for society or civilization. And the relationship that we as individuals have to civilizations would be similar to the relationship that the cell has to the organism. And with AI, the big question is, can we build a eyes that she has purposes with us? Where we basically serve the same God? The same overarching spirit?

Unknown 1:24:02
Yeah, right. I mean, we probably learn a lot about what that purpose should be from, like something as intelligent as, you know, super intelligent AI as well, the purpose that we're given just by our biological imperatives can't be the be all and end all of what we should be doing in this universe. I mean, we, it sounds to me like it should be a discovery process as much as it is one that we've sort of inherited,

Joscha Bach 1:24:28
but might not be actually that heart. You know, it's not obvious that the regulation inside of a cell are much easier than the regulation inside of a brain or inside of an organism, in the same way as the things that you need to solve in your own life are not necessarily faith much less easier than the problems that the organization that you're part of, say the company that you're a part of has to solve. Of course, you're not able to deal with all the complexity of your own life because a lot of that is done for instance, by your judgment stiff systems that is dealing with many, many details when you say have an infection after you ate the wrong thing, or when you need to fight off a disease, very complicated decisions are done by subsystems. And the only the coordination of the subsystems is what you need to concern yourself as. And the same is true for a society or for an organization inside of a society. It could be that understanding the purpose of a civilization and its place in an ecosystem on the planet is not necessarily at heart, because I suspect that a civilization is not going to be the top level system on the planet, it's actually about the dynamics of the cell. Right? Life on Earth is about the cell. It's not so much about us. Yeah, just one thing that the cell can do. And I don't think that we realize the entire potential of the cell, we typically think about the cell as a part of us, right? It's the building block of us. But you know, every cell in your body is an instance of the first cell, the first cell never died, every cell in your organism split off from that first cell. It's an instantiation of that principle. And everything that lives on this planet, as far as we know, is still an instance of the first cell. It's just as differentiated by evolution, and is building complexity to fill all the niche. And we are just a small episode in that whole thing. Right, this so the cell is the actual hyper organism that is populating Earth. So in its big Cooper's that we think we are more important at the moment. Yes, I think versus respect to life. And now the question is, is the sale the optimal computer Omnium? It's probably the most robust one, it's very hard to imagine something that is smaller than the cell. And it's a Turing machine built into it and the self replicator and it's stable and able to adapt to a very wide range of circumstances by itself, right? It's the cell is the only thing that can do this. And you cannot probably make this much smaller. So it's very difficult to imagine a molecular structure that can do what the cell is doing. And it will still function, the same degree of robustness.

Unknown 1:27:00
Have you read much of Drexler stuff? No creation and that sort of thing? Yeah.

Joscha Bach 1:27:07
And I know that exists, but I haven't read it now. I have read I think a few articles by him, but I haven't read his books.

Unknown 1:27:13
Yeah, it's fascinating stuff. Sells weren't intelligently designed, but evolution selected for, I guess, a good enough system, such that would sort of work. And it gained, you know, dominance, you know, ancestral environment. And well, most most organisms on Earth, I think most of the biomass is single cellular, if I'm correct, right. Yeah.

Joscha Bach 1:27:37
Interesting question would be could you evolve of cellular structure into something that forms rockets and goes to the stars and populates other planets? And the solution is probably us, in some sense, right? This is the way that cells self organize to make that happen? If it happens,

Unknown 1:27:55
yeah, I guess that sort of assumes some sort of teleological stance to the way it's

Joscha Bach 1:28:01
not necessary. It is an emergent phenomenon. Yeah, it's just happens by the evolutionary drivers, because there's going to be a niche that makes it possible to get to the next stage in harnessing work of the available complexity, to basically complete the purpose of control is to harness complexity, to shift the order further into the build bridges into what otherwise would be chaos. So you can live in there that you can harvest chemical reactions and the and entropy gradients in domains for them chemical reactions cannot. And that is what intelligence is doing. And now the question is if you build something that is completely removed from biological cells, basically built an AI that is only using constructed artifacts that are microscopically constructed, not built from a self similar components as organisms. Could this outcompete cells? And it's an open question to me, it's a very interesting question.

Unknown 1:29:02
We've only got one sample of it working here on Earth. I mean, we can assume that maybe there's life out there in the universe, we can't see any evidence of it, especially advanced life, right. You know, there's no megastructures there's no evidence of expansionism and everything that we see out there in the cosmos seem to be explainable purely through, I guess, basic natural laws without intelligence involved. Yeah. So if we found, like, single cellular life in the valleys on Mars, for instance, underground, that seemed to come from, like a very different phylogenetic tree than ours, then we can make assumptions about, well, maybe it's elsewhere in the cosmos as well.

Joscha Bach 1:29:49
If you find life on Mars, I would be surprised if it doesn't have the same source to me or biogenesis. To get to the first cell seems to be very unlikely. I suspect that you need to roll the cosmic dice super often. Maybe as often as their planetary surfaces.

Unknown 1:30:05
Wow. So that that would mean that what that translates to were you saying that like, even single celled or basic life, it's very rare in the universe to be like, okay, so panspermia, there was something happened somewhere else and the basic building blocks were transported to Earth. And

Joscha Bach 1:30:26
I suspect that basically the probability that if you put a few cells on some meteorite and saw it into space, and it hits number is probably higher, that this thing survives somehow, and takes root on that planet, and life span taneous Lee emerging, maybe there are theories that explain this. And I'm not completely aware of this. But basically, when I look at the complexity of the cell, and the fact that individual parts of the cell have difficulty to work with each other in a stable way. And it's difficult to imagine this spontaneous formation of these things outside of a membrane, and so on, that's, yeah, that makes it quite unlikely that complex organic chemistry is able to spontaneously form all these dynamics.

Unknown 1:31:15
Well, we had like cells and like, we got, like, you know, the mitochondria that was swallowed by another cell, and then they sort of learned to become partners, and to coexist, but they

Joscha Bach 1:31:26
were both cells or were to start with, right. So all this functionality seems to come from a certain subset of functionality that you need to bootstrap all this. And all this is very complex molecular machinery. So I really cannot blame the Intelligent Design proponents, that they think that the first cell is highly unlikely. But I would think that if we are in base reality, then the we need to account for the intelligent design by a non biological intelligence. And so interesting question is, could there be non biological intelligence? Is it could there be a gas giant, for instance, that spontaneously forms, self organizing structure? Maybe it is? Yeah. So imagine you have a thing where you have, say, atmospheric storms. And these are beautiful spherical forms create magnetic fields, because the clouds have metal particles in it. And this means that some clouds are going to move differently from other clouds, depending on how they react to these magnetic storms. And then at some point, you have a self organizing system. And if you let this fester for a few million years, maybe it self organizes more and more feedback loops until it becomes sentient, and then realizes, Oh, my God, I am alone. It's not going to suffer from this because it's not evolved to feel bored if it's alone. But maybe it finds it interesting to populate other planets with intelligence. And then it constructs the first cell is a for Neumann probe. This is pretty out there.

Unknown 1:32:56
every direction.

Joscha Bach 1:32:58
So maybe this is how it happened. Maybe we should ask Jupiter of what we're doing here.

Unknown 1:33:04
That'd be awesome. So category learning and categorical perception. These are some things you've done some research on, is that correct?

Joscha Bach 1:33:17
Yeah, a little bit. But it's, it's not nothing very special. Basically, if you do supervised learning, it means that you are training to recognize categories and patterns. And it's unsupervised learning, you can try to identify minimal structure that explains the majority of the phenomena. And this minimal structure, if you try to separate it into orthogonal features, then these features can be understood as categories under certain circumstances.

Unknown 1:33:49
So I mean, have we we got category learning correct in AI? Or do you think like, in certain ways, it can go a lot further.

Joscha Bach 1:33:59
The general form of a model in my view is a set of parameters, each of these parameters can have a set of possible values. And between the parameters, you have constraints, which say if the this parameter has one of the following values, then it's going to dictate the following possibilities for the following other parameters. And as a result, you get a very big computational structure. And if you now take some of these variables and fix them to sensory input, for instance, to the things on your retina, then this means that this forces, the internal states of that model, into particular configurations, right, and this is going to take some time until all the information travels through this and then you can create a feedback and use this feedback to predict the next set of sensory patterns. So you have some basically some continuous oscillation that goes through the entire model, and is going to regulate against the disturbances by a person. natural process and against the disturbances by a motivational process, for instance, by the needs of an organism. So you can imagine that you have just a system that is not doing much anything but surprise minimization, it's going to try to predict the dynamics of the parts that you can control and those that cannot control by changing the parts that you can control. And categories are a certain way to talk about some of the hidden states in there.

Unknown 1:35:30
Right? I mean, they useful.

Joscha Bach 1:35:33
Yes. And so the question is more or less is this the best way to talk about these hidden states, it's probably a pretty good way to talk about of some of these hidden states are the necessary under many circumstances, there probably are not for all. So there are some systems which are, in some sense, computational irreducible, and you wouldn't have categories for you just have more or less continuous features that are propagated to the system that you cannot abstract very well, but our own mind has strong bias to find abstractions. And part of that is because we want to communicate them to others, and we want to make plans over them.

Unknown 1:36:09
Yeah, I mean, it's not it's not computationally feasible to like see, everything is a gradient, which doesn't have enough head space to do that.

Joscha Bach 1:36:17
But we know that the difference between say, a wolf and a dog is somewhat contrived, right, at some point, there is, it's not clear where the wolf ends and the dog begins. And in some circumstances, makes sense because a wolf is a team doc. So I thought about around the dog as a team, Wolf. And you can breed dogs in ways that make them quite unwell flag. So you could say that most of them are canines. And now you have a category that is quite different from feeling, felines and slightly more clear cut. But it's difficult to identify for normal observer, what the clear cut distinction is because you have to go back to an ancestor tree and look at the phenomenon of dogs and cats over a very long time to understand the fundamental design differences, which are, after all, quite subtle. And so when people made the distinction between cats and dogs, they probably did this index equally. So they could point at this and say, This is the difference that we propose. But it's nothing that an individual would discover in their own lifetime, you can discover this when it's suggested to you and when you dig down, you'll get to the foundations in the sense that you understand the reasons for the differences that you observe, and the classifications that you make. It's so in this sense category is a useful concept,

Unknown 1:37:33
it's certainly very useful. I mean, I can imagine it being useful to like have sort of superimposed the idea of like just strike categories with very sharp divisions and wide distinctions between them, and then have the idea of like gradients between them superimposed on top for when you need it. But you don't need to compute it all at once. And you can slip between one layer and the other. When, when needed, you don't have to always think in categories, you don't have to always think in ingredients. So continuance, it's useful to be able to do both. And it's useful to be able to compress non salient information away. So as you're just using your clock cycles to focus on the signals that are selling, right,

Joscha Bach 1:38:19
so I find that in our everyday interactions, scientists mostly use decision trees in a way to decide things, they have discrete models, so they can make proofs. And common sense reasoning is, tends to use some much more fuzzy thinking to distinguish reality. So it looks about distributions and probabilities and probability, mass and so on. And the reasoning processes spoke differently and common sense reasoning than they do and scientific reasoning. And scientific reasoning works very well on the margins in special cases. And it's exactly in those cases where common sense reasoning fails. But common sense reasoning works over a much, much larger set of circumstances and is far less brittle. And generalizes better and tends to be more stable, right? The ideas that we get in scientific reasoning tend to change every decade or so in the frontiers of our thinking. And often we have quite fundamental changes in the way that we reason about reality, which do not necessarily lead to fundamental changes in our common sense reasoning. So, common sense reasoning, in a way is more robust for for the most part. And when we think about the way that we are making sense of reality, at the boundary between science and common sense, and the scientists might have the wrong biases. And it's also something that resonates of this Hornets thinking about simple grounding, that a lot of people feel that scientific thinking is not properly grounded. And indeed, often it isn't, or it isn't quite in the same way as people into it that it is

Unknown 1:40:00
But it sounds like to make common sense is more at the level of wisdom. And it's, you know, wisdom that is sort of case from understanding and understandings previous. It's not really the same as understanding. It's more like, yeah, I guess materialized useful rules of thumb that sometimes come from understanding. That's why it's common, because it's commonly used. And it's, and it worked a lot. But understanding seems like a lot more of a dynamic process that's continuously going on.

Joscha Bach 1:40:32
Yes, but this does a specific thing. It's just a result of an aesthetic in which you understand the dynamics of the world that you're in. And you're able to choose the right goals based on this understanding of the big picture also involves choosing what not to do. And not everybody who feels that they have common sense reasoning, are, is able to do this in practice, in our society. Common sense means more that it's common, which means it's shared between people. And people can be insane in groups very easily. That's right. And so personally, I find the aesthetics of people that prefer a group thing over autonomous reasoning disgusting, this idea that you should give up your own insights and things that you can discover to be true. Just because the group thinks otherwise. And you, you want to belong to the group. That's a very ugly thing to me. And often, when we talk about common sense, we think about the sense that we have common with others. And this is not the common sense that I would like to talk to in this this way. I would like to talk. Often we would say that wisdom is rare, it's not common.

Unknown 1:41:44
Sure. Okay. That's interesting. Yeah. I mean, like, I do agree that a lot of our common sense does sort of come from tribal biases, or, and things like that, you know, wanting to fit in or wanting to the aesthetics of your peer group. The people who want to accept you as part of their group.

Joscha Bach 1:42:06
And it makes total sense if you don't really understand reality, and there's often not a benefit from wasting many, many years on understanding reality very deeply. And that your group has pretty good recipes, if you follow them, you're going to get to share their resources, and they're going to treat you as somebody who's trustworthy, as long as you are willing to reveal a set of beliefs as a batch, like this. That's a very common pattern that happens in societies, that we basically ask people often even to bring a sacrifice that they're willing to accept beliefs that they can recognize as patently untrue. So random people would not have the same belief and identify themselves as group members. So for instance, religions do this very often that they accept you as you to accept a certain creed that no random person on the street would logically accept. And the reason for that is that it makes you recognizable to your own group.

Unknown 1:43:02
Yeah. You don't want to end up on the wrong side of the spear,

Joscha Bach 1:43:07
of course. And you also want to know that you can trust that other person, even if you don't know that other person directly. So the difficulty when you build a civilization is how do you get over the Dunbar number? How do you make it possible to relate to strangers in a predictable way, and make sure that they are serving the same purpose as you do. And one of the best ways is to signal this your allegiance with certain quotes that it basically act as checksums that show that the contents of your intellect are not randomly generated, they are the result of a particular indoctrination, that makes you likely to share the following preferences in sharing resources and making decisions

Unknown 1:43:46
makes it much easier to case a whole heap of cardboard cutouts of everybody who believes in religion X, when there's millions of people to deal with, because we can't fit everybody in our own heads. You mentioned a Dunbar number before having sort of just assumptions that sort of constrained the way that they'll likely behave makes it much easier to deal with huge amounts of people. Yep. Yeah. Interestingly, handed also wrote a paper more recently, that category learning, the idea of it was category learning can alter the perception. And the way that we see things the way that we sort of understand things. And so like, for instance, if somebody gives a name to the difference between two colors like green and blue, instead of having one name to describe both green and blue grew, then one culture who sees green and blue the same will get the same essential data, they'll be able to recognize differences in the gradient, but it won't stand out as much. Then if they have different categories for green and blue. It's interesting to talk Got that. But it's also interesting to talk about, like the way that we describe the, the the enterprise of artificial intelligence and achieving the grand goal. And that is achieving, like super intelligence or strong AI. But there's different words. And I'm just wondering about your view on the differences. Well, the the idea of general intelligence, people say that as soon as we achieve general intelligence that said, we'll have super intelligence, but I don't necessarily see that it is the case. And like, a lot of people think that general intelligence is will have intelligences with understanding sort of somewhat equivalent to ours come along for the ride.

Joscha Bach 1:45:41
So let's first talk about the category of general intelligence. Yeah, I think intelligence is the ability to make models. It's not the it's not wisdom, it's not cleverness, it's not rationality, it's just modeling capacity. And it typically happens in the service of control problem. A system that controls something needs to implement a model of what it controls that is isomorphic, to the system dynamics that it's controlling. And so this is what intelligence might be for. But very often, when you have a problem with regulation, you compensate this because making more elaborate models. It's so many highly intelligent people. The reason why they faced so many resources, or use of manual resources and analytic thought and on modeling is because the regulation is so bad. So they try to understand the circumstances, for instance, in which the regulation is bad, or the reason why it is bad, and then they are how to compensate for this by acquiring special domain specific skills. So for instance, allow them to earn a lot of money that makes up for the fact that they don't get along with their neighbors. So it's in some sense, hypertrophy intelligence is often a prosthesis, for lack of in regulation in the same way as high emotional sensitivity can be a prosthesis for a lack of analytical understanding. Yeah, it's basically a question of how do you allocate resources, general intelligence, in my view, is the result of having to solve control problem is that is so general, that requires you to become sentient. And that means that some control problems require you to understand the type of universe that you can possibly be in in the conditions in which you exist in it, and you the way that you work on that universe. So you can improve those factors and factor them into the outcomes of your models, right, so you can control for them. And so, this is a very simple operational definition of general intelligence that in some sense, is feasible, there is a deeper one. That is, I think that if you are building a system like Turing suggests that you want to make a Turing test, and you know that the system needs to be at least as smart as you are. And you are at least smart enough to build a system that is intelligent. Right? So the question that you should ask the system is, how do you build AI? If that system is able to explain how it works, I think it has mastered a certain test, if it explained can explain to you what intelligence is, what itself is it then it doesn't make sense to claim that it's not intelligent,

Unknown 1:48:20
right? And you can imagine these explanations already exist in text form somewhere. So we could have, like, you know, a chatbot, just mining text, and spitting out based on questions like that. And, you know, for instance, like GTP, three, could come back and give you the definition for artificial general intelligence, except maybe reword it in a sort of way and take sort of examples from various different places. Read, Shane legan. Marquez has sort of paper on different sort of, like definitions of intelligence and splits.

Joscha Bach 1:48:53
So you wouldn't be able to ask the system how to implement that. And if it comes back with an implementation that is testable, right? Fine. Maybe it's able to integrate over the information in a way that you could not. But the wrap is, of course, to understand this answer and to check whether it's given the correct answer, you need to understand what intelligence is. So you need to be genuinely intelligent, to administer this kind of general Turing test.

Unknown 1:49:18
But you see the the word general, sorry, the word general intelligence come from psychology, and that way they test performance in humans. And this is very behavioral, in a sense, it's, it's stopping right down the line, you know, the black box approach to what's going on inside of human mind where they're smart enough to be able to, I guess, learn across a variety of environments and solve problems across a variety of domains. So I mean, what I mean that's very behavioral, but it's not really it doesn't really, necessarily indicate that we should be looking underneath the hood, or looking up looking at more of the upstream causes of the behavior of generality. You see you would have the semblance of generality come from just a very interesting chat bot, it could appear general, especially in the limited sort of testing environment of just having a screen with text on it.

Joscha Bach 1:50:15
So let's sit down to the system and discuss the differences between a chatbot and the general intelligence system. So for instance, let's discuss the difference between green and blue in its nature. Now, I'm not sure that Hornet is generally intelligent in the sense when he makes this distinction, maybe colors are a was an aspect of the way in which he makes sense of physics. And if you don't make sense of physics in the, in the in the way as we do in physics, where we'd have quantum mechanics and so on, but in a very simple way, where you want to explain your perceptions, and you can explain all your perceptions via a physics engine that is mostly Newtonian. And that is mapping everything that you can see here and touch on a region in the dynamics V space. And so physics is describing the flow of information through space. And the space is simply a set of locations that are discernible, and that can hold information. And so which information flows and trajectories, right, the structure of the space, the topology of the space is the integral over all the trajectories along which information can flow. Amazonian physics Yeah, yes, it's also true for all other physics, all physics describes the flow of information between locations. And that two different types of information. And it turns out that also in some sense, your feelings describe information, there are additional dimensions to that. But you experience feelings, typically as something playing out in your own body map is the space in the map into this body map to make them distinguishable from each other by our perceptual system. And it talks to your symbolic mind. And you feel expansion and contraction and pressure, and so on. And expansion and contraction pressure all describe the flow of information or derivatives of the flow of information. And then the addition to this have valence, which is a parameter that relates to your need dimensions, which tells you how desirable or undesirable a certain thing is. And you also have color and this color is a stationary property of visual information. Right, and you can describe color as a polar coordinate, it's usually an intensity and an angle. And the angle can play out on the intensity in multiple dimensions. This describes different color spaces as a result. And the reason why it's represented as an angle is probably because it's represented by oscillators, if your neurons are very good at forming oscillators, and so if you want to encode the color space, the natural way of doing so is basically to use harmonics and an oscillator. And that's why colors form octaves, they form a circle. And the same thing is true for sound, there is a similarity between the pitch perception and the color perception. And the similarity is so large that when you are as soon as state, you realize that they are functionally the same thing. They're basically the same functions being used to represent different modalities and different contexts. Both the sound and the color play out in space. So when you are in a state, you see color patterns moving through space, but they don't move in the same way as photons do, or reflectance of photons. So they're in some sense, superimposed in a different dimension in the same space.

Unknown 1:53:37
And part of the reason for that was the parts of our brain which process color and and sound somewhat close together, and some people's are more close than others and have entanglement neural entanglement going on.

Joscha Bach 1:53:52
I suspect it has more to do with the way attention works, you are normally not getting getting exponential access at low level sound processing. And if you do, then you will perceive the colors of the sound. It's also the reason why many drugs have the property that you induce synesthesia, it's not because they change the alignment of the brain areas and move them closer together. It's because they give you exponential access to parts of your brain that you normally cannot get conscious access to. So you integrate these experiences into your mental protocol and thereby become conscious of them

Unknown 1:54:26
that you normally filter out. Yeah,

Joscha Bach 1:54:29
you don't need them. So you basically don't integrate them into the model because they work automatically just fine. And so basically, these these colors are not necessary for distinguishing the sound. You use abstractions over the properties of sound in the most part to make sense of them. Versus when you do vision. The color is often an important distinctive property. Color is what for instance, green is what all objects that can appear green regardless of the lighting have in common and blue Guess what all the objects that can appear as blue have in common are when you correct for lighting. So it's something that is an invariance in objects somewhat independent of the lighting. And this makes it different from the measurement of a wavelength because the wavelengths depends on the lighting. So it's not directly the measurement that your green blue cones do on the retina. But the dimensions of the color space are dictated by the number of different types of color receptors that you have on your retina, right? If you had more than you could perceive color and more dimensions. So the difference between when and blue is in some sense, a relational one, it's the type of feature, this is one of it. It's a stationary information feature, similar to texture that you can touch, but is to find your resolve to, to basically, structurally resolve it, but it's homogenous. And that texture, texture has emoji homogenization that you can distinguish. And it's can be organized in the polar coordinate representation. And it's an embedding over certain types of objects. This makes it very, very distinct. So you will have difficulty to not associate red with blood, because blood is in the context of the objects that can appear red, and it's much harder to associate blue with blood. And the semantic properties. For instance, that red looks like a color that is hot versus blue looks like looks a color that is cold, is precisely because of these dimensions that are associated with the objects that typically are red versus the objects that are blue.

Unknown 1:56:37
There could be ways in which we define the problem or define the category of problems that we need to solve, so that we more efficiently solve them. Not not just more quickly, but more thoroughly and more qualitatively, better. So when I think of general intelligence, I think, well, it let's just focus on the behavioral output of AI. You know, it doesn't necessarily mean that we don't open the hood and find out what the upstream causes are for the general output. But if we sort of focused in and tried to define different categories of AI that we need to solve in order to achieve super intelligence, one of them, which I thought was quite interesting is machine understanding. I'm not saying that understanding in its usage tracks, what it should be if there was an ideal understanding to track to, or what's actually really going on inside our brains as an instance of understanding. But it just sounds like it's a closer common definitional term than generality would be to what the goal of AI, or one of the one of the important goals of AI should be.

Joscha Bach 1:57:55
I think the generality implies that you are able to make models in general, so this uncertain constraints of learnability. And our universe is learnable. Because it's a controllable universe, you should be able to make all these models. And it seems, for instance, that it's not obvious what types of models cannot be made by the transformer. Beyond reaching for, and so on. So it's clear that there are limits in the encoder and decoder. But the type of model that is being made seems to be potentially a general model. And the understanding that we require humans to make is that they are able to make exhaustive sense of the sensory data. So they are able to entrain their brain with a physics engine, like in a computer game that generates a virtual reality that is modeling the dynamics of the objects in the room that are in and how to transition from one room to another and how to generalize over the things that happen in this room, and out to learn how to talk about the things that happened this room, and how to attribute agency to other agents in this room and more of their mental states when they observe the room and so on, and so on. Right? But it's if you look at this, it's just a lot of stuff. It's not there is no miracle going on in the entire thing. And so an important question is, can we make that understanding comprehensive? So are you able to make sense of everything that you are entangled with and all your affordances? And can you make it real time?

Unknown 1:59:25
It gets to be hard to have, like ever, like a real time understanding of everything that's going on at the same time. But that's part of understanding is understanding the salient aspects of reality around you, so that you can achieve your goals.

Joscha Bach 1:59:40
But the interesting thing is what are your goals? And now as the goals are the result of extrapolating or integrating over our needs, over long time spends, when we are an infant, we have only very short reaching goals or not no goals at all just action tendencies. And we form goals when we can anticipate Through future situations and see them as desirable or undesirable. And we can envision the past that get us on the present situation to these future situations, and how our actions are going to influence these past. This is all required before you can formulate a goal in your own mind. And when you integrate over goals over a long time, you can have purposes. And then at some point, you realized the way the world plays out. And this puts constraints on your goals and purposes. So once you have this model of how the world works, in general, you have an aesthetic. And using that aesthetic, you can form coherent goals, right? So you can only talk about ethics. For instance, if you understand how the world plays out at a very long timespan, because your moral intuitions don't mean anything before you are able to understand the consequences of what you're doing. So for instance, I'm unable to judge whether Building Strong Artificial Intelligence is ethical or unethical, because I just I'm unable to see how it plays out. There's also this difficulty that I am part of a world that largely doesn't care about what I think. So my own actions in this world perspective building AI are not going to make that much of a difference. And it's much more interesting for me at this stage to understand what's likely going to happen and what the consequences are. And one of the main reasons for me to be close to the people that build AI and try to contribute to this is, I want to be very close to this epicenter of the most interesting philosophical project that humanity has ever undertaken.

Unknown 2:01:27
Absolutely. All right, me too, which is why I'm interviewing you, by the way, which I find your stuff is so interesting. Look, just like it. We've been talking for a while now. But just a couple of questions. I thought I might sort of lay on you. I'm going to be interviewing John Horgan, have you? You know him? Have you read his book, The End of science?

Joscha Bach 2:01:49
I don't buy it. Do you think there will

Unknown 2:01:51
ever be an end to science? So do you think it will be continuing continuous process and

Joscha Bach 2:01:57
I think there can be an end of science in the sense that you can find the set of algorithms, the optimal search for truth. And once you find that algorithm that optimizes the search for truth, in the general case, there is nothing that's left for you to do. But to follow the algorithm, which is probably very boring, you can leave this to machines. And so scientists will or go to the beach. And the remaining sense will maybe be an artistic project where you try to make sense of what the machines have been doing, or an employment program. But this sense, you would reach the end of science, once you are automating the things that people are doing. And you're able to do this on a level that is out competing people at every level. I think that's conceivable that if you get there, there is also diminishing returns in science. And I think this is more what John Horgan is referring to, and I think this relates more to the end of modernism. This means that we no longer incentivize academic institutions to do actual science. Instead, we incentivize universities to pay people for applying methods. And this has diminishing returns.

Unknown 2:03:09
And sounds like engineering, though, more than

Joscha Bach 2:03:11
it's not exactly the same thing. An engineer is being paid to get stuff done to make things happen. Applying. And yes, and a scientist is paid for producing papers that get published and for reviewing papers that others want to publish. And the research that you're doing is more or less instrumental for this, because we have inverted the incentives. And the way that we are hiring people, we don't hire people, because they're dedicated to fascinating questions, and they have mindset are uniquely disposed to work on these questions.

Unknown 2:03:44
Medicine is such a big topic I'd love to discuss at some stage. Yeah,

Joscha Bach 2:03:48
yeah. But for the most part, science is treated as an employment program at the moment.

Unknown 2:03:55
What currently untestable theories that excite you that may be testable with future technology or advanced, you know, what, which ones excite you that we can't prove are real now that we can't sort of falsify even do you think that at some point in the future, or practically falsify me, because we just don't have the tools to measure or to investigate the universe at the level we need to? So what currently untestable theories excite you that may be testable by science in the future?

Joscha Bach 2:04:31
One of the most interesting things is in physics. So foundational physics is still unsolved, right? We don't have a unified theory. And the question is, if we look at the history of physics, that's tech 160, and 70 years old only, and is physics really much, much harder than go. Go has been played for more than 2000 years. And when we set the machine learning system on this, it's able to discover in the space And of ours, that humans have moved at a local optimum and solving goal. And it's possible to get out of this local optimum and play better golf than people ever could. But this is big, can we build a system that is able to play better physics than people ever could. And I don't see a reason why that should not be the case. Right. So if you build a machine that is trying to find unifications, that give predictions that are compatible with the standard model for some epsilon that is already known. And you know that the standard model is correct with some epsilon isn't within some boundary, we find a more compact representation about what's going on there. And I suspect that this is the case. And I also suspect that the family of models that we are going to find are going to be computable models, which means automata model of the universe. And at the moment, computational physics is like initial discipline. A few years ago, I did a small survey among a group of foundational physicists at a conference. And I think about 6% of felt that they were completely committed to this idea of computational physics, which means that was already a growing trend. And it's I think it's going to continue to grow. And again, I might be wrong, maybe my understanding of girdles proof was wrong. And Penrose idea is correct. And Penrose believes that human minds can do things that computers cannot do, and physics need to enable human minds. So physics needs to be able to do more than computers can do. And maybe consciousness is enabled by the parts of physics that computers cannot do. And of course, everything that we know in physics is computational. Except we don't have a good theory for quantum gravity. So Penn was gets drawn to quantum gravity when he tries to explain consciousness because it's the only part of it is unknown. But what is the chance the quantum gravity is the non computational think, except if you understand that, basically, mathematics is the domain of all languages, and computation is the part that works. quantum gravity works to write needs to be computational at some level. So to me, this foundational physics thing is super exciting. And I suspect that we will see deep learning approaches to physics in the next decades that are going to possibly make big progress. Now, the question is quantum computing, quantum computing has been around the corner since the 1980s. And in some sense, we now have experimental quantum computers that under some circumstances, are able to make computations about quantum mechanical systems. So they mostly compute quantum theory, which is a little bit fishy, right? They don't do really things that you cannot do with classic computers, except for describing a theory that explains the difference between quantum computers and classical computers. Maybe the issue is in the baby formulate a theory. Right? So maybe if we fully discretize, the time dimension to quantum mechanics, what happens then? And I don't know that but the idea of quantum computers in some sense that the particle universe is inefficiently implemented on the quantum universe, quantum computers are not possibly breaking the church Turing thesis in the sense that all computational systems are equivalent. If quantum computers are able to perform things that are the different computational class than what physical computers can do, it just means that there is a substrate universe that runs more than polynomially faster than the particle universe. So basically, it means the particles are inefficiently implemented on the substrate. And quantum computers, the idea is that somehow we can bypass the particle universe to directly access the substrate, not fully, but at least to such a degree that we can harvest some computations from the substrate that could not be done with particles. Yes, right. With particle computers. This is the idea and it's so far and and tested, or an open question that is being in the process of being tested. We don't know that it's very exciting. But I'm still willing to bet against quantum computing at this point. Because I think the people that are championing it is strongly incentivized to, to say that it's possible because all their research and funding depends on it. And if you look at the funding that they get is relatively small. If quantum computing works, it would be worse, many hundreds of billions of dollars. And they don't get that funding right now, which means that the bet that even Google is making this is relatively small, given that the funds that Google has available in the piggy bank is not using. So it's basically humanity makes a relatively small Batum quantum computer and despite our Scientific American reading, that it's pretty certain that it's going to work.

Joscha Bach 2:09:32
But also think that the Scientific American is not employing that many modernist minds that basically are able to make the full march to the foundations and back, okay, and any kind of understanding and every generation requires that you understand your own foundations, and most people don't they hallucinate the foundations that are battling. And as long as they're babbling in the same way as the others do, and nobody sees the difference they get along have this gptc Like output, which is very cool. Turning to me. Right? So now

Unknown 2:10:06
I've often thought that would be very, could be a reason why we don't see that many advanced civilizations out there this because they worked out how to sort of tap into a more efficient way to generate computation than this universe can afford.

Joscha Bach 2:10:21
And there are multiple reasons why this could be the case one is the most obvious one, maybe life is very rare and is not infectious. Another one is, if life emerges on the planet, it typically takes many billion years until it moves to reaches a certain level of complexity and has terraform the planet to perform large environment for performing that kind of evolution, that it would build the technological artifacts that would make actions of life beyond the planet visible. And the third theory is the dark force theory, which is also very plausible, right? As soon as you understand that life is likely and probable and abundant, you will shut the fuck up. Because if somebody else notices you, there is going to be always a shortage of habitable planets.

Unknown 2:11:08
But the idea that like there's such a strong attractor, to piping into something more fundamental about, you know, the whatever substrate, this universe sort of floats upon this inefficient universe that we live in. Maybe the strong attractor is that there's a better way to compute outside of this universe, and that we find a way to sort of pipe into that and do something more interesting

Joscha Bach 2:11:35
somewhere. And of course, it means that it's not outside our universe, it just means it's our understanding of the same universe that allows us to do more with it, right. So in some sense, it's outside of the particle universe. And the particle universe is outside of the exponential phenomenal universe that we have paid, you cannot see electrons, but we can interact with them. And you can build machines that rely on our understanding of electrons. And now, we would basically go beyond that.

Unknown 2:12:03
But that if we could do that, we wouldn't need to sort of exist in this sort of like experience or this particle plane, or we'd just go somewhere else, where we could sort of do things a lot more efficiently, we wouldn't need to worry about the heat death of this particle universe or anything like that.

Joscha Bach 2:12:19
You don't need to worry about the heat. Because we don't exist when the heat has happens. We have exist only in the heat right now. And we don't exist in physics, we only exist in the dream that is generated in the skull of a primate in some physical universe. But we never get to visit this physical universe. All we visit is our own mind, we only see the VR generating our own mind. And we only see the now. And the thing that you are is our this is just an estate and an information processing system. That things that remember its past states and identifies with the trajectory of the past states. So imagine you upload yourself into some kind of artificial intelligence that then goes on to conquer the stars, is that thing going to choose to identify with Adam? I don't know, right, it's only going to identify with you and its history about you, rather than treating this as many as an origin story, because it's so far removed from it as you are removed from microbes, right? This is only going to identify as Adam, if it's useful to edit things, it's the best identity it could possibly have. And this also applies to all the other ais that have ever be built, right, some of them might choose to identify as you if they think that's the best identity that they could ever have. And the probability that the ones that you've been uploaded into that came randomly about is almost the same. So it's unlikely that you will exist in the future, because you must, at some point, not choose to remember yourself as being a human because you just are not anymore.

Unknown 2:13:47
Sure, I'll become post human. And there'll be a pattern, it's so far removed from what I am now that it seems weird that I can consider the identity of Adam in its current form.

Joscha Bach 2:14:00
And it has something to do with the notion of identity as we operate on it. Right. So once we transcend the notion of identity and realize how it's constructed, I think we lose lose the idea of being afraid of that because our participation in the universe is no longer bound to a particular identity.

Unknown 2:14:16
Yeah, well, that I guess that you know, thinking that way does relieve a lot of concerns, but still they haunt me Oh, it's been wonderful speaking to you, again, very thoughtful and enlightening discussion. Every time I speak to you, so it's always a pleasure.

Joscha Bach 2:14:37
Have a wonderful day in Australia. I will have a nice evening I drive. My family is eagerly waiting for me to return from Australia.

Unknown 2:14:50
Think around the background, perhaps cooking for dinner for you or something here. So it's been fantastic. Yeah, I hope to bump into again at a conference sometime soon.

Joscha Bach 2:14:59
After depends. Yeah, that would be nice. Let's hope COVID is over in a few years and civilization is still standing and we have still air traffic and so on international conference would be awesome indeed. Thanks. Thank you. Bye bye

This transcript was generated by https://otter.ai