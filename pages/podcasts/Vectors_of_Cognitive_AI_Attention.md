Unknown 0:00
Okay, hello, everyone.

Unknown 0:01
So, today we're holding our panel on attention in artificial intelligence, and cognitive science and neuroscience. So, the seminal contribution of attention is only needed by a bus finding at all. Which introduced transformer algorithm has triggered a small revolution in machine learning. And unlike Convolutional Neural Networks, which construct each feature out of a fixed neighborhood of signals, Transformers learn which data feature on the next layer of a neural network should attend to. However, in neural networks, it is very different from the integrated attention in the human mind. In our minds, attention seems to be part of a top down mechanism that actively creates a coherent, dynamic model of reality and plays a crucial role in planning, inference, reflection and creative problem solving. Our consciousness appears to be involved in maintaining the control model of our attention. In this panel, we would like to discuss avenues into understanding of attention in the context of machine learning neuroscience, cognitive science and future developments in AI. Our panelists today are Michael Graziano, Vasudev, wild Russia, Bach and Jonathan Cohen. Our first presenter will be Vasudev well, so laxative Lau is a research scientist at Intel Labs where he leads the multimodal cognitively it, he and his team develop AI systems that can synthesize concept level understanding from multiple modalities such as vision, language, video and audio. His current research interests include equipping deep learning with mechanisms to inject external knowledge and self supervised training at scale for continuous multiple for multiple continuous and high dimensional modalities like images, video and audio. And last of the four is no yours.

Unknown 2:05
Thank you, Diane, for the introduction. So hi, my name is Russell Leyva. And good morning, good evening, good afternoon to everybody. I'm an AI research scientist at Intel Labs. And my talk will be representing our team, the multimodal cognitive AI team. And I'm going to be talking about attention in the context of multimodal transformers. And I'm very excited here to get viewpoints of attention from the neuroscience and the psychology domains. So this is a rough outline of the talk, I want to focus on how multimodal fusion is brought about by key value query based attention, which has become really popular in machine learning. And it's exploited by transformers. And I would like to show how we can end up read this attention and machine systems quite quite a bit through interpretive examples of image to text and text to image attention components and multimodal transformers. And finally, I'll end with, you know, some of the challenges with current AI attention methods. We know they these systems that have come into being over the last couple of years are really powerful. But still, you know, there's a lot of problems that these systems don't extend to. And I'll try to dwell a little bit about what some of these challenges are. So, consider this rather cute image and accompanying caption of a dog and a cat playing the goal of multimodal fusion is to get the representation of the image of this dog and the representation of the text dog to get to the same match up to a same a model representation. If this is successful, you know this should happen for all other entities alike, you know, the representation of the cat of this image should be matched to the representation of the of the text card, again to an A model representation of the cat. And this can also be referred to as concept level vision text alignment. Such concept level vision text alignment can help accrue knowledge about the concept from multiple modalities. So an a multi modal representation of a dog can capture all the information about dogs and text corpora that the model has been trained on, as well as all the visual information about a dog and all the images that the model is trained on. So attention mechanism and transformers have had a disruptive impact on natural language processing and understanding since the first paper that came out on attention. That's all you need and around 2018 And since then, we've had many transformer architectures like Bert GPT three The five that are leading all benchmarks in the NLP field today. More recently, transformers have also become state of the art across tasks and multi modality. And some of these tasks are image captioning, visual question answering multimodal semantic search, and image and text retrieval. Now one reason that transformers are so effective at these multimodal tasks is because a transformer is contextual attention mechanism has proven to be a highly effective mechanism at fusing a representation of tokens that are originally coming from different modalities. And a common set up as that you have image and text tokens that go into the input of a transformer. And then through the Transformers contextual attention. These the representation on these tokens is transformed into this a model representation space at the output of the transformer at the output of all the attention layers.

Unknown 6:06
Now, we'll actually try to see how this multimodal fusion happens using the popular paradigm of key query value attention that has been developed in machine learning systems. So for simplicity, let's assume that there are four tokens to get back 10 images and to the pectin cat, text of dogs and cats. And the first step is computation of query key and value vectors from the embedding of these tokens. Now, let's say we are really interested and computing the self attention for the dog text token and evolving the representation of the dog text token. So for this, we need to have the query vector for the dog text token. And we need to have the key vectors for all the other tokens. And the next step, and the process has a simple dot product between the query and the key vectors. And the resulting calculation is scaled and normalized using a softmax function. And what we obtain is our DS attention scores. Now, we can now write a new representation of the dog text token as a weighted sum of all the value vectors of all the other tokens were these weights are referred to as attention. And especially, we see that these coefficients a three, one, a three, two, this is image to text, multimodal attention. So essentially, what we see is that the updated representation of the text token dog will have contributions from the image token of dog as well as other image tokens as well as other text tokens. And these weights, a three one and a three, two is cross modal attention image to text attention. So already we are seeing how this type of attention is going to help fuse the representation of image representation of dog and the text representation of dog. Now, these image to text attention coefficients can be visualized as a heat map on the original image. And just for illustration, we might get something like this where the a three, one coefficient is high, and the 832 coefficient is low. If this is the case, we will actually see that the new representation of the text token of dog has a heavy contribution from the image token of dog. And this is how this kind of attention and machine systems has become really effective at fusing the representation of the same concept in different modalities. Now, we'll see some real world examples of this image to text attention. And I'm using a pre trained multimodal transformer that has been developed by a team at Intel Labs in collaboration with Microsoft Research. And rather than just looking at one particular word, we are actually seeing how this image to text attention changes for a sequence of words. So we see that the attention is on the woman when the woman has mentioned and the text then on her break first. And then finally it goes to her working on the computer. This is one more example where we can see the attention focused on the heart of the man and then the guitar and then finally on his face and especially close to his facial features, which show that he is smiling. I'll show this example. These are actually two examples. The image is the same but the text is different. To show how contextual this attention is. So in the example above, we see the attention is initially on the on how miny because she's mentioned on the text, and it remains close to, you know her neck, since that's mentioned in the text. And in this example, below, we see that the attention is actually on, you know, Harry Potter, since he is mentioned in the text, and especially his eyes with looking at, and, and then it shifts to the window here as the window is mentioned in the text.

Unknown 10:39
Such models can also be used for question answering visual question answering. So I've taken this example, which actually comes from neurips competition that was held in December in which it made it to the waiting list. And this is one example of the type of question that you have an image of, of this bird. And the question is, what color is the ring around the eye of this bird? And this system needs to generate an answer in natural language. And for this example, our system generates a fluent answer and gets the correct answer read. And interestingly, it's able to interpret the question correctly, you know, there's a, there's a small red ring around the black eye. And if we look at the image to text attention for the word color, we see that the attention is in proximity to the eye of the bird. Now, in a similar way, we can consider an image token. So let's say we, we, we consider the self attention for the image token of the dog. So just like in in the previous example, we'll take the query vector of this image token of the dog. And we'll take the key vector of all the tokens. And the result in dot product will be scaled and softmax. To get these attention scores, so now we can write and update the representation of the image token of the dog as a weighted sum of our value vectors of all other tokens. And specially these coefficients, a one three, a one for our text to image multimodal attention components. And just like how image to text attention components can be visualized as a heat map across the image. These can be visualized as a heat map across text. And I'll show one example. So given an image like this, this particular image which comes from a movie scene, and and a corresponding text sentence, it's actually a question and answer related to this particular image, we can see the text to image component for this particular image patch. And we can see that the heat map is highlighting the verbs, which are associated with this object with the woman. And this kind of attention will actually tried to make the representation of the woman and the image closer to the representation of the word thinking and wondering. And to me, this is really fascinating, because thinking and wondering, are very abstract concepts, which you know, don't really have any, you know, into your computer vision of your vision. These concepts don't exist. But in this multimodal problem through diffusion mechanism of attention, the representation of the woman will, will be made closer to these abstract concepts of thinking and wonder. And again, this is brought about by the role that attention plays in multimodal transformers for fusion, different modalities. So, we've, you know, we've I've tried to illustrate how attention is very effective at fusing modalities to this a model representation, and especially this key query value attention paradigm, which has become very popular with the transformer architecture. And we've seen how the representation of text tokens depend on image tokens through the image to text attention, and this can be visualized as heat maps across the image and how the representation of image tokens are dependent on the representation of the text tokens. And typically, you know, attention is trained in a self supervised way in the pre training stage of a transformer. And what we ideally expect to happen is Add the various embedding vectors which we learned in such a way that they will be more emphasized on, let's say, the a three, one coefficient here, and a one three coefficient here. So that we really start having proximate a very similar representation of the same concept of dog in both text and an images. So, so far, you know, I've shown some attention visualization. And the visualizations I'll show now are aimed at actually showing that these representations move to a common semantic space which we can consider to be a model.

Unknown 15:40
So what we are doing here is we've propagated and a population of examples through our multimodal system. And we are taking the hidden state representations which are in high dimension and projecting them into a 2d dimension for visualization using the T Snee algorithm. And what we see is that before any attention, the text tokens and eminence tokens occupy very different non overlapping spaces in this representation space. But then, after going through the transformers, contextualized attention mechanism, we start seeing some intermixing, we start seeing some of these blue points and the yellow points getting pushed closer together. Now, this doesn't happen to all the image tokens and all the text tokens because not all words have a corresponding pair in the image. And vice versa, that not all parts of an image will be mentioned in the Word. But if we zoom in a little bit, we'll start seeing for individual examples. For example, we see that the representation of the word card is pushed really close to the representation of the image of the card in the system. And this is, like I mentioned, at the beginning, if this is successful, this really helps the system accumulate information about the card from various modalities. So whatever the system might have learned about cards, through textual data, through all of its training on text, corpuses, as well as whatever the system might have learned about images of a car, this is

Unknown 17:26
notice for massive, okay,

Unknown 17:29
it's all tied to one common representation space. So I briefly mentioned that just like attention is very helpful infusing different modalities. Recent work has shown that attention and machine learning systems can be very effective at fusing external knowledge with neural networks. So I'll end by just showing some challenges of this framework. So the K QV, attention scales as a quadratic function of the sequence length. And this is highly problematic, even for language for long documents of books. Unfortunately, all language most language benchmarks are short sentences. So transformers end up doing really well. With this is a major problem as you go too long documents or book level corpuses. And this is obviously a huge problem for multi modality, where in video without audio or transcription, this will scale quadratically with both space and time, and current transformer systems do not really scale to videos have on even a few minutes. Whereas you know, humans can very easily absorb the content in a three hour movie and be able to answer questions about it, and so on. The other key point is that the paradigm of all to all attention does not really play well with entity specific representation. So the way the transformer architecture work out is that sequence lengths are conserved through all the layers. And the sequences are made made out of individual tokens that are very atomic, they represent individual words or small image patches or short audio waveforms. So they're not really very consistent with, you know, how humans started regarding entities and the world as they as they encounter them. And then, you know, attention is really learned on its own during self supervised retraining of transformers. And it's quite common actually do see attention patterns, which will not seem intuitive to humans, and potentially some interesting work here can be more supervision of attention. And at least in the language domain, there has been some work previously and inject things in syntax dependency relations, directly into attention and so on. And some of our work is also aimed at supervising attention in a way so that it becomes more and The printable. Alright, so those were on my slides. Thank you for your time.

Unknown 20:06
Thank you, Macedon. That was a great presentation. So, next up we have Jonathan Cohan, Jonathan gone is Robert and Lynn Bentheim. Professor in neuroscience and co director of Princeton Neuroscience Institute. Professor Cohen's research focuses on the neural mechanisms underlying cognitive control and their relationship to the human capacity for general intelligence. His work seeks to develop formally rigorous and mechanistically explicit hypotheses about the functioning of cognitive control systems in the brain. And to test these hypotheses in empirical studies. Professor Cohen holds a BA in biology and philosophy from Yale University, and MD from the University of Pennsylvania, and a PhD in cognitive psychology from Carnegie Mellon University. He joined Princeton faculty in 1998. He has been conferred the highest awards for research in psychology, including the American American Psychological Association's Distinguished scientific Contribution Award, and the William James Fellow award from the Association for Psychological Science. Professor going the floor is now yours.

Unknown 21:16
Thank you. We make sure I can. My good, my side.

Unknown 21:22
Yes, we can see your slides. Your voice is a little bit quiet, though. Let me see if I can turn my mic up. Sorry about that. Audio Input fine. It's not letting me control it. It's not too bad. Maybe you could just a couple. Is this better? Can you hear me now? Okay. Yes.

Unknown 21:50
Sorry about that. Okay. So I really enjoyed your talk. And I'm not sure what I'm really going to add of substance, I think you sort of covered the ground in terms of where we are with the state of the art of what I have to say, and covered grand the things that I'm not going to talk about, but totally interested and engaged in. So maybe the way to frame my talk is sort of a deconstruction, it's less technical, but it tries to make a simple and fundamental, those subtle point that attentional control and semantics are really made of the same cloth. And this is a point that I think gets obscured it's gotten skewered certainly in psychology and neuroscience based on different experimental approaches that have been taken to attention, or control and semantics. And to some extent, I think it's hard sometimes to figure out what's going on in complicated models, like the transformer, the neural Turing machine, where this is definitely the story. But it's all sort of one big jumble. And it's hard to know what's going on. And so maybe if nothing else, this talk will help sort of lay some simple conceptual foundations for what's going on and why these things work. So well. Given I think that it's a model for what goes on in the brain and why it's doing so well. It's capturing many of the things it's not everything that the brain does. Um, here's my position statement, I were asked to write it, so I figured I would read it. This is an totally attention. This is about attention. So this is an intuitively accessible construct that has been equally ephemeral, equally ephemeral, when subjected to close empirical scrutiny, certainly, in psychology, neuroscience, work using neural network architectures to develop models of tension and the closely associated construct of cognitive control that address empirical phenomena concerning human performance suggests that there's an intimate relationship between attentional capabilities, and the structure of the representations both on which it depends and over which it resides. This relationship is an instance of a broader, more fundamental relationship between the statistical structure captured by semantic representations and the capacity for control relationship that is productively informed by studying the kinds of representations that develop in neural network architectures, and how control operates in such architectures to guide processing. So I think, hopefully, that captures, or gives you a sense of why I feel so aligned with what Vasudev said. And again, what I hope to do is sort of bring it down to simpler terms and provide at least a little bit of insight. So here's the canonical example that at least I use in my work for attention, it's the stroop effect. And I won't make anybody do it, because it won't be very interesting, since nobody can see anybody else or here, then. But the bottom line is that it's a stimulus representative of many such situations that has at least two dimensions that are compelling, but one is more compelling than the other. The word and when you ask people to simply respond to the stimulus, and don't tell them what to do, except to say something out loud, they say green. And yet if you tell them please name, the color, they're capable of doing that. In fact, that's one source of evidence that the word reading responses sort of pre potent and that you need attentional control to attend to the color dimension and sort of filter out the word in order to respond to the color and that effect. act is powerful enough so that with just a couple of trials, usually on the first trial, you can do it. If I were to say to you name the color, you would have said, red, but you were passed, you were in a room, right. And even after a few trials, you might not earn, but you would take reliably 150 250 milliseconds longer to name, the color here, then if the word itself was red. And so even though we had this strong capacity for control the ability to override that pre potent response, it's not complete information leaks through. And that's a really important observation, not just the guard to attention, but as we'll get to it later in the talk with with some indicator I think of of semantic influence. Okay, so let's start with attention and control, though, and what might it look like in the neural like architecture of the brain itself.

Unknown 25:50
And this is a model that's been around for a while, it was one of the first things I worked on. And it simply captures the idea that there are these two pathways, one for word reading. Sorry, I got it. Can you see my cursor? Okay. Yes, okay, good one for word reading, which has thicker lines, because it's presumably more practiced, and more consistently practiced. And so it captures the observation that that's the pre potent response. And then a mapping also, from colors to words. And, you know, these could be brain areas, this could be visual word form area, this could be v4, some associated areas, and then a area in, you know, motor cortex that's responsible, or, you know, maybe anterior temporal, that's responsible for creating phonological codes, it's going to lead to a motor response. And, again, the basic observation is that if you present it with the stimulus like this, you'll process both dimensions, you might get some processing of the color, but the dominant process will be the one for the word, again, because it's more practice. And if there's any competition among these, then you'll say green, most of the time, not red. And yet, if I name if I asked you to name the color, you can, and so something else has to be here. And the idea is that there's something else here, we might imagine that this is the role of higher multimodal areas that represent sort of categories of information, you know, maybe parietal, inferior temporal, or prefrontal in this case, we think that this might capture the most important function of the frontal lobe because it needs to be maintained over many trials. The bottom line is that you have a representation somewhere in your head, that is of colors that lets you prime, the color naming pathway, or the associative representations that associate frequencies representations of frequencies of light with phonological codes wording, such that you can overcome the otherwise proponents of this pathway. And this, I would argue, is a the simplest connectionist model, and the simplest connectionist model of control. And I won't take any time here to go into the data that this very simple model seems to do a good job of accounting for but it's it's it's voluminous. The key point, though, is that this operation is really no different than this operation. That is the influence of this representation on knees, it's no different than needs, it has the effect of biasing these units here. In such a way that you get out should have done this or such that you get greater flow of activity through this pathway than this one. But it's done through a very simple mechanism through the sort of run through additive input, transform through a non linearity that effectively acts like a multiplicative connection. Okay, so without having to implement multiplication explicitly, we just exploit the fact that they're nonlinearities. And all these places, when you add this extra influence here, it takes this unit from a place where it might have ordinarily been inhibited. So here's actually a sort of more specific representation of what's going on, you have these inhibitory biases, presumably, or some sort of tonic inhibition on the system that stops things from happening that you don't want to happen, then you send activity to these that offsets that and the net effect is to make them bring them to a more sensitive part of the non linearity so that they can be processed more readily. And this idea, as simple as it might be in a connectionist model comports pretty well with what we know about the neurobiology of attention that I won't go into. But all the evidence is that that attention acts in this sort of multiplicative way. And this is a way that you can get it essentially with additive biases. Okay. So that's sort of a quick whirlwind view of control. The other thing I'll point out before I sort of turn the corner here to semantics is that, you know, you can think of these as the task instructions, or the task demand, which is the way I thought of them when I first constructed this model, but really, they're just abstract representations of a category in this case of a set of stimuli that happened to share a particular functional form of color, right, represents the frequency of delay, and this one for orthographic form, but I could have easily labeled this context, attention, intention, control or instruction. It's all the same Same thing, the main take home message is that control in this view is simply the impact that one kind of representation, often at a higher level has on selecting the relevant information for processing at lower levels of representation. And there's nothing special. And that's why I took the pains to say that this is an adder bias. These weights are exactly qualitatively mechanistically, the same as these right? It's the non linearity and sort of the pattern of conductivity,

Unknown 30:25
ie semantics that does the work. Okay, so having said that,

Unknown 30:31
let's turn the corner and note that I could call this a semantic dimension, right semantic category representations. And in fact, the form of this model is pretty similar to the form of the very first deep learning networks, namely Jeff Hinton's family tree model, and Dave Romo heart, Devil heart semantic network. Now connectionist networks are certainly very, you know, comfortable with semantics. Or put another way, semantics is no stranger to connectionist models. And I won't make this point, I think this is sort of an obvious one, the point I really want to make, is that right? So these, these hidden units have semantic structure, I think Bozidar showed you a little bit of that analysis of his hidden units. And what I really want to make, though, is that in these networks, there's some representation, usually the context or the task, and if you convolve that with it, or if you add that it really has the same functional form as the control model that I presented initially. But as you can think of this as a network that has much more greater forms of representations, but still has a control like architecture, the task, or the relation, in this case, that you're asking it to sort of report is, can be thought of as an instruction or context, just like name the colors or read the words, the only difference is that here, the category structure is rather discrete, right? These are orthogonal categories. And here, it's much more graded, overlapping and richer in some in statistical structure. But the point is that the organization the this sort of fundamental architecture is the same. So

Unknown 32:02
right, so the fact that we call one

Unknown 32:05
an attentional paradigm or an attentional task and talk about it as reflecting attention, the fact that we call the other semantic is really just a sort of an emphasis on the phenomena in the context of the conditions under which we're studying it. So attention is sort of typically thought of as the selection over discretely dissociable orthogonal representations. And that way, because it's easier to study, right, it's easier to see what it is you're controlling, to look in the brain for the different representations right before we had MV PA, that is multivariate ways of looking at patterns of brain activity to tell things apart, we have to do it locally mystically. But it's really the same sort of idea, as in semantics, the study of categorization, only in this case, in higher dimensional, often overlapping representational forms, you know, animals and plants and instruments and vehicles rather than colors and words and locations and sizes.

Unknown 32:56
I think it's, you know, that, if you now it's sort of zooming back and asked, well, what are what is sort of the overall typography of these representations, they obviously vary along important dimensions. They're different domains. And so you can think about control in this in this simple model, I focused on the role of control here. But in fact, for the person to do this test, they had to know it was a verbal response. So they had to play the same game with a with a representation of us my mouth, not my hands to activate the phonological code, as opposed to some motor motor code that's going to control my hands, right. So there would be some sort of motor representation that corresponds to different categories of of outputs. And similarly perceptual, right, I might actually select which perceptual dimension visual as opposed to my auditory, I wasn't listening in this task I was looking. And so there are different domains over which these control representations or context representations have to be sort of be laid out. And then similarly, there's different levels of abstraction. So in this case, it was colors, but it might have been anything visual, or other forms of sort of abstraction. And I'll get to that in just a moment. So really, you know, semantics is all of this and how it interacts with what you want to do is control. And they're really the same thing. And I'll come back to saying a little bit about what might be specific to control later. But the key point here is that it's really built at the same cloth. Another way to think about this is that attention, and control are just really the warping of semantic structure. So if I asked you to distinguish animals from instruments, there's a clear plane of cleavage that you can, you can use, and obviously, classified networks can learn this too. But if now I say, tell me what's big, and what's small, you totally rotate the whole space, right? And the idea is that those control representations in this case of size, are reshaping through the sort of nonlinear influences or the nonlinear effects of additive biases and the right places to totally reshape the semantic space. And we can think about therefore control is system shaping the capacity to shape semantics that means that A, you have to have the right cleavage planes or the cord sort of organization in the semantic structure itself. But that's exactly what the rumble Heart Network shows you can be learned, right? And then you have to have the control representations that build on that and know how to sort of activate the right produce the right influences that segregate or or align or what space in the right way. And I don't have time to go over the experiment, because I suspect I'm getting close to my time here. But recently, we've done an experiment that tried to sit right in between an attentional experiment in the semantic experiment, and ask what happens if people do a Stroop like test where there's two sources of information here dissociated one is word and the other is picture, as is often done, in fact, also in some semantic tests, and we asked people to judge the size of these objects, and then the end it was of the object that was in the Word. And then we had a distractor, that could either be from the same category and or the same size, right? And so that's what this two by two shows here. And I'm not again, I'm not going to go through all the details. But what we found is that when all the stimuli were interleaved, that is they always had to do size judgments on the on the word, but sometimes they saw animals, and sometimes they saw instruments, there was a pattern of results, that was distinctly and statistically different than when it was blocked by either only looking at all animals or only looking at all instruments. And what that tells us and I'd be happy later in questions to unpack this in a little bit more detail. I'm sorry, I'm going through so quickly. But the qualitative point is that what this tells us is that when they had to judge size, they had a representation of size that was sort of that was that was independent of whether it was animals or instruments that were looking at it just sort of the canonical notion of size. But when they could exploit the fact that it was only animals they were looking at, right and had to respond to not I shouldn't say looking at because there were there were other distractors. But when they were only asked to judge animals, they could focus in and in effect, it shows that they have an animal sighs representation, sort of a rotating corner of size space that's specific to animals, and that exploits the correlational structure that animals have with respect to size that instruments don't, hey, you can ride big animals notice. Great, I'm almost done. Thank you. And so this just shows that if you look carefully, you can see the semantic effects on control. Okay, blocked and presumably means that you're invoking a different control signal than you can an interleaved, you have the chance to sort of invoke.

Unknown 37:39
Okay, so. So basically, I just want to summarize by saying that the same learning mechanisms and representation of statistical structure are involved really in both and that structure can range from simple to complex from specific to abstract. It's just as sensitive to statistical relationships involving action and perception. I think residents pointed to this briefly. And I will say this is something where I think there's a lot of room to be done. Most of the work in semantics has been on perceptual and sort of inferential semantics, but affordances Gibbs Sonian, you know, influences are just as important and I think are a rich place to look at the connection between semantics and control. Control simply reflects the use the selection of information relevant in particular contexts and or for particular purposes along the semantic dimensions. And representations used for context and purposes, however, may require specialized processing apparatus in some situations. So if you have to maintain a representation over enduring period, and more generally, that happens with abstract representations than specific ones in Stroop, I have to keep colors in mind, red, green, blue, and orange are changing from trial to trial. But colors isn't. Right. So the more abstract information the more long, the longer it's likely you need to maintain it. And that may require special apparatus than simply the registration of a stimulus. And then this sort of replacing it with the next. And that may therefore invoke certain mechanisms, fracture maintenance and sequencing, such as LSTMs. And, you know, get into tractor mechanisms, and binding mechanisms for rapid acquisition of of novel sorts of configurations of categories that may be needed in a particular circumstance. But that said, it's still the case that semantics and control are inextricably intertwined. And that in all these cases, you can think of attentional control as simply a parametric warping in this high dimensional semantic space. So with that, I just want to end by sort of visually, pointing to all the people that I think have been key in this work. The people that for me frame the question way back when and I think are really at the heart of everything that's going on now in this space, Dave and J. And then the various people that I've had the privilege with him to work, including Tyler and Sebastian, who have been sort of pioneering this particular line of work in my lab and Michael operators Tim Rogers and Declan Campbell on this semantic study. And then here are a few citations for anybody who might wish to read about this in greater detail. Thank you. Thank you so

Unknown 40:11
much, Professor Khan. So, next up, we have Michael Graziano. He is a professor of neuroscience and psychology at Princeton University, also a writer, composer, composer and an occasional ventriloquist. Michael is the author of many books, both novel novels and your science books, and has written for the Atlantic, the New York Times, the Wall Street Journal and other media outlets. I believe he has some of his books in the background behind him is research of the Princeton neurons Neuroscience Institute has spanned topics from movement control to how the brain processes the space around the body. His current work focuses on the brain basis of consciousness, he has proposed the attention schema theory of consciousness, a mechanistic explanation of how brain based agents believe and insist they contain consciousness inside them, and how that self model is useful for effective functionality. Michael, the floor is now yours.

Unknown 41:13
Thank you. So hopefully everyone can hear. And also my slides are showing up. Okay. Yes. Excellent. So thank you all so much. I'm very happy to be here virtually. And I'm going to talk to you about attention, and consciousness. And I'll tell you about a specific testable and buildable theory. My lab has been working on it for about 12 years now, at the conceptual level, the experimental data level and the computational model level, and in 15 minutes, I can give you only the broad strokes, not the details. The theory is at its heart, very simple. It says that the human brain, possibly the brains of other animals, constructs an attention schema. We've heard in two excellent talks about attention on a representation. But what about a representation of attention. And that's the central idea here, it's a simple idea with large consequences. And when I began this work, the idea was alien. To many people in my task ever since has been to test this proposal in humans, test it in artificial intelligence, and try to convince people of its utility. So what the heck is an attention schema. And I'll start with an analogy, another schema constructed by the brain that I have studied in the past that has been studied for more than 100 years, the body schema, your physical body, is represented in the brain by a bundle of information like a simulation, it's sometimes called the body schema, it represents the shape of the body keeps track of movement makes predictions, it's necessary for the good control of movement. That's a general principle of control engineering. Any control system needs a model of the thing that controls the same brain networks participate in a double purpose looking at someone else, and intuitively understanding the other person's body. And here's a third major consequence of having a body schema, because higher cognition and language have some access to it. So body schema gives us explicit reportable knowledge about our own bodies. When you close your eyes, you can no longer see your arm. But you can still describe it, how its positioned, how it can move, you know about your arm, because of the arm schema. About 12 years ago, while I was studying the body schema, I was thinking about attention. Which is a lot like an arm in a sense, it's it moves around in a complex way over a large set of of dimensions, some of them physical dimensions and some of the more abstract dimensions and the brain controls it strategically. And by attention, I mean, selective attention. So the way the brain especially the cerebral cortex can enhance one set of signals and suppress competing signals. And classically, people study visual attention, but you can also selectively attend to a sound to a touch or to a thought or a memory or an emotion.

Unknown 44:55
So at that time, we proposed that In order to monitor and predict attention, the brain must construct an attention schema. And originally, this theory had nothing to do with the topic of consciousness, it was a way to explain the human ability to control attention. Like I said, a fundamental principle of control engineering is that a good controller needs a good model, descriptive and predictive model of the thing that controls what is that model, the attention schema, it's a bundle of information that describes a tension. It represents the complex state that a person's attention is in, and it predicts the effects that attention might have on the person's thought and behavior, and so on. What are the consequences of having an attention schema? First, an attention schema would be useful for controlling the movement of your own attention. As I've already said, it's control engineering, you need a model of the thing you're controlling. Second, having an attention schema that you can apply to other people should be useful for social cognition. People call it theory of mind, or intuitively knowing what other people are feeling and thinking. And part of that is knowing what other people are attending to. And knowing what it means for a mind to focus attentively on something being able to use that to predict other people's behavior. And third, the attention schema might be the source of beliefs, and intuitions about consciousness. Why? Well, here's a simple piece of logic. Everything you think you know about yourself every claim you make, no matter how certain you are of it derives from inflammation in the brain, or you wouldn't be able to think the thought or articulate the thought, you may jump up and down swearing that you really do have a hard problem. Essence of experience inside you an ability to mentally possess items, different items, at different times you're searching, you have that internal essence. Logically, there's only one possible explanation, information in your brain tells you that you have it. And here we have a simple account of where that information comes from, and what real physical process it imperfectly represents. Many people have suggested that consciousness is a tension, the two are tightly linked. But here, the idea is that what we call consciousness is a sort of distorted or mythologized picture of attention, as it has been filtered through the process of building a model of attention. And then passing that information through cognitive access. This is why attention and consciousness match most of the time, but can sometimes become dissociated. All of this, in principle is buildable. In artificial systems, we've done a little bit of that, we built a neural network model that controls a kind of visual attention. We tested it with and without an attention schema, an explicit representation of its own attention. When its attention schema was disabled, it was almost totally unable to control or learn to control attention. Now, our simple machine did not have these other branches. But in a person with the attention schema shut down, a person's conscious report should also be impaired. And we've collected a lot of data on this interesting relationship as well, in people conscious report, and the effective control of attention are deeply connected in a way that matches this architecture.

Unknown 49:10
And now

Unknown 49:13
let's talk about this middle branch. We propose that people not only model their own attention, but also model the attention of other people. And we think this is important in social cognition, one of the foundational properties of social cognition, it's very hard to know what someone else is thinking or intending, unless you first know what that person is focusing their attention on. And so modeling other people's attention is fundamental to social cognition, to theory of mind, we've done a lot of work on this I want to focus on one type of experiment that gets at the strangeness of the model that the human brain routinely builds. When you look at another person, when you look at another person looking at an object, it turns out that intuitively, without knowing you're doing it, you can't help seeing that person as having a subtle beam of energy emanating out of his head toward the object. But it happens only if you think the person might be attending to the object. If you think the person is uninterested, or is busy attending to something else, the effect goes away. So what is the effect there's several effects you've seen, for example, this, when you look at a person looking at an object, you generate a motion after effect in the space in between a measurable motion after effect, indicating some perception of motion flowing from the person to the object, the motion processing parts of your brain rev up, we know that we've put people in scanners, and we've seen the motion processing cortical areas revving up in a directionally specific way, as though processing motion flowing out from people's heads. And there's a number of other experiments we've done on this now, nobody explicitly realizes that they're perceiving other people in this way, you don't look at someone and explicitly see motion streaming from their eyes. It's all subtle, and under the surface, and you see it only as a kind of visual bias. So what's going on, we think, we think the brain evolved to draw subtle moving arrows on the world around us, not vivid enough to interfere with actual vision, but just enough to enhance our ability to keep track of who is attending to what. And we have some data showing that these subtle sub threshold motion signals significantly impact people's social judgments of attention. And this is a perfect example of something that I think is really important of a profound truth. The brains models are not literally accurate, they never are. The job of an internal model in the brain is not to inform us about the actual world. In a scientifically accurate way. The job of the brains models is to help us survive. Sometimes the brain's models are bizarrely inaccurate, physically incoherent, but they can still serve a useful purpose. And in this case, modeling a person's attention as an invisible energy diffusing out of the head can help in efficiently mapping the sources and targets of attention. And maybe you can see how these natural inborn models bubbling up into higher cognition might result in cultural mythologies about mental auras that are otherworldly, the energy essence of consciousness, the internal magic. The topic of consciousness is often viewed as a matter of unsolvable philosophy or a spirit or, or magic. But look at what we have here.

Unknown 53:26
We have a rational engineers explanation for why the brain evolved an important chunk of information and attention schema, why that representation serves really crucial roles, controlling attention, and understanding and predicting the attention of others. And how that information about ourselves might lead to distorted magical beliefs about ourselves. So is consciousness an illusion? No, in this account, it's a caricature. A caricature is a distorted representation of something real. And a caricature is often drawn for a specific useful purpose. If the attention schema theory, if this kind of architecture was right, then and attention schema serves purposes that are so important. We might want to build them into machines, it would make them more capable of controlling their own attention. And if implemented, the right way it could help machines in social interaction by modeling the attention of others. And as I said at the beginning, I've spent 12 years now trying to push the importance of an attention schema or representation of attention, not just attention on representations, and hopefully this little talk I particularly to this audience can help further the cause. And I will stop there

Unknown 55:09
thank you Michael Alright, so last but not least we have your Shabbat Yasser Bach is a cognitive scientist and AI researcher with a focus on computational models of cognition and your symbolic AI. He has taught and worked in AI research at Humboldt University of Berlin, the Institute for cognitive science and Austin Burke, the MIT Media Lab, the Harvard program for evolutionary dynamics, and is currently principal AI researchers researcher at Intel Labs California. Your show the floor is now yours.

Unknown 55:56
You're on mute? I think we have the view, including the next slider. Let's see, there should be a different way of doing that just switches on the screen is it working now? Yes. Excellent.

Joscha Bach 56:46
Our human perception is different from the perception that exists currently in AI systems, especially its unified, we don't just produce classifications of stimuli, but we have a continuous perceptual process that is representing eventually a single function. And the single function is what we usually call The Universe. Another thing that is different from AI systems is that the degrees of freedom in this model of the universe that we have across all domains, is trying to match the degrees of freedom of the universe. So the ideal model basically has one state that corresponds to a world state and vice versa. Of course, we don't really get to a one to one correspondence. But currently, our models are many orders of magnitude, degrees of freedom above what they're trying to model. So they have more degrees of freedom and the parameters that the domain SV is being modeled. And as a result, the model is not a sparse and as stable as the models that human minds or animal minds can come up with. What we will also notice is that our presentations are hierarchical, and the higher levels and these representations are increasingly sparse and stable. But that's just a phenomenon that AI representations are increasingly matching. When we think about how our perception works, it starts out with patterns. Some of these patterns are visual, others auditory or tactile, or proprioceptive, or emotional, or there are features of our imagination. And over these patterns, we integrate percepts. So for instance, we integrate the visual, auditory and tactile domains into environmental percepts, and tactile and proprioceptive patterns into somatosensory percepts and proprioceptive and emotional patterns into motivational percepts and emotional and imaginary percepts patterns into mental presets. And on the next level is these presets are being integrated into mental simulations which form a perceptual space in this perception space, we can imagine it to be like a game engine that is continuously simulating a three dimensional world with objects in it, which we could call this Deckard res extensa, the extended stuff. And then in addition to this, we have stuff that is happening on our mental stage that our ideas and thoughts and revolutions and so on. And an important part of this perceptual space is our self model. And this model contains a somatic self and the social self and personal self different domains within the self. We integrate our environmental presets into a current model and our mental presets into a mental state in which we have imagined states like hypotheses that we want to test memories of past situations, imaginations of future or counterfactual situations. And we integrate over the perceptions phase into global maps. So the current world state, especially the scene that we perceive ourselves to be in the room in which we are the objects that are in the room, the tendencies that this film has to has in its developments, basically our immediate expectation horizon In the current scene is going to develop. But he also able to bind all the different scenes in our life together by having basically a global map of the spaces in which you move and how to get between these scenes. And this is basically an area of knowledge that is increasingly propositional and the build this up into a unified model that has lots and lots of abstract knowledge, also, including knowledge about our self, and about our mental representations. And the whole thing is driven in no small part by an attentional system that we can imagine to be like the conductor of our mental orchestra. And that's the conductor of the metal orchestra, the conductor is not playing the music, and the conductor is not even deciding alone, which music is being played. But the role of the conductor is to harmonize what the orchestra is playing to do back the performance if there are contradictions and inconsistencies in it, and to make sure that everybody is playing the same piece. And the attentional system, this conductor also has a representation in the self. And this is what I would call the attentional self. And it's I think, what Michael Graziano describes is the attention schema. And this attentional self is being used to drive what the attentional system is doing. And one part of what the attentional system is doing is it singles out stimuli, especially from our environmental percepts for the current situation, because they are perceived as being relevant for our volitional needs, or for this aggregating the interpretation of the environment. And for the same reason, we also single out features in our perceptual space, including mental representations.

Joscha Bach 1:01:43
And the attentional system is driven by the self model. And to do this, it needs to have something like a protocol memory, so it needs to have a memory of the states that are attended to. And this protocol memory is extended over time and to a biographical memory. So it allows you to remember what you did to the last few seconds, comes during the last few minutes, and eventually forms a model of your own identity of yourself together over different instances with different scenes over different days, years, and so on. And in this way, it creates a biographical self. So our attentional system is a crucial component and forming the type of self model that we have as human beings. And the roles of the attentional system are, first of all learning. Our brain is not neatly organized in layers in which you can do Stochastic gradient descent, you do have something like layers to bring online during early learning in our early childhood as infants, on which we have layers with high neuroplasticity that then basically get baked in before we get the next layer online. But for most of our life, the learning happens with an attentional mechanism. That is, we single out the features in our control that we want to change, and the conditions under which you can evaluate the results of the change. So an example that I sometimes use Imagine you want to learn to improve your tennis play. So you are directing your attention on the parts on your system that is controlling your movement, you are not an dotnet current neural networks are piping the entire learning through your visual system, first new sensory apparatus and applying lots and lots of little changes that consistently cancel each other out over enough samples. So the visual system remains roughly unchanged. But your visual system has been almost exhaustively trained in your early childhood. So not going to touch it by playing tennis, you directly go for the change and your motor control at a very high level that you want to make. And you also take this partial bindings that allows you to address it in working memory to store it together with the expectation of the outcome, which means maybe you're going to wind the match if you're doing the following thing. And a few minutes or a few seconds later, when there's environmental status available, you recall the changes that you made, and then we enforce them or undo them as necessary. And this attentional learning is absolutely crucial for the way attend learning works in our mind. So getting attention on something is crucial for learning, it allows us to do one short learning in many situations, while machine learning system would need to have many, many samples that would be protected at the system many times. So we are much more sample efficient than most of the current machine learning systems. And another thing that where our attention is crucial is reasoning. And we can imagine reasoning to be a learning process by which you do not need to wait for the environment to supply the result of the changes that you're making. So then you reason you don't need to wait to see if you want the tennis match. You can immediately see how the change that you apply to your Metro presentation plays out and its consequences based on the symmetries that you have already learned. And this means that you can and understand much of reasoning as something like real time learning. Right. So there isn't a tension learning process that when you perform directly on mental representations allows you to make changes to your mental representation that play out immediately. And again, there is a slight difference to what our current attention based learning mechanisms to when they do inference, because none of these learning systems currently is memorizing the results of its reasoning. So it's not learning from its reasoning when a system like GPT three makes an inference, this is only something that happens in the present working memory state and is not committed to long term memory. And this also is true for learning. During learning, the attention is only directed on the present context, the current data that are to be consumed by a system. And when we learn, be constructed working memory state actively, we basically remember all the things that are important for interpreting the current thing. And we make changes to these previous contexts. So when we read the final chapters of a book, we might recall the introduction. And we might reinterpret what we learned through the introduction, and use this for interpreting the last chapters. And I personally am very keen on the idea that the attentional system as Michael such as this, enabling consciousness, I think that consciousness is in large part to be understood as a memory of the contents of our attention. And mica called consciousness caricature, I would say that it's a part of simulation. And in part, it's a simulacrum. And the difference between a simulation and the real thing is that the simulation is a causal structure that preserves the causal dynamics of the domain, but it's

Joscha Bach 1:06:45
implemented on a different causal substrate. A good example is a computer game. The physics in a three dimensional computer game are not implemented by having interacting particles or quantum mechanics or anything like that. Instead, it's a very high level abstraction that is implemented with symbolic operations. But the cost structure of computer game is sufficiently similar to the physical domain that you can learn how to drive a car or fly an airplane, or run around and shoot things in a computer game, right there is a causal structure that allows you to have interaction is simulacrum. And comparison is only reproducing the observables figure out the causal structure that connects things. And this means that the movie is a good example, for a simulacrum, the movie is producing a sequence of observables that allows you to recognize a scene that you know from a domain, it allows you to learn from it. But it doesn't allow you to causally interact because there is no proper underlying causal structure. So the supernatural only pretends to be a simulation. And I think that our consciousness is a part of simulation. And in part, it's a simulacrum. It is for instance, to for our decision making process. So our perception of freewill, I think, is what looked at what decision making under uncertainty looks like to you between discovering the first person perspective. And before deconstructing it again, as soon as you just deconstruct your first person perspective, in decision making, the process of making your decision becomes indistinguishable from predicting your own decisions, right, and you basically see the algorithm that happens during decision making to some degree. But before you can see this algorithm basically and magical operator happens that allows you to jump from an assessment of the situation to a volitional state to commitment to what you want to do. And this magical transition is a simulacrum, you basically have the impression that you understand what's going on. But you're not because you don't understand don't model don't reverse engineer the underlying causal process. And our own self awareness is basically a gradual reverse engineering of the cognitive processes that determine our own agency. And to make that happen, we have an attentional system that has a representation of the current binding state of working memory. Working memory is basically an interpretation of the current situation along this coalition of states and all other parameters that are changing all the time, the contents of working memory are composed from long term memory knowledge that can be tested knowledge about skills or can be propositional knowledge. And in every situation, you compose it into a model of what's currently the case. And a part of that is being attended to by what we call phenomenal consciousness. It's basically what the state of the working memory looks like to reflect the refractive system that is able to make an index memory of part of their binding state. So you can data reconstruct it and they also have a presentation of the fact that we are attending to something and the mode in which we are attending to it. So we are aware of whether the thing that we are attending to is conditional disambiguate. Question of the scene, for instance, or whether it's a memory of a past situation or hypothetical situation, or something that you consider to be currently the case. And then you have reflexive consciousness, which is basically a representation that the attentional process is indeed the attentional process. This might be a result of the fact that our mental processes are self organizing. And we need to identify the nature of the process when we're using it. So our own attention is characterized by flitting back and forth between the awareness of the contents and the awareness of the nature of the process that attends to

Unknown 1:10:34
the contents.

Joscha Bach 1:10:38
There's a difference between perception and attention, I think our perceptual system is distributed, and it is convergent, which means it largely follows gradients. And it's high dimensional, often continuous, which means geometric, and the hierarchies of perception tend to be fixed. So for instance, we have at a low level, we perceive color as it's at the high level, we perceive texture, spatial frequencies, then we perceive surfaces, and then we perceive geometry that is moving the surfaces in a certain way. And then object identity and so on, we don't do this, the opposite direction, you have a fixed hierarchy similar to a game engine, that is representing features of a scene in a certain hierarchy of abstraction. And in our attentional thinking, we have localized representation that are constructive, when you're constructing something you cannot just follow a gradient, you need to remember what you tried, what worked and what didn't work in which spaces your speaker's states and the space you already visited. So you need to create an index memory that can be revisited by the attentional process that allows you to recreate previous states or modified creations of the previous states. And as a result of that universe, you need to have pointers, which means that your representation is going to be low dimensional, there are only a relatively few features that you can manipulate at a given time, because of the resulting combinatorial explosion. But this also means that your attention needs to be integrated with perception, because the way in which you modulate your attention is going to be given by some kind of a perceptual process that gives you tendencies on how to direct your attention. So, these are not entirely separate systems versus strict boundary between the system one and system two, but rather there are systems that are interchanging information and that require each other attention is largely analytic. And the analytic hierarchies that we are constructing are very often flexible, which means we can construct hierarchies. But you for instance, say you have a nose inside of a face and you can have a face drawn on the tip of the nose and then a nose again in that face and so on right. So you can have in some sense, arbitrary hierarchies that you construct using conceptual structure, the conceptual structure is acting as an address space for the working memory contents that you address in perception.

Joscha Bach 1:13:05
Transformer is a very fascinating model. To me, the transformer, I think, evolved in natural language processing rather than an image processing because of the unique demands that natural language processing puts on the algorithms. And image processing, there is an interesting property that is neighboring pixels in the image often are related semantically, right, because objects individual domain tend to be connected. So if you just build a hierarchy, where you are looking at the surrounding pixels of each x pixel, and you group them together layer by layer, you have a very good chance of discovering connected objects. But the same algorithm doesn't work as well in natural language. Because the natural language, you very often have beginnings of sentences and ends of sentences be semantically connected and the stuff in between less connected. And if you try to build connective wages using engrams, strings of connected things, you have so many different possible words, that for statistical reasons, you're often not going to be able to discover the relationship between an article and the noun if there are three adjectives in between them, for example, right and very often, it's not just a across many words, but it can even be the connections across multiple sentences or across different chapters across different books. So we need to have an attention mechanism that is able to spend much wider area and to be to do the sparsely so basically, you need to learn the you need to pay attention to you make statistics over what you make, make the statistics over. And the algorithm that was discovered in 2017 was is relatively simple. It's basically one in which you are half attention hats that the neural network learn which features to attend to based on the present context. It's a layer by layer attention in a way it's different from the attention of While in mind, and during learning the transformer performs only an offline processing. And the attention only acts on the present context. So it's not constructing a configuration, it's taking the present configuration that is being fed into the machine learning model, and learns which part of that context it needs to pay attention to, for instance, in famous GPT squee, it set in 2048 tokens, which are words at the same time. And it's taking this buffer of words and identifies those words that in a given context, are important to pay attention to when it learns to predict structures that you should build from them as a result, and the representations are static. So every time you pay attention, you pay attention to this present scene, which is a single object and the interesting media attention algorithm of the transformer also works on many, many other domains. So it was invented, for doing vision for doing natural language processing, it turns out that the transformer algorithm can be used for vision and for audio. And for cross modal representations. If we find ways to deal with the fact that is more features in them, and basically have to make attention sparse, they have to be able to leave out things from the attention. And then a very active area of research is how to deal with video, for instance, which has dramatically more data in it. And so we need to have much more sparse attention that is more actively constructed.

Joscha Bach 1:16:38
And our own mind has solved many of the problems that the transformer still has for our attention is integrated, and it strives for coherence. It's basically like an agent that lives inside of the perceptual system. And it features online learning, which means that our attention system interacts with a perceptual system that is entangled in real time with the world and in real time, tries to make a dynamically updating model of reality. And in this, it's driven by the past state and the direct the interpretation of the present and the future. So in this sense, the attentional graph, the attentional structure is building a dynamic scene graph of the world. And the features in the scene graph are basically not just pictures that allow you to recognize things, but there are parametric operators that advance the scene predictions based on these operators, the features, acting on the present perceptual representation of the world, and gives you a prediction of the next perceptual state. And the attention integrates these latent features into the scene graph. And our attention also drives active perception, which means the direct our readiness to sample data from the environment, when even get up to the bookshelf and get another book and open it to change the context of our working memory into a more suitable one to understand a present thing. And so we have an active perception that is connected to our attentional system. And its goal directed. So we have things that we semantically want to understand in a given situation. And that drives the actions the operations that the attentional system performs. And this is, I think, a far cry from what our current algorithms and AI can do. And this is correct.

Unknown 1:18:24
Thank you, Yoshua. So now we have our q&a and discussion set session. We have quite a few questions in the chat already, I would like to start off with a couple of our own Vasudev. So have a question for you. What are the current developments in attention models? Especially for dealing with video? And are there attention models for the motor and control the means that you're aware of?

Unknown 1:19:00
Yeah, so you know, video transformers is very nice in stage. And this is primarily because even with images, you're sort of at the computational limit of transformers. And, you know, video is a sequence of images. But there are two pretty neat architectures that I would say have come out in the last couple of years. And by name, you know, they're called times former and motion formal. And both have tricks on how to limit the attention. So in times former, it's a very, it's a very simple decoupling of attention and space and attention and time. So that you have you have the same spatial attention that you would have in like vision transformers for images. But then for the time axis, the same patch of the image only attends to the same spatial location and other timeframes and of course, That's so limited that, you know, this paradigm breaks down if the, you know, the object moves or camera moves. And that's a huge limitation. Very recently, there's been this architecture called motion former, which has this concept of trajectory attention. And it's really interesting because they use the attention mechanism itself to sort of track the location of a moving object and the frame. And instead of the attention being, you know, decouple in the space and time in which will break down as soon as you have movement. The attention is really around this moving object following the trajectory of a moving object. And we do. But even you know, that also still has pretty severe challenges with its compute complexity. So I think this, it still remains a very rapidly evolving field. But the direction of motion form and times form of a video is, I think, a good step and the last deal.

Unknown 1:21:00
Thank you, Pastor. Jonathan, I have a question for you as well. So transformer models have been developed without much inspiration from neuroscience, and mostly driven by need to create more efficient statistics? Do you see ways in which we can incorporate ideas from neuroscience more readily into AI models and which ideas should be tested?

Unknown 1:21:24
Well, first, I wouldn't really necessarily agree with the idea that transformers haven't been inspired by neuroscience. The path is not direct or mana. I mean, it's not explicit. And it's not necessarily monotonic, but it's definitely there. And that's why I took the pains to put Jay McAllen and Dave Romo hearts faces up in there psychologists sort of look with eyes towards neuroscience. But there's been a lot of back and forth that unfortunately doesn't get credit. There hasn't been quite the scholarship, I'm afraid in many of the most recent innovations without detracting from the innovations themselves at all. They're built on ideas that have been around for a while transformers are built on the idea of gated attractor systems, which were very early recognized as as things that the brain implements. And I suspect, I would have to leave it to, to Juergen huge Schmidhuber and Hochreiter. To decide whether or not they were inspired by suspect there were but their work was presaged by by people like David Zipser, looking at recurrent networks with prefrontal cortex and, frankly, work that we did on modulation in gating the role of neuromodulators and gaining access to attractor systems. And similarly, episodic memory, I mean, Deep Mind was founded by a hippocampus ologists to recognize the importance of the complementary learning systems hypothesis, in particular, the distinction between episodic memory capable of arbitrary binding and rapid sort of association formation and semantic neocortical learning that was statistical and the complementarity needed for those to accommodate both statistical knowledge and the and the flexibility of forming rapid associations. So I wouldn't say that transformers are not inspired by neuroscience at all, I'd say the opposite. That said, um, clearly, they're incredibly, you know, great insights coming out of that work. And the brain is complicated in the more sophisticated people that we have that understand how to build complicated models and make them work and have the resources to do so the better. And I am taking inspiration from those models, as much as I believe they've taken inspiration from our predecessors. I think that one of the problems and one of the reasons of like the thing, well, one of the reasons why I took the pains to keep my talk simple was to say that, you know, you can throw a complicated device at a complicated problem and get an answer, but not necessarily understand the answer. And that often may be what's necessary, at least the first step, and then you can analyze that complicated thing in silico experiments are critically important. But at some point you you'd like to know what's going on and understand it and simplify them. Science is about the art of simplification and lowering of the dimensionality down to the things that are really driving the sources of variance of interest. There may be problems that simply don't yield to that. And it may be that in some sense, the human mind doesn't for just the reasons that Michael and Yahshua pointed to but but that's the goal of the scientist anyway, right. And and so, I think there are principles that that these mechanisms are exhibiting and if we can distill them down into simpler form and examine them and sort of a petri dish, we might learn something much as we are learning something from throwing complicated versions of them to complicated problems. And I can point people to some examples from our work and others, where that's been done. And in those cases, it also makes it easier to bridge between what these things are doing with the brain Let's do it. Because if you sort of understand what the underlying mechanisms and principles are, and one example I gave of that is this idea of multiplication through nonlinear transformation. Right? I mean, the neuroscientists argued for decades about how attention work, then there were, I won't belabor that now, but but a lot of the confusion was because there wasn't a clear formal understanding of the kinds of mechanisms that could sort of account for all the phenomena with in one simple way. And I think this idea of summation through non linearity is a really powerful one that transformers are using as well, right? That's how they have their modulatory effect, how they have their attentional effect. So I think there's lots to be learned from all the approaches. It's just incredible that we have all these people working on it now, but but in there needs to be some simplification. And, and I think that that is one of the paths towards sort of bringing in neuroscience.

Unknown 1:25:57
Thank you, that was an excellent answer. So we have a question for Michael. Our machine, it's a question from HDR in the audience? Are these machine learning attention experiments capable of understanding or maybe using how early attention experiments where one sees different things in noisy images, say lots of shaded dots that hide the for me hide the donation dog, or those 3d images that once one focuses attention on and control gets himself under control? A major thing a magical 3d World appear?

Unknown 1:26:36
Ah, I'm not 100% Sure, I understand the question. But so how does attention to transform kind of piecemeal input into a, into a integrated world? Is that the question essentially,

Unknown 1:26:55
um, let me let me try to rephrase it, now that I've read it. So are the experiments that you've done able to understand how attention constructs the world where the representation requires a specific attention pattern. So for example, focusing in a particular way on an image to see a dog or

Unknown 1:27:23
similar, right. So at a much simpler level, one of the things we've looked at is how attention moves across a visual array, a simple visual array, you know, we don't use a, a dog, we use little spots, and how attention moves across that array attention, the attention control mechanism, learns the statistics of the environment, and learns to shift attention in strategic ways across an environment and it learns to do it in a way that we're not even aware of. So with repeated exposure, we learn to every time we see this, we move our attention first there, and then there. And so there's a pattern to the learning. And, and, essentially, this is sometimes called model based learning. That is, there's a model generated, that's predictive of what states of attention should follow, which other states. And what we find in our experiments is that this kind of model based learning or this kind of rich control of attention, depends on people being able to be reportedly aware of the relevant stimuli. So the with respect to the work that I talked about at least, the rep people's representation of their own attention, their ability to know what their own attention is doing and where it is, and that they're paying attention to this. And that is crucial to that level of of control, and learning how to control and learning how to focus attention on the right parts of the stimulus. So at a very simple level, maybe we're studying something like that. I'm not 100% Sure, I grasp the whole question. But

Unknown 1:29:13
if, if, if I may add, I like I think it would be an interesting experiment. I think the question is referring to these optical illusions. You know, like, sometimes these images, you might think there's a different shape, but if you focus on it properly or differently, you end up seeing a different shape, or, you know, the same argument, but like blue and yellow color of bags and so on. I think it would be pretty interesting actually to show the stimuli to artificial systems and see if they, like if you block certain attention pathways, you know, does it get confused in the same way as humans? I would suspect that probably not in the same way as humans just because you know, the transformer based systems are still you know, probably you reasoning, like gotten isms are quite different from how humans reason about images. But I think it would actually be an interesting set of experiments.

Unknown 1:30:09
Thank you. Yoshua. Oh, sorry. Thank you, boss, we have a question for yourself. So, could it it's a question from Sean kumin. And the question is, could it be argued that the attentional system would also control which part of the unified world model will be used to function as a sort of transformer to work on the perceptual space in order to validate and create community?

Joscha Bach 1:30:40
I suspect that it is quite different from the transformer. So there is a similarity in the trend that the transformer is selecting features based on statistical relevance, or statistically estimated relevance. And I think that is going to be similar, eventually, for our attention system. But the mechanism is probably slightly different. So the evaluation and the implementation will work in a different way. And currently, I suspect that there might be basically some kind of Neural Darwinism going on, that is determining our mental architecture, that is the structure that your mind has, is the result of an evolutionary competition between possible architectures in your mind. And this evolutionary condition is not done completely randomly, all from scratch, but the evolution is wrecked by the innate setup of your brain. But basically, your brain, your genome was able to efficiently encode the architecture of your mind, not just because it's in details specified in the genome, but because it sets up an evolutionary competition and this, which was basically a very, very sparse representation yet that you would need. And it's also a flexible one, because it allows you when you are being confronted with a different environment, your your upbringing, to have a few different bifurcations into NTFS, and slightly different architecture. And it's also more robust against developmental aberration. So basically, if your own brain is formed in an atypical day, because you had some endocrine disruptors in your early childhood or something, then the evolutionary competition between possible metal architectures is going to lead to a solution that is better aligned with the actual brain architecture on the physiological level that you are stuck with in this. So in this sense, I would think that the attentional system that we have basically evolves in a similar way as a government evolves in a society. So there are certain types of government that are optimal, given certain conditions in a society. And in the same way, you will have formation of a stable structure. And the reflexive attention is probably part of that, because being aware of the fact that you are the observer is going to stabilize the structure that is going to play the attention agent. And it's going to bring order and consistency and coherence in the representations that play out in your mind. So it's a very speculative theory that I just presented a basically a vague idea that it's trying to add a little bit of perspective on the possible genesis of the attention mechanism in our brains don't think just a mechanical, statistical process that happens layer by layer as the transformer. But there is more basic and organic formation of a governing structure that is actively trying to create coherence in the mind to understand.

Unknown 1:33:51
Yeah, I just wanted to sort of embellish the points that both of you were making and put a term on it inductive prior. I think what's missing from most of these models, there's one inductive prior, which is gradient descent. That's a really powerful one and a really useful one. But it's not the whole story, right. And so I think part of what our job now is to search for what the those inductive priors are, and I think you also mentioned one kind, which is sort of evolutionary encoded genetic dispositions towards certain architectures. And those may be patterns of connectivity. Something like honing networks, right, the bias towards local connectivity. But there may be other more macroscopic forms of of, or Mizo architectural constructions, like once again, the complementary learning systems idea, the idea that you have some parts of the brain that learned very quickly and they're segregated from other parts of the brain that learn more slowly, and the interaction between those may be a really important ingredient. Sorry, I should have said The separation between those together with their ability to interact under certain conditions may be a really important ingredient. And I think that is one of the magical ingredients of the neural Turing machines, you know, in its original conception, and now it's been sort of realized that a different form in Transformers, we have a very simple again, we'd like simplicity. So a very simple version of it, that I'd be happy to send people a reprint of that was published in ICML. We call it the ESB and the emergent symbol through binding network. And it basically distills down this idea that if you have an LSTM that's learning sort of an embedding space, how to make embeddings, but it's being forced to sort of record those as let's say, values in a key value system that is now isolated from the embedding space. But as seeing the sequences that go on, and is subject to the same learning pressure, that isolation allows it not only to do things quickly, but to be protected from the embedding space so that it can learn the abstract rules that underlie the sequences without being in any way impacted or distorted by the embeddings that are being used to exhibit the rule. And so that kind of separation, I don't believe that it's complete by any means. But just demonstrating that shows that there's a principle in play, that may be an inductive prior for learning abstract representations of the sort that I think both of you were referring to as being needed to do dimension reduction in attentional selection space. Right. So I think I think it's the search for those priors that are beyond the the ones that we have now that that is sort of for me where the frontier lies.

Unknown 1:36:39
Thank you, Jonathan. I have a question for Michael, speaking of divergent neural architectures, in light of it's a question from Alesia. Why here? Pardon if I mispronounced her name? In light of the attention schema theory, given the different proprioception and theory of mind in autism? Can we expect that the attention schema is different in autism? If so, what would be the implications under for understanding of the consciousness?

Unknown 1:37:12
All right, I believe you're on mute.

Unknown 1:37:14
There we go. Now, I'm not muted. So it's a really good question. Something that I've been very interested in for a long time, I think that autism is actually a pretty large, complicated collection of stuff, and not one thing. And it's certainly on a huge spectrum, which makes it more complicated to study. But it is true that, on average, people with autism have a disruptions in control of attention. They have disruptions in control of eye movement. And they have disruptions in in social cognition and theory of mind and in understanding other people's attention. And so there's some possibility there that that may be so that there may be some element in some aspects. In some instances of autism, were being able to model attention either one's own or other people's and use that in a predictive way, either to help control your own attention or help predict other people and understand other people, maybe there's some some problem there, we've we've done a very small amount of work on autistic, comparing autistic people to neurotypical people looking at areas of the brain that we think are involved in constructing and attention schema, and the results are, you know, complicated? That's a topic that we're really interested in looking at further. So this question inevitably arises, does that mean that autistic people are not conscious? And I don't know if that I presume that's not what this particular question was asking. But that comes up a lot? And the answer is goodness, no, we would never want to say that. I think that there's a lot of really interesting ways that our self model, the self model, that makes us say, I have consciousness, that that self model can be different between people, and also be in different states at different times within one person. And once you begin to think of consciousness as a self model, that's computed by something in the brain. I mean, it's basically a theory of mind model about yourself, once you begin to think of that and once you begin to look at the, the information in that model, and how it's constructed in the brain, then you start being able to come up with the beginnings of a theory for how it is you can have altered consciousness or people who have an altered self model and therefore would report being in a different state of consciousness. So you start having a theory to maybe explain some of that diversity across people and across time within one person and that's, that's really interesting to us. But pursuing autism with respect to an attention schema is really central to my interest right now, I guess I should say that the answer is I don't know. But I'm really interested in it.

Joscha Bach 1:40:15
I sometimes wonder to which degree, autism is correlated with the ability to integrate over many layers. So basically, if, if the penetration depths of the neural signals during perception would be too low, you will be forced to integrate over fewer layers, which means you have possibly higher resolution within the layers, but you have difficulty to see the big picture. Also, you might have difficulty to stabilize your representations. Because in order to get stable representations, you need to integrate over many layers to see the regularities that that happened. And as a result, you get behaviors like stemming, which try to create an artificial stigmatisation by producing an offset to the input signal that you can predict. And a similar thing might be happening in ADHD, you have difficulty to integrate over the control functions and you basically driven by the short game functions that give an immediate reward. And while you're able to model high level reports, you're unable to act on them. Because the control signal does not penetrate far enough in the control hierarchy. That's, that might be just one aspect that plays a role in the attention in on the spectrum diseases. But all it is orders. But I wonder how important it is. And whether it's something that's testable.

Unknown 1:41:36
I'll add to that, again, from a neuroscientific perspective, since I was asked to represent that something that's missing from all of the deep learning models and cognitive models is neuromodulation. And, and by that I mean sort of broad scope neuromodulation. Neuro Dopamine is a neural modulator. And it has local focal effects that I think are fairly well understood and thought to be, you know, the neural grounding for things like reinforcement learning, but there are other forms of neuromodulation in the brain that are widespread nor adrenergic, neuromodulation, cholinergic, neuromodulation, serotonergic, neural modulation, some of which are not point to point. And that sort of, sort of widespread parameter setting can be dismissed as just sort of like, Oh, it's just a tuning parameter, but it actually interacts in profound ways with different kinds of computation occurring in different circuits, and maybe a very important contributor to control. And, again, it what brought it to mind in this context is ADHD and autism, where both are thought to be associated. In fact, just about every psychiatric disorder is thought to be associated with some sort of neuromodulatory deficit. But but those in particular, related to norepinephrine, and there's been some work looking at the role that nordnet norepinephrine may play on exactly what you're referring to. So that is that sort of the narrowness versus scope of integration that occurs in a moment to moment basis. And I'll post a link to a study that that that addresses that in just a moment.

Joscha Bach 1:43:07
Now, the deeper problem, I think, is that there is no model from neuroscience that has been published in such a way that is actively driving representational architectures in AI beyond some narrow area of neuromorphic. Computing. And even the neuromorphic computing people do not claim that your morphic chips are directly implementing representational structures as they're being found in the brain. The important insight is that almost all the models that currently exists in present machine learning and AI, are using artificial neural networks that don't even have much of a superficial similarity between a visual representation that the brain is producing. Basically, the representations in neural networks are chains of weighted sums of real numbers. And the reason why we use them is bigger turns out that they're very good at representing linear algebra with a few nonlinearities thrown in between. And we have discovered a family of learning algorithms that allow these representational structures to converge to almost arbitrary functions. But we know at this point that the neurons in the brain are working in a very different way. And if you want to translate neural modulation, from brain models into representational architectures, we need to discover different representational architectures in the context of AI. So I think we would need to do a lot of transfer from ideas from neuroscience into AI. And I found that when I look at the neuroscientific literature, that there are a few streams of ideas or a few schools of ideas on how the brain is facilitating representation and computation. But there is no canonical single perspective that could be translated one to one. I think it is correct me if I'm wrong.

Unknown 1:44:54
Well, I don't I don't think you're wrong, but I think what you said is incomplete. There's first I don't think there's ever going to be one because the brain functions at many different scales any more than there's sort of one level of physics at which you describe something. If I wanted to describe how to build a bridge, I asked me, if I wanted to describe what it's going to take to blow it up is Schrodinger, right? I mean, so there's no one, it's there are relationships between them. That stands to be better understood, for sure. But the reason I think there's a diversity is because there's diversity of level of or levels of organization, and therefore, levels at which we can afford to simplify and still explain. So I think asking for one translation is just not going to fly. That said, there are ones and again, I can point to work from my colleagues and I, looking at how neuromodulation plays a role in shaping representations and low dimensional representations and frontal cortex, I'll post the link to that, too, that is inspired entirely by basal ganglia circuits, but is written in what is essentially an LSDM. And so I think that would be recognizable to any modern AI person at the time, written in the early 2000s. It wasn't recognizable to AI people. But now Now, if you mean recognizable to people who do AI and strictly symbolic form, that remains a challenge. And I think that's an interesting challenge, and one that I think we're gonna start seeing addressed. But if you mean AI broadly now, including machine learning and neural network connection is modeled whatever the right word neural network means. Now, when you're referring to a transformer, I think there are there are definitely examples out there of work that Matt, Mike Frank, at Brown is doing a beautiful job of that I think relating the details of in some cases, receptor specificity in particular circuits to Bayesian optimal solutions to problems that it seems the brain is solving. So I think there are pockets, there's not as much so I share with you that frustration that there isn't enough of it. But there's some and there are places for that to be found. And again, I'd be happy to direct anybody who's interested in that to to least ones that I know that

Joscha Bach 1:47:01
you've been discussing, for instance, of course, Burke's adaptive resonance. Another one that's context. Pitch is basically trying to understand mental representations as resonances between oscillators. And that's a paradigm that is very different from the ones that we use in AI. I agree with you, Jonathan, that there are many approaches in machine learning that are functionally equivalent at some level of abstraction to what the brain is doing, which includes, for instance, Bayesian modeling. On the other hand, there are phenomena that seem to be happening in the brain, like the shifting or rotation of representational fields, overactivity, short time spans that are difficult to explain with our models of synaptic storage, for instance. And on the other hand, there are things that are quite central to how machine learning works right now, like weight sharing, or stochastic gradient descent where multiple layers where it's not trivial to find biological equivalence, or to explain the biological mechanism by which this would be achieved. It's not that there are no models at all. But these are very speculative. And it's currently basically poor evidence on how the machine learning would be one one translated and be I think we all also understand that the brain by itself, organizing nature is using some fundamentally different design mechanisms for its models, because the individual neuron has to be the active unit rather than centralized control that is imposed on all the units.

Unknown 1:48:33
Thank you, you're sure we have a question for vasodilate. When you were talking about the experiments that were performed for multimodal attention, you mostly used photographic images? How about any experiments being done on more abstract images, for example, things like blueprints, designs, models, etc? And it's a question from John Coleman.

Unknown 1:48:58
Yeah, that's a really interesting question. So, you know, and, and, in fact, you know, pretty commonly, even in the semiconductor industry, there are a lot of images, which are not photographic images. So you could be obtaining scanning electron microscope images, and you're really interpreting, you know, the signal you get from these scanning electron microscope, you know, at the nanometer scale as an image. And it actually turns out a lot of the same architectures that have been really successful at photographic images, like, you know, resonate. And you know, Transformers now are actually pretty effective at these kinds of more abstract images. And, you know, there's a version there's a version of image net, which is like sketches. So instead of having pictures of bananas and carrots, you actually have sketches of, of these objects. And again, you know, same similar models do pretty well. But I think it's a very interesting area to study and like building blueprints might be pretty interesting. And, and one application I can think of is if you can train generative models for it. Maybe that's how future architects get aided, you know, you have a machine algorithm that generates a first pass of a new blueprint. And then it's fine by by an architect. Yeah, but I think this is, again, a very interesting line of study, not studying just photographic images. But these other kinds of images. Do

Unknown 1:50:30
we have a another question for you, Acida from UBS and the new scale, and disregarding slide 11, where there was a picture with a bird with a red circle around the eye? How is the correct answer generated in this image? The attention heatmap doesn't quite focus on the rain around the eye. So how do we know where the answer red comes from?

Unknown 1:50:58
Yeah, this is this is again, a really good question. And, and, you know, it was pretty interesting, Yulia was able to spot that. Now, when an image is fed into a transformer, it is, you know, some representations refer to grid touches on. And, you know, because of computing limitations, you know, it's sort of a trade off between how big you want these credit patches to be. And, and, you know, these can be quite coarse. So, for example, and, you know, the original version, transformer paper, you know, type size, arbitrary bytes 1616 pixels was used. And it's very similar in our architectures also that, you know, the representations going into the transformer for the image representing patches of these images. And they will not completely, you know, overlap features in the image. But, for example, that particular bird's eye, you know, it gets overlaid, as best as it can with these grid patches patch feature in that particular example.

Unknown 1:52:06
Thank you. That's it, if there's a question from cool see, too, I presume all the speakers? Do all the speakers consider that the or the the old broadly share the same idea of what attention ontologically is, for example, some mechanism which to sign up to gain? Or do they believe that they are there are more some important differences in their perspectives? And how they consider this cognitive function? And if yes, what are these differences?

Unknown 1:52:40
But I think

Unknown 1:52:43
I was just gonna say very briefly at the outset, that there's a very large number of kinds of attention and ways that that term is routinely used. And so I think we probably all agree on the basic many definitions of attention. It sounds like most of us are talking about what's called selective attention. And I think we broadly agree, and that's all I wanted to say that there's many, many kinds, many definitions of that word, but go on the rest of

Unknown 1:53:11
it. Again, it depends on the level of analysis that you're interested in. If you're interested in the fact that agents can have internal representations that determine how they're going to manage external information. We all agree, if you want to understand the details at a operational level, I think we're probably all pretty much in agreement, if you want to understand how the brain implements it neuron by neuron. I don't think any of us and we probably to the extent do with disagree. And that goes to your service point, right. I think there's depending upon the precision of understanding and the detail of the mapping from at least normal implementation to function and its ability to be built into artificial systems, that still very much open. Even there, I would say that there are certain principles, I think there's general agreement at this point that the tension has what at least functionally can be described as a multiplicative effect. That is if it representation is in some neutral state, attention doesn't suppress it or excited, it leaves it. It's only if it has some internal source of inhibition, that it's accentuated and more inhibited. And if it has some internal excitation, and it's excited more, that's what it means for it to be multiplicative. Right. But beyond that, I think the details still, you know, the devil is in the details for the neural mapping. I don't know I'd be interested to hear what what does it have and also think about whether this idea that multiplicative effects of one representation on another is sufficient functionally to get as far as we, you know, to get us what we need at the level of AI right as opposed to mapping to brains.

Unknown 1:54:56
Yeah, so I think I think I'm broadly agree with everything that that Jonathan's said, and I would say that, you know, the, the formulation of attention and transformers is pretty simple. You know, it's, you know, this key query value based attention. And, you know, I'm sure it's a gross simplification of the attention that really happens from a neuroscientist point of view. But it does happen to work pretty well on existing benchmarks. But, you know, I also believe that some of our existing benchmarks get gamed by transformer. So for example, a lot of NLP tasks are in terms of multiple choice questions, and it gives a lot of, you know, opportunity for transformers to cheat. And a lot of NLP tasks are also very short sentences, you know, most of the tasks that make up Munis card, glue, super glue. They are, you know, all 100 words, sentences, and so on. And again, the paradigm of K KVQ, attention really blows away all previous architectures and deep learning. But I think there's a lot that remains for these systems to scale up to, you know, full videos with audio, you know, with transcription, and, and for that, I think we need more of the inductive priors to be inserted into deep learning. And, you know, that's where I think we can get inspired from, you know, learnings from neuroscience and psychology.

Unknown 1:56:27
You know, of course, it might be worth seeing that all this discussion that we've had so far is on the sort of different impact of attention on processing the selective effects, but implicit maybe in the question, and I think another place where there's really not a lot of deep understanding is what drives the selection of what you're going to attend to. So there's sort of two halves to attention, right? There's the afferent part, like what's driving what you're going to attend to? And then what is how does once you've decided that the system work to select just that information, most of the discussion has been about the latter, I think part of the force of the talk that I gave was to say, look, it's at least built on apparatus that is the same as what it's acting on. That is how you get to select is depends entirely on how you represent information, you can't select something that you don't know how to represent. And so there's this sort of virtuous cycle, it's virtuous in the sense that it lets us work, but it's not no so virtuous in terms of well, it's not that it's not virtuous, we don't understand where the virtue comes from entirely. On the afferent. Side, how do you decide what to attend to, and that's a whole level of sort of evaluative processing that AI has barely touched, I mean, the experimenter says, You need to play Go, and doesn't care about where you get your electricity from, who's gonna pay the bill for that electricity, or anything else. And yet, we as agents have to worry about all of that, right. So I might decide to play Go now. But I might decide I need to go out and make some money in order to provide some food so that I can continue to play cup, and none of that yet, is really very well developed on, you know, on the side of AI that's looking at these, you know, at these sorts of complicated tasks. And, and that's something also that neuroscience has a lot to sort of learn them about. But but that's the other half that I think doesn't often get talked about, and yet is key. And, I mean, maybe that gets talked about as control that's, that's the control part of value of select, you know, sort of part that contributes to what you how you decide to control, but it's part of the same system. And so if we're really gonna understand stuff, we need to understand that sort of input as well as the output.

Joscha Bach 1:58:42
I agree with what you just said to understand you. So you had two points that you just made? The one is, is it what directs attention in the first place. And I think that attention gets directed like the rest of our motivation by expected reward and their average, pay the increase in expected reward. And ultimately, that is going to motivate the entire attention, but the primary reward that we get, and perception is probably an aesthetic reward, which means it find more sparse, more efficient, more coherent, more stable representations, or as your show of NGO said that he says, suspects that consciousness is basically a function that is trying to find the largest possible dip in the energy function of the mental representation, which means that Representative rotation gets optimized by setting parameters in the search process. And I suspect that what our attention can do is the synthesis of programs in a way not just converging to state, but actively searching for the state and a construction process and that might be a crucial thing for our type of attention. When you said, Can we all express the sole multiplication of inputs? At some level Yes, because this multiplication of inputs is general enough to enable symbolic architecture to select features and to amplify them or to inhibit them. And when I built cognitive architecture in the past microbe side, which combines motivation with your symbolic representations, we used activation fans that spread out along different axes that correspond to neurotransmitter dimensions, basic chemical message types, through the neural network, and then you can use the multiplication for selecting innovating and processing. But the question is still how many bits to go in this multiplication, sometimes this might just be one bit, right, which means you have a non linearity. And if then, not, sometimes it might be four bits. And it's probably when it's more, it's quite stochastic. And you need to combine multiple multiple applications and redundancies to even that out. So I would say that at this point, we basically look into a shared direction when we talk about attention. But we don't have a unified theory of attention at this point, at least, not across all of our labs or our groups. And, but we are at a stage where we already find each other very inspiring and useful, and can communicate across teams. And that was also the goal for putting this panel together. So I think that we are very successful today in presenting very different perspectives that at the same time, were able to complement each other and learn from each other and also give something to the audience. And I would like to thank everybody, for taking the time today to join us in this panel. I thought it was excellent. It was a great pleasure to be with you here. Jonathan Michael and Vasudev. And, Tanya, thank you for moderating this and I'm grateful to Intel Labs for allowing us to do this and setting up this series of meetings. Thanks, everyone. Thank you, everyone.

This transcript was generated by https://otter.ai