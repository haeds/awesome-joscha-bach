Unknown 0:00
Hello, everybody, welcome to current session three. AI machine consciousness. We have five speakers, I think two are? No, I think more than No, I hope, at least you are here or Larry is alerting here. Anyway, we have two or three, live and two or three remote. The first speaker, I'm pleased to introduce someone who's spoken at our conference on a number of occasions, Yoshida Bach, from Intel Labs in California. The origin of consciousness in the evolution of governance in the society of mind, that's what got me OSHA.

Joscha Bach 0:44
So some preliminaries to get out the way what you're talking about, in which way we understand it here. Consciousness is colloquially known as the feeling of what it's like. And I separated into the awareness and attention to features and objects, which I call content consciousness, then we have access contents consciousness, which is attention to the mode in which we attend, for instance, whether the thing that we are attending to is hypothetical or conditional or treated as real. And then we have reflexive consciousness, which is attention to attention itself. And to me this question by the being is conscious amounts to the question of whether it's acting on a model of its own awareness, whether it's aware of the fact that it's aware, and this is reflected in its actions. And in this way, I can conclude that, for instance, a cat is actually conscious, because the cat is aware of the fact the fact that the cat is aware and is able to communicate about these states of awareness, this meeting recognizes my own awareness. The functional role of consciousness is sometimes debated, I think that it can be understood roughly as the conductor of our cortical orchestra, it's the purpose of the conductor is not to play the music, the purpose of the conductor is to make sure that the music is coherent. If you're unconscious, for instance, if we go up at night, it's the booking, then the instruments can still be playing, you might be able to open the fridge and concoct a meal and so on. But it's just nobody who was making sure that the orchestra is playing a unified tune. So this potential event is the integration of what's happening, that is the main functional role. And this is congruent with some other adjacent models. For instance, Michael Gracia knows idea that consciousness essentially control model of attention, or Yoshua. Bengio is idea that consciousness amounts with the search for a low dimensional discrete function that minimizes the energy state of the perceptual model. It's basically an attempt to govern your perception in such a way that you can find a set of parameters that make makes reality snap into a piece where everything makes sense. You know, this thing when you wake up in the morning, and you try to get reality to make sense. Or you try to get the features in an arrangement that gives you a cohesive model of reality that you're part of. This is like taking breaths, and imagine you would not be able to take that breath. And it would be like drowning. In a world that makes no sense. What's the ontological status of consciousness, there's a lot of confusion around the obvious fact that physical objects cannot be conscious, because they're just mechanical, and mechanical processes themselves are not conscious. And biological systems can also not be conscious, per se. So cells probably not conscious and organism is itself not conscious, because it's a mechanical arrangement of cells. And there is a coherent pattern the activity of the cells that coordinates them and gives an advantage. And this these coherent patterns are able to perform modeling tasks, and would be very useful for the organism what it would be like to be a person, so it creates an SF person, if it's completely virtual. So consciousness is a virtual property. When Christoph says that, simulations cannot be conscious, only physical systems can be conscious, I think he has it exactly backwards. You can only be conscious in a simulation in a dream, physical systems cannot be conscious.

Joscha Bach 4:09
So there are a few questions about how consciousness comes about, for instance, that our genome was pretty sparse, there's not that much data in the genome to decide how our brains would be wired up. And our mental states are extremely complex, and they're very diverse. And they also very robust. So if you have the mental problems in your code in your genetics, or due to endocrine disrupters, then your brain will still find some kind of order that makes you conscious and makes you somewhat coherent. And also, we are very adaptive in the sense that when you put us in a different environment, you might develop a different mode of interacting with the world in a different way to relate to our own self. So how is it possible to implement all this with neural circuitry that is encoded in the genome? And there are some interesting ideas that go away from the notion of circuit as they are existing in our classical computers, for instance, Steven Grossberg has written a fascinating book, conscious brain, resonant, conscious mind resonant brain in which he explains how consciousness emerges over resonators that are implemented in neurons. And Jerome boozer Meyer explores the idea of using quantum probabilistic models to describe mental states not because you think that the brain is literally a quantum computer, but because he thinks that the mathematics that we need to understand mental states have to account for the superpositions. And in the way in which the mental representations relate to the substrate and themselves, the response is a collapse of the observer that happens in the brain. When we become conscious of something when we collapse probability space into a single interpretation and start to interpret our own history as if it had been a sequence of events, while it was a superposition of possibilities.

Joscha Bach 5:58
Another perspective comes from Marvin Minsky, the Society of mind, where he treats the mind as a collection of interacting agents, that each of these agents is self motivated. And as a result, you get a self organizing process. And what I really like Gary, ailments idea of Neural Darwinism. So this idea is that the organization of your mind is not something that is completely defined in your genome in some sense, and then impose some kind of pathways and circuits in your brain, but rather, there is a potential of different orders, and they fight it out. So there is an evolution of possible mental organization in everybody's mind to our own development. And this evolution is, of course, rake. So the outcome is predetermined by the way in which this genetics are set up. But this means that you do not need to set it up in detail, you just need to set up the bifurcation points, the criteria for which organization is better than the other ones some hints to, to get closer to one organization or the other. So in some sense, it's comparable to the Darwinian evolution between different systems of social organization in society, where there are somewhat optimal social organization, society, depending on the economy and scale and context they are in, and the order that you find something that takes often generations to develop. But it's an evolutionary process by which political systems develop. And so in some sense, the organization of ourselves and our conscious mind could be understood as some divinium evolution for different forms of governance. And what I want to introduce here is an implication of this, which is, what is the seed for this conscious mind? What is the seed agent that is branching out. And to understand these principles of self organization in nature, you need to understand some ideas from cybernetics. And open loop means that you are acting on the environment without getting any feedback. And most of our robots are in the sense open loop, the industrial robot grabs a piece, expecting that sort of a certain position, and puts it into a different position expecting that its notion of space is still correct and not out of alignment. And if it goes out of alignment, somebody else needs to come in and recalibrate. But if you give the seep the system feedback, you have close loops, and you have adaptive control, so you can adapt to changes in the environment. But what's crucial for organisms is and for social systems, that is that they have extending loops, that they branch out and build new loops into the environment, that the probe the environment for things that can carry it out them and become the substrate. So you take the chaos that you find around yourself in the environment and turn it into complexity, complexity that you can settle that you can expand into that you colonize. So these extending loops require some kind of minimal seed, something that's able to turn chaos into complexity that is able to branch out. And this basically is the foundation of the Inside Out design of biological systems. Now, what is inside out design, it's the opposite of outside in design, technological design. When we design something in technology, we start out with a deterministic lab, or workspace or floor. And then we have some substrate that we extend the determinism into that V structure using the tools that we have. So you go outside in from space that we know how it works, that you completely control and extend this control by building new machinery. And biological systems don't work like this social systems don't work like this. They work with seed. And this seed is branching out into the world. And sometimes when you have a tree this initially you don't have a tree initially you have something that wants to become a tree. So you need a kind of organization that is a precursor to a tree and that is able to branch out and your organism grows. You do not have a single seed that colonizes. But in multicellular organisms, you have many and the cells can rely on each other because they have a shared destiny and they are have mechanisms that are very similar to each other. So they can link up and form an organization on the next level. Some kind of next level agent.

Joscha Bach 9:53
What is an agent? simples form of an agent is a controller for the future. No control over In cybernetics, the famous example is thermostat, the thermostat itself is not an agent, right? It's only acting on the present frame, it doesn't want anything, but give this thermostat the ability to model the future and integrate the temperature differentials of the future. So it sees that the world is branching into different paths, depending on decisions that the thermostat is making. At any one point, suddenly, they have all branches that are preferable, not preferable. And now you have a system that is acting on plants and has intentions. It's very, very simple, you just need a controller that is able to control the future. And to control the future, of course, you need to model it. And to model the future, you need to have a system that has a causal structure that is different from what the world is now, right. So you need to insulate something you need to make your own universe, your own universe in which the future plays out, despite the universe not having happened yet, in your universe, where you actually live, right to build this hypothetical universe in which the future happens. Now, even though it's not yet happening, the minimal thing that you need is a computer, you need a system that has an independent causal structure. And the simplest computers that we know to exist in nature ourselves. Cells basically have readwrite tape like a Turing machine with with a read write head on it, and that they can change state. It's more complicated than this. But they're basically computers, among other things. And this allows cells to regulate future states to adapt to anticipate future states to anticipate disturbances that hadn't happened yet and to regulate for them. This gives us insight into a hierarchy of causal systems at the bottom of the hierarchy are simple mechanical components. The next step are controllers. And then we get agents which are FP computational computation, and they have integration of future rewards. So they expect what the future is going to be like an act on these expectations. And then the next step, you have group agents, group agents combined the individual motivation of their members like individual cells, this the greater whole, and they get subordinated to this greater whole the individuals. And this happens, for instance, in a pack of wolves, or in a family unit. And then the next step above, this is a state building agent. A state building agent is a group that scales beyond reputation systems beyond knowing each other. And then you have infinitely scalable state building agents. So it is in relation between the group and the state is very interesting group agent, like this pack of wolves works because all the members know each other, and they can model each other. And they can have an economic exchange, it's transactional in a way. In the state building system, you interact with strangers and you interact, expect them to behave well based, even if it's not locally in the best interest. So this requires some kind of domestication and sales or state building agents and therefore multicellular organisms. And ants are famously state building agents and social insects. And humans are state building agents. And you have not always been state building agents. It's a feature of domesticated humans of the modern human homosapiens, that we enter these infinitely scalable states where we willing to act on rules, even if you don't understand where they come from this domestication is extremely crucial for our success. You're not the smartest woman but we are the programmable hominid. And their inventories infinitely scalable stapling agents are difficult to achieve. The larger social services become, the harder it is to set the incentives for governance correctly, right. So this is a problem that happens to all organisms, there are basically almost no organisms and states in nature that can grow indefinitely. And it's because of the communication overhead. And the trade off is editability. But some of them have found a solution. For instance, what you see here is a curve of trees, and they're all a single organism, they're all connected by the roots, and they're all clones of each other, they're basically the same tree. And this thing can grow, but you cannot adapt. It's basically frozen in time. If, if they would update its genome, it would not fit together anymore. And another example is species of South American departments. And they form infinitely scalable colonies, and have taken up the habitat of many species of ants in Europe, for instance. And apparently, human states are to some degree scalable, almost indefinitely, so on we don't know what the limit is for this. But do you have a trade off and you do this you have a communication overhead, or you have overhead between trade off between activity and coherence?

Joscha Bach 14:34
So the crucial thing to make an organization work is to have a government and the government is an agent, that is making your individual behavior compatible with the common good by changing your payoff matrix. So making sure that you get the right rewards and punishment. So there is an offset to your local payoff. To make you actually act on your own incentives and by acting on your own incentives. You are acting on the common good and setting the incentive Just for governance, right, is the most crucial thing. And in some sense, part of the role of consciousness in our own mind is to create this coherent governance to act as this conductor. So the thing that emerges in all of these systems is something like a virtual operating system, or operating systems or virtual, it's virtual machines to control the future. They exist in organisms, minds, groups, nation states, and so on. In the old word for these operating systems for the software that emerges in the Korean interactions of individual agents is spirit. And it's this bird that we have forgotten in our civilization. But the word was needed before and there is a non superstitious reading of what the Spirit is, it's really an operating system for an autonomous robot invented at a time when the only autonomous robots known were things like plants, and people and nation states and cities and ecosystems. And some spirits are a sentient, not all of them. And sentience is the ability of an agent to model the world so deeply that the agent discovers itself in its interaction with the environment. So it begins to know what it's doing. And many social organisms are not sentient, so we can build social organizations that have agency and that people serve, but not at all of these groups that emerge, know what the interaction with the world actually is and what they are. Our western world tends to be very confused about consciousness. There is this notion of idealism that we live in a dream. And I think that's basically Correct. There's also the notion of physicalism, that there is a causally closed mechanical layer at the bottom of everything, that also seems to be correct, because we have to explain why everything is so regular, right. But we are confused in our society on how to put these notions together, that we actually do not live in the physical world, but that we live in a dream. And this dream is dreamt by a mind on a higher plane of existence. And this higher plane of existence is physics world that nobody has ever seen, and can never see. Because you cannot be conscious out there in the physical world. You can only be conscious in a simulation created in the skull of a pregnant. But it's something that our culture has known before, and many other cultures know. So one of the oldest stories that we have in our culture is Genesis one, and we get taught about it from the Christian perspective, which ceases as a bootloader, for a cut. And in this reading of the Christians, Genesis about it's about the creation of a physical universe by a supernatural entity. Right. And this makes no sense because the entities that have been described in Genesis are not physical entities, there are no colors and objects in physics, they are created by our mind, to make the patterns that are systemic boundary to the physical universe intelligible to us. Right. But if you if you take this reading, and you reread the whole thing, turns out this is a six stage theory of development of consciousness and personal self. Stage one is at this creative spirit, which is in the in Genesis is called Hua Elohim created two domains, the domains of the ideas and the domains of the world. Decart calls disk, rest extends and rest. Cognitive means the world of stuff and space, and the world of ideas, and both of them are in the mind. And it starts out this whole story by the creative spirit moving over a substrate that is to Babu it's void and formless. And then it manages to produce light out of the oscillations of neurons, and separated from darkness, so you get contrast. And then it associates the light with brightness, and the darkness is the absence of light. This is the night. At stage two, this thing starts to separate the contrast into space by measuring extensions along several dimensions. And it separates special regions. And it divides between the world and the ideas, this area where you have stuff in space, and stuff that is not in spaces.

Joscha Bach 18:55
And stage three discovers that there is a plane that is you can associate with the ground, and that there's a space above this plane, which is the sky and everything below the sky. And it discovers how to model solids and liquids and organic shapes basically builds a game engine, our brain that is able to model reality. In stage four, it discovers illumination and the constancy of shape and the changing light. And it discovers that information has to come from something that just covers the light and the sky, the sun and then sometimes changes and that changes with the flow of time. In stage five, it discovers all the plants and animals and starts giving them names and it develops language. And in stage six, it discovers that its purpose is to control the world and its interaction with the world and that there needs to be an agent which does this. And so it goes to work and it builds an agent in its own image, something that is able to comprehend reality and act on it. But it does not create them as this creative spirit it creates them as man and woman as a personal self. So somebody who thinks they're a human being in a social world. And when you observe children, you notice that there is a phase after they're born, and they are not people, then there is no person, but they have this unified creative perspective where they are just observing reality as it is. And then it switches, there is a person that they are describing, and initially that describe this person very often the third person. And then more and more often, they use the first person perspective, because they start to model the world from this new perspective. And you will find that a break in their memories when they switch over, because they seem to reindex the memories from this new perspective, there's a new relevance, right, there is a new self emerging in the sentient mind. And I think that's a very beautiful theory that approximates pretty well of how modern cognitive science sees the development of the mind. And does this I want to enter talk, my time is up. The illustrations that have been used recently have been created by Delhi and new generative AI, that has been trained on the few 100 millions of images by open AI. And this image, which you see here was generated by the prompt the generation, the creation of the emergence of consciousness from the universe automaton. So this is what the AI thought after having looked at 400 million images and doing statistics over them.

Unknown 21:31
Thank you, your ship for an extremely interesting talk and finishing precisely on time, we have time for a few questions. And step up to the microphone. Here comes Sue. Wow, he

Unknown 21:47
was gave so many amazing ideas that I'm kind of reeling. I'll just comment on to one comment. And one question. comment is I really love the idea that not because Kox wrong, because I you know, I don't mean that meaning? I mean, that that switching around from is the is it the physical, that we've got to understand clearly the problem, the hard problems, so you get rid of it. But in this way, what surely no, you haven't really got rid of it, have you? But what what I found so interesting was back in 1986, I wrote something like that, and then lost confidence. What I said was, there's nothing it's like to be a bat. There's only something it's like to be the bats idea of a bat. Would you agree with that? Yes, I would. I then lost confidence because I didn't really know what a representation was. And I've been playing around with representational theories. And that's what you're doing. So I want to look much more into into your work there. So that was just a comment. The other thing I'd like to ask you about is the Neural Darwinism of Edelman, which appeals to me very much from my work on replicators, and memes, and so on, and the competition between constructions between representations. So would you say, then, if you if he's right, and what's happening is kind of fight between different representations going on in the brain? We will do take a pan psychist view on that, and say that all of these competitive things, there's something that is, this was what I feel myself forced to, and I don't normally speak about it publicly. But would they all would those competing things, that'd be something I'd like to be each one.

Joscha Bach 23:26
I think of Pan psychism, as the idea that consciousness is out there in physics, but because we cannot understand how the physical mechanisms give rise to this by the interaction between physical parts, it's an intrinsic part of matter that you cannot split off from it. That's indistinguishable from consciousness being outside of the universe and generate an apparent universe. The issue is with Pan psychism, that an ontological sense that people who believe that it's actually primary and matter is generated from consciousness is that consciousness seems to be very complicated, more complicated than meta. And it seems to be compositional and constructed. Because I noticed that I can deconstruct my own consciousness, I suspect that consciousness and it might be something that is intermediate that exists between discovering your self and your second order attention, and deconstructing it. And it could be that basically, when you live long enough, and you were able to observe yourself long enough that you would get into a state where you just couldn't construct all your quality. And you might have noticed a phenomenal experience for qualia, you can deconstruct them, you can take them apart into components that are no longer qualia. They're just representations. And typically Buddhist meditators call this going towards enlightenment states, very basic, everything becomes apparent as a representation. And so I don't see pan psychism in the sense as a useful theory, I see it more or less as the insight that the world that we think is the physical world because everybody in school tells that what you touch here is physics. And you realize that's actually a dream and it's supervenes on my mind, and that's actually true, right? Because the thing that you observe and you touch this thing is not out there in physics, it has to be generated in your mind. It's a simulation generated in your brain. And when you deconstruct yourself and the simulation, you realize that it's a mind below it. And that mind is not you. That is the creative spirit.

Unknown 25:19
That's taking this and another. Yes, sorry. I still want to discuss later.

Unknown 25:26
Yeah, yeah. I appreciate you at all, thank you. But I noticed that that you refer

Unknown 25:39
only two

Unknown 25:41
reflective consciousness and reflective self consciousness of liking your first slide. But according to me, that is only an alpha of the phenomenon, consider the non reflective consciousness like mirror self recognition for, for instance. Is this able to make the difference in your account or not?

Joscha Bach 26:11
I'm not quite sure what you mean by non reflective consciousness. But recognize yourself in a neural you don't need to be conscious, you can build a robot that is not conscious, and is able to recognize itself in the mirror as a complicated pattern. And some people argue that today's neural networks might be slightly conscious, I'm not convinced that they are. I think that they do not have an awareness of the fact that they are aware for the most part, but they're able to create a coherent story to some degree about something that is, so at this level, at this level that you have this multimedia narrative of officing that conceptualize itself as conscious inside of the simulation the simulacrum of a conscious being, you can have that conscious but you do not have it as the level of the mechanism that recognizes itself in the mirror you don't need to be conscious to do that. You just need to be making machine that is good enough at pattern recognition.

Joscha Bach 27:15
Gotta go the next speaker Thank you very much.

Unknown 27:23
Yeah, let's give your show great.

Joscha Bach 27:32
The next speaker is th o Perconti from the group of Antonio Chela and Ricardo mon sati. And Pietro is working at the University of Medina it's Oh, it's you amazing Thank you. And he is presenting an avatar to deal with psychological distress. And this avatar is using ideas from the research into consciousness and so focusing on volition and embodiment and valence and so on integrated in the model I'm very curious about your thoughts

Unknown 28:24
Okay, the this talk is co authored by the Antonio K lair Ricardo zotye Maybe the attend the conference but in Europe is we are in the middle of the night

Unknown 28:47
okay, the outline is basically DVD divided into two parts. So the sketch out of the project Alter Ego the focus on the trust and deference in humanoid robots Well, the big grand idea is to take advantage from the nowadays techniques, technological advances in chatbots in assistant like Alexa or Siri and so on, or chatbox and website and to produce a more sophisticated avatar endo with with two traits. embodiment and the attempt to have a personalize it, the avatar an individual Not a simpler example of an nm in a cognitive architecture the existing goals Laxus essential aspects of successful human communication all all of us are aware of these we are upset a lot of time speaking with Alexa or Siri. So, how to get an ecological in interaction between humans and machines you need a lot of capabilities or abilities of course, from social coordination to mirror self recognition and so on. And in in what follows I will focus on the issue of tasks and read and difference in the relationship between human robots and humans we have we have a social problem, the problem and to be to becoming familiar with robots spatially with humanoid robots and the question I would like to address here, here's a How does people could become familiar with shots artifacts and what kind of problems does such familiarity race we have on the whole the three kinds of problem ergonomic problems, cross cultural problems and more problems. Ergonomic problems are articulated in into two parts, classical ergonomics and cognitive ergonomics classical grammars go because after all human robots robots are physical object. So we have to deal with this problem. But they are artifacts and here we have also cognitive ergonomics because artifacts are in though with with the psychology on the own like cars for example Castle sometimes seems to be smiling son sometimes they seems to be aggressive they're our designer are perfectly aware of these and produce these feelings in us we have also cross cultural problems consider for example the Japan case and the different attitude Japanese have to wear it uno Roberts in comparison with the Western people Japan is the the best place in the in the word in humanoid robots investigation. This is not surprising of course, Japan is an amazing country, but my point is their cultural attitude towards humanoid robots a positive attitudes on the contrary, Western people are characterized by a twofold attitude toward humanoid robots um on one hand, Western people feel fear towards humanoid robots or on the other hands, they would like to enslave them and we can call these like a Frankenstein syndrome typically in Western countries, not in Japan and I there is not empirical investigation about this. Unfortunately, we can only speculate and the speculation can can be based on the preference for impersonal relationships typically change in Japanese. Just forgetting if you consider a Japanese in in choosing between a BA to get a ramen for example from a restaurant or from a vending machine maybe she chose the vending machine and it is well known the Japanese this this question to well what's the most intimate and personal sphere maybe this is able to explain the different attitudes Japanese and Western people have towards the humanoid robots but this is only speculation we we need empirical status bowlful we have a lot of moral problems

Unknown 36:01
and these kinds of problems arise arises from sale for driving cars from killer robots from sex robots and so on. moral problems can be a dealer with due to different accounts and two different problems. We have two problems first of all, what are the ethics needed to govern relations with machines that have intelligence and what techniques do machines that are equipped with with intelligence and self awareness how the first question is about how ethics to be used in relation to machines. And the second question is devoted to the ethics that machine themselves should have. And of course, there is the world known to fold accounts from top down approaches to bottom up accounts that Multan app counselor are kind of developmental robotics. Like from by Cangialosi from MIT Press Okay, autonomous driving cars. Bryce had a lot of problem you know, but I have to to go to going on and it's better to consider maybe a killer robot so especially for in this week's in which he had the war in the east, east Europe. And the ban against the killer robots is saucing significant in this moment. This ban exists and a lot of country around the world ban killer robots it killer robot is a killer that is is not deferential towards human beings in the final choice to kill but they killed by themselves. And these are is ban it

Unknown 39:20
I don't know if these ban is really realize it, apply it but there is a banner after on the left or the of the slide you can see Armani. Armani is a kind of doll. A sex doll. The bat. Not exactly the sex dolls. Tp going over the last decade is a new kind All sex dolls and people say that they fall in love. They want to marry, marry the harmony. But is this really a right or not? According to Kathleen Richardson, not she proposes a campaign against sex robots. This campaign was until now answers unsuccessful and I would like to add fortunately, according to Regis, when indeed we have to ban sex robots, because the relationship between human beings and sex robots is based on the idea of exploitation and exploitation is it is a bad thing among human beings. It is a kind of pedagogical concern that or covering breach also maybe it is an exaggeration according to me, sex bots are ethically neutral. And in fact, for example, you can consider a an as an alternative Fe t shirt account about sex robots and you can consider the interest towards sex robots like it a fetishistic interest and in this regard, the ban sound the absurd, senseless because a ban on fetishistic propensity seems to be nonsense. how find a solution. This way, I think, first of all, we have the we need the ecology. And we need to consider that in the environment. There are a lot of antrum of anthropomorphic mental triggers, when we are faced with when we find these cues in the environment, we feel the attitude to attribute the intentional states to that object, no matters if that obvious object is a really and doorway with the mental states. My proposal is ontology golden nail, I can buy any ontology all this is inspired by the intentional stance by Daniel Dennett or Alan Turing in The Imitation Game. Here doesn't matter on charge, it is just a matter of discover the grammar of the attribution of the mental states to other objects and

Unknown 44:03
in the environment, sometimes we have sound cues like the attitude to classify different leaving from normal living creations. We have, we are able to distinguish in intuitive way way without any education, biological motion from artificial motion, the attitude to recognize meaning meaningful face, in them in the environment, in percepts, you in in a given perceptual, perceptive pattern and the ability to joint attention with other people and to follow their gaze when we are phasing in viral In the in the environment with these tools, we are prompted to consider that object like a rational agent, females a logger and he is cold a cold colleague us from the Donders Institute in Nijmegen in the Netherlands proposal, the sophisticated modal how on how tasks the works in human robots interaction and you can you can see that the appearance behavior style performance, not reputation and similar to the mental triggers above mentioned it and we have to consider the context and the human propensity to tasked robots based on the Eastery all the interaction and other things the outcomes of trusting that way seems to be miserable

Unknown 46:34
Well, this about tasks, but we need something similar about deference. Indeed, trust and deference are two main epistemic component parts of social cognition Theory of Mind mentalization deference is not a popular epistemic attitude, nowadays, deference is not a good word now, because individualism and in then a lot of good values, I completely realized that that but I think that we have to recall their friends from the past, because deference attitude, differential attitude, was able to produce amazing achievements in human history. In fact, the Why am individuals in individual life deference is often associated with the importance lack of enterprise submission, like in other animals, the cases in social interaction it is in in an essential component of a cost tactile way of living together. Social difference indeed is not a social mechanism of submission, but an intelligent strategy aimed at maximizing personal utility and building productive social relationships. It is the attitude that leads the individual to take advantage of the best confidence that's that's other people has to be the a given area of knowledge and experience. We have We are differential when we rely on a physician to diagnose and treat the disease, when we call it a repairman to fix an appliance or when we consulted vocabulary on the meaning of a given word, and this is not a treat of submission, this is madness. And we have a differential attitude towards calculators. In the calculator case is not just matter testing because the calculator but of being deferential to eat bad we are ready to have the same attitude toward grace. A new a new humanoid knows maybe not. But But why massively, massively. De France is a behavior a behavior or attitude while tasked is an intentional, a pure intentional state where word difference can also be associated with the mental states in this case, we have differential attitude and this is my take to home take home message, the final message selective difference is an epistemic attitude able to create a year or Key among the kinds of social knowledge to be most deferential towards, and the others in which the subject instead has a direct commitment to the semantic content of what she is saying. And this is important in the case of Avatar, because we have to distinguish the case in which are but here it is better to call back and the become another way in other time in church. Okay. Just to advertising in, in the hand, a book a forthcoming book on these few theories, the landscape and and to hand just the invitation to join us in the next conference in Italy. In next year in in Sicily, Italy. Thank you

Joscha Bach 51:38
very much for your presentation. We won't have time for questions, unfortunately, unless our other speakers are gone AWOL. I don't know, if somebody's technical setup here who can help? The next speaker would be tawa O'Leary. And I don't know if he's here in person, I suspect that he is calling in from from New Zealand and I hope that he does.

Unknown 52:04
Yes, well, he's not I am I'm this current speaker.

Joscha Bach 52:10
is here, or is he a president? Sorry, I didn't catch that.

Unknown 52:16
I'm speaking on his boss.

Joscha Bach 52:18
Oh, okay. Just so you are?

Unknown 52:24
Yes. Just

Joscha Bach 52:26
okay. What you are presenting is, I think a proposal for a virtual world in which we can gather resources or a virtual agent so and developed its own agency. And we can use this as a paradigm to explore consciousness. That's correct. As the sole showing, please introduce yourself. I only know a little bit about Tonga, O'Leary, but who is an analytical philosopher in Auckland. And it is a PhD, at least in analytical philosophy. And you are in lab for Strong Artificial Intelligence, which I think is awesome. Please tell us about it.

Unknown 53:04
Take it away. Okay, so I am Dr. Josh Robinson. I'm just standing in for Dr. Larry who had something come up. I myself am actually a psychologist, turn computer scientist. And what I am presenting today is a project that we are trying to get off the ground that is an attempt at studying consciousness through the evolution of consciousness through simulations. Yeah, and because this is our proposal, we'd be interested in any feedback, oh, emails up here. Also, I believe an alternative email for there is in the program itself. So what we are actually planning to do is, like I said, study consciousness through through simulations. Now what we try, what we are planning to do is to create a world that replication is as close as practically allowable of Early Earth ecology, and then using different AI agents with different complexities, which we'll get to in a second, running them through the simulation to see what they can achieve. And using that as a way to determine what properties are important for the evolution of consciousness. Now, the reason we're doing this is that there is you know, mounting evidence that things such as sociality and other certain problems we had in our ecological niche played a pivotal role for determining what how we evolve our consciousness and other cognitive components that are critical to the evolution of consciousness. So the general idea here is that consciousness released from L. Definition of consciousness is that it didn't just appear out of anywhere, but it was built upon some other cognitive concept So that was selected from the environment that was selected for a reason, our belief is that, you know, there are specific problems they help them solve. And if we can show which cognitive components were selected to solve those specific problems, or at least provide evidence for that, then our goal is to show possible plausible evolutionary pathway to consciousness itself.

Unknown 55:23
Now

Unknown 55:28
maybe like we said, we both work in an AI Lab at the University of Auckland. And the interesting thing about AI, I don't have to tell you if the AI and consciousness session, but it can actually help us inform our understanding of consciousness through a couple of different ways. There are at least two that I know of one is, you know, determining what counts as an artificial consciousness. So this is a if you work in AI, you know that what you can do now, you couldn't do three years before that, and three years before that, and so forth. Basically, AI advances at such an exponential rate that we are developing machines that are becoming more complex as time goes by more intelligent, depending on your own definition of what intelligence is. And at some point, you do start to wonder, when do these things count as being conscious? Now, this is obviously a difficult problem for multiple reasons. For example, if you went to any of the other conference that are on right now, I'm willing to bet you define different definitions of consciousness. In fact, I'm pretty sure the other speakers in this session have their own definitions of consciousness. So you get into this problem of, you know, how do you define consciousness? And if you can't define it, how can you tell what should be considered as conscious? And again, this applies to other animals, plants, anything you want to determine whether it's conscious or not, you need a definition. And that's where the problems come in. As we're focusing on the second point, which is the idea that if you were to try to work out what's important for consciousness, and why is it important, so what you can do is create a property that you believe is associated with consciousness and put it within an AI model. Now, that's reason you do this is to see whether having that property helps the AI model solve specific problems. So this is the sort of work that I've been doing recently, and is the core behind this project. If we were to put in say, something like a theory of mind, what does a theory of mind help the agent do? And so can we find evidence that this property exists for a reason, and it's not just something that is good, but you know, something that might have been selected for specific reasons and to do with conscious to do with our evolutionary history?

Unknown 58:02
So speaking of El

Unknown 58:08
evolutionary history, um, so we are approaching consciousness from the evolutionary perspective, I know that there are many different ways of how to Yeah, theories of how we develop consciousness houses, yeah, coming from the Estonia point of view. And basically, what we're saying is that there were certain evolutionary pressures that were there to us developing certain cognitive components. And by developing these components, things such as attention, communication, theory of mind, by developing those and a bunch of other components, we use them to solve problems, that meant they got retained to help us, you know, due to selection pressures. And then these components were able to start to interact with each other. In other words, you might have a tension, say, a communication in theory of mind start to interact naturally, as you learn how to discuss you know what you believe someone else is doing something like that. And you will find that that somebody helps you solve more problems. So, because you have these base components that were selected for their own reasons, they then get a chance to interact. And some of these interactions lead to better improvements, but also increasing complexity. They then they get retained, and so forth. And so you get this general hierarchical format formation of all these cognitive components. And at some point, consciousness evolved from this. There's of course, assuming you're talking about an all or one all or none theory of consciousness, you know, some of you were unconscious, and some of you are, you could also make the argument that you know, different if you think that there are different levels of consciousness, then you know, the different combinations of these components would have different levels of consciousness. But the point here is that we assume that consciousness evolved from somewhere And that was due to our ecological niche. Okay, so what are we actually planning to do? Well, we have two separate components that will be co developed and obviously interdependent. One is our environment, which will be based on early Earth ecology get to that in a second. The other is the AI agents themselves. And we are planning to build these simultaneously and obviously, keep iterating them. So the idea will be, we'll build the environment with specific problems in mind. And then, you know, we'll test different levels of agents with their own different amounts of code Live Inspired components. But this could be a bunch of different, more different models that we intend. Now, the goal here is that we will be continuously developing. So think of it as an arms race, if we build the environment of some problems, and then we find that our agents can solve them quite easily. Well, we need to make those problems more difficult. And obviously, they will cause a rise in the agents, as you will need more complex agents to solve those problems. And the thing we are planning to do here is to make our environment open source, so once we complete it will, it will never be truly complete. But once we've made a first version of it, we have hoping to release it to everyone else. So that people that want to work with us or have their own things that they want to test, they can work with our environment. Okay, so as for the environment itself. Now, the thing we will be focusing on is early Earth ecology. We know that there are a lot of different AI environments out there. That's not a problem. But for us, because we want to focus on consciousness and its evolution, we need to look at our evolutionary history. And things we will be doing is creating very complex but ongoing problems, if you want to think of a high level things like resource gathering, and predation. Now, it's going to be a bit more complex than that, because you can think of Pac Man as being a resource gathering problem, you know, you're hunting darts and things are hunting us. So then there's predation, we're gonna go for something a little bit more complex than that. Think is in a situation where you have, you know, water in one source food and another source, but you don't have enough.

Unknown 1:02:47
You don't, you can't just eat at one and then move to the other to solve your problem, you need to find something to help you basically learn how to bring over food to water ordered food or to a third party location. Because we are also interested in things like sociality, we will be eventually building this up. So we will have multiple agents working at the same time, they can do that cooperatively or competitively. Again, our goal is to create the agents add the components and see how these interact without earlier ecology. So you could in that previous scenario, imagine the situation where the group splits up, so I'm gonna gather water, so I'm gonna gather food, and then they bring them on to this location that they're defending from predators. That's what we're aiming to get to, of course, that would take a lot of time. But that's the general goal of what we're aiming for, can we keep solving more complex problems with more complex agents. As for the actual implementation, the first version we're going to do is going to be a 2d grid. Now, again, for those who are familiar with the idea of a grid world, which is where you have basically an agent moving around a grid, trying to find a reward and avoid pitfalls. We're gonna get a little bit more complex than that. Yeah, it's, what we're doing is not completely different from other things. It's just we're making something that is more complex than what you would use for a standard AI agent. Because we're not interested in just making smarter agents were interested in learning how do different components of the AI help the healthy agent solve specific problems? Okay, so the second part of this is obviously the agents themselves know, they're a a lot of different ways of how you can build a conscious agent. I know, they generally fall into two parts. One is you define your architecture, you put them all into place, and say, Okay, this is my conscious agent. The other strategy is you start with something really simple and start adding additional components to make it more complex, right. So that's the second strategy is the one we'll be building with. To begin with our agents will be nothing more than A simple random agent, and then we will build things in like trophy cases. So you know, move towards the specific stimuli, then you can build a bit of reinforcement learning on top of it. And you know, we will keep adding these components in as time goes by. Now, the thing is, we are assuming that these components are useful. And as you make an agent more complex that generate performance better, but what we want to know is whether you gain what you gain and complexity actually leads to a gain in performance. So, say for example, we had an agent that had beliefs, desires and memory. And then we tested it against an agent that didn't, and even though our beliefs, desires and memory agent performs, you know, maybe 10% better than the previous agent. But we found that l more complex agent required, I don't know, 10,000 times as many computations to do that as well. We haven't really shown the any benefit of these individual components, all we've shown as it makes an agent more complex has led to better performance, what we'd be looking for something as say, if we gave an agent memory, and suddenly, it required, you know, 10 times as many computations to use that memory component, but suddenly it was scored will survive 100 times better, you know, showing that this little bit of increasing complexity lead to a greater gain, that is what we're after.

Unknown 1:06:36
Okay, so where are we with this? Now, I know there have been several projects in the past that have aimed to do just this, and they tend to vanish after a presentation at a conference. We actively building on this at the moment, it's a little dangerous to have a deadline on here. But at the moment, we currently have three different teams working on three different tight implementations of their environment. Two of them are showing promise of showing a lot of promise. And what we are doing is expect those finished by June give us a bit of chance to work on what should be in our environment and what shouldn't, and then ideally made this available to anyone who is interested in at least trying out what we're doing. One thing I will point out is, when you build an environment for computer science, there is this danger that when you build it, you build it with problems that you can solve. So you're not really creating a general problem, you are just creating, you know, something that makes yourself look impressive. We're going to try and avoid this by using the latest research on leaf ecology to work out what should be in an environment and what shouldn't be. And otherwise we're gonna have other people solve that problem for us. We will also because this is virtual, we're able to run the simulations as many millions of times as we want. And we're going to put a bit of randomization in so even if we do include something that shouldn't be there sometimes will be present, sometimes it won't. And we will get a chance to actually see, you know, what does the effect of having this problem in your environment have on the evolution but you know, the complexity required to solve that problem. And that is effectively where we are and what we are going to do. So like I said, this is a work in progress. If you are interested in living in talking to us about it, please feel free to reach out to us. But other than that I am finished for this presentation.

Joscha Bach 1:08:50
Thank you very much for this presentation. I found it very interesting. Do we have questions from your audience? Yes, please come here.

Unknown 1:09:01
I just had a really quick question about the goal of the agents in the model. So like early humans seem to have complex goals around like survival and reproduction and food and political power. Like what is the what's the goal of the agent? What are they trying to optimize for?

Unknown 1:09:20
Okay, so start with it will be survival, but we do want to, again develop towards things Asya political power would be an interesting one. But that is that'd be future goes over to Richard we don't want just one simple problem. Again, in AI you have the tendency of creating things too simple. So the agent solve them straight away. We want to make it complex. It's basically show that you can solve this we'll see what's required to solve these multiple problems simultaneously.

Joscha Bach 1:09:55
By the way, I've been working in the past in a similar paradigm we build a virtual AI islands that was explored by virtual robots, and they had to look for food and resources interact with each other. The reason why we didn't want to build actual robots was the difficulty to get sensors and actuators to work in the real world to give Rich affordances. And it's much easier to build affordances in the virtual world back then. But we also had this difficulty which you anticipate that the agents are only discovering what we put into the world in the first place. Don't think that randomization is the right strategy, because it's going to make the world less deterministic, less structured, it's going to obliterate some of the logic of that world. And so, at the moment, we have so many solutions for sensors that are already made, because we basically have systems that are trained on ImageNet, and so on, isn't it time to build systems that can work both in the virtual world and in the real world? I mean, it depends

Unknown 1:10:54
on what problem you're trying to solve. If you are trying to generate an artificial consciousness or a general agent, then yes, working in the real world and the virtual world is ideal for us, because we're more exploring the why do these components exists, then work in the virtual? Well, it's not going to be the exact example of the problem, it should help us provide evidence for there being a reason for this problem being solved or that this does exist in the first

Joscha Bach 1:11:22
place. Thank you very much.

Joscha Bach 1:11:34
Our next speaker is going to be Julia Ruffini. He is a physicist, who is also the CEO of star lab and Barcelona, an institution that is working in multiple areas, including neuroscience and AI, and is he's working on a theory of consciousness that is based on Kolmogorov complexity. And I'm very much looking forward to this presentation.

Unknown 1:12:03
So thanks for the kind introduction. So I'm going to be talking about the framework for the study of consciousness called Katie. And I have two collaborators that have been helping out in the last year or two. So let me go on with the presentation. So here's a bit the structure of the of the talk in seven parts, I will just start now with the motivation. And the motivation for a framework can be provided from two different routes one is the first person first person or the experience route or the other one is the third person or one we call life as you will see in a second or two lead to the same place. So the subjective route starts from the fact of experience first person subjective standpoint. And we know from meditation psychedelics reduce experiences, that experience can be pure primordial, free of mental constructs, such as the ego and from this and the self evidence of our own experience, the what it's like to be we we start from the assumption that there is primordial experience that does not allow or require prior causes, okay. So, in other words, the theory does not deal with the hard problem of consciousness, it assumes that there exist experience and instead, we focus on the problem structure experience. So, we aim to build a theory around the notion of structure experience, where mathematics and experience meet and by mathematics, I refer here to the definition that I found perchance in the Encyclopedia Britannica that says, The Science of structure order and relation which is quite feeling, we do observe the experiences structure, at least during wakefulness, there is a spatial temporal and conceptual organization of our first person experience the world and that includes yourself. So with this, we defined structure experience as the phenomenal structure of consciousness that encompasses both sensory qualia and the spatial temporal and conceptual organization of our experience and this is what we will aim to explain as the scientific strategy for this study of this structure experience. We observe that it can be explored in reporting humans and we would like to characterize it with methods that then can be applicable to a range of systems including artificial systems. This strategy will be to quantify the structure of experience from first person reports and then attempt to associate this with third person data sets eg fMRI or behavior or whatnot, whatever you can measure, and this decision is to be carried out using mechanistic insights derived from neuroscience and mathematics. If we are able to do this, then we should be able to study third, from the third person perspective other systems such as nongroup Already in humans, other living species are artificial agents and with this background provide an educated guess. Because we can approve it, of course, about the agents structure experience. The objective route starts from that attempting to define what life is. And finally observe ation that what remains after the passage of eons must rightfully be called a persistent pattern. In the world, there may be different types of these patterns, so maybe rather impervious to the world and study in a sense, such as problems. And there are other patterns which we call life in the in the framework that readily interact with the world but persist by partly capturing structure in the world. In the world they inhabit, either to stay for homeostasis, or replicate, which we call meta homeostasis, the replication of pattern, the connection with the first viewpoint, the one or the first person viewpoint is that key in this in this framework, this generalized version of life definition of life is what is capable of structure experience. So as part of the problem, we should also study the algorithmics of emergence of life. So a few words about algorithmic information theory and Kolmogorov complexity because they provide a mathematical background to the theory. So we can think of agents as physical systems, and eternal physical systems as dynamical systems calculating effectively computable functions, right. And this allows us to analyze agents from the standpoint of computation theory. So a warning here, because I've gotten this remark, sometimes computation is a mathematical concept, we're talking about Turing machines essentially are equivalent. And we should not be this should not be construed to mean that we are actually stating that the computer that they bring this out is a computer like a laptop.

Unknown 1:17:05
This computational perspective leads us directly to the algorithmic information theory central concept, which is called more complexity of a data set. And this, this concept is basically the length of the shortest program capable of generating a data set. So the shortest program that is capable to generate or which can be used to compress the data set equivalent. Here's like an example of this. For the typical example, on the left, you have a bunch of digits of pi, you can have as many as you want. They look random. For most purposes, you cannot distinguish this from a random sequence, when in fact, there's a little program that actually generates the sequence. And it's, it's very short, I mean, this list could be as long as you want, but the problem itself is very, very short. And I provided below as supporting potential link was it actually the program causal graph, so this is the causal relationship between the variables in the program, which presumably has something to do with the causal structure of this program. Okay. Another concept, that means from 80s, mutual algorithmic information, which is basically the amount of algorithmic information in one string, that can be used to compress or to generate another. So it's a generalization of the mutual information in general terms, but in algorithmic terms. So if you have high mutual algorithmic information between two systems, it means that you can use one to compress or generate the other efficiently. I know two programming languages, this probably will not be very relevant for what comes next. But there's a hierarchy of different computational systems. And for the ones that we're mostly interested here, which are three machines, you do need recordings as part of the programming language. So you can have simpler programming frameworks, but they are limited in what they can do. Turing machines are the ones that can compute effectively computable functions, which we believe is what the physical world is actually computing. Okay, so, concept of model, which is associated to this algorithmic information theory. In KT in 18, we call a model the model of a dataset, any program that generates a data set. And there are there may be different models that generate the same data set and they represent perhaps different functions because the data set is finite. So we can have different functions that approximate the same data set, but they're actually different, or they may actually be the same function implemented in different ways and both aspects will probably matter. We will focus on the ones as I mentioned before, that implement the right function succinctly. And this leads us to the optimal model of our data set, which is the shortcut programs that generates are commonly compresses the data set. Okay, so this is the definition of model that we use in the independent work. Now, for our model to be optimal, on average observation is that it needs to capture and exploit all the structure in the data set and nothing else. So, in addition, the structure of the model can be described by the group of symmetries of the data set. Okay, so, this is this can be formalized, and I want to not go too much into this here. But as an example, consider the example of being given a stack of images of a hand that are actually generated by a computer, for example, from one sophisticated program, like blender or something like this. And you have actually generated using a simple function of some parameters. Here's a view of this, of this potential images generated by the simple program. So, the structure of the data set, which is actually maybe a huge data set, maybe a very long movie gigabytes of moving in hand, it's actually a very short program that encodes the function. And this is actually been very an object in the theory. It's the model of behind in algorithmic terms. Okay. Now, why are we talking so much about models and why are good models? Good.

Unknown 1:21:32
So, the rationale for the importance of compressive or succeed models is already a replace of as, as you know, Occam's razor, that one should not increase beyond what is necessary the number of entities required to explain anything. Okay, but why Why should that be the case? Well, there are several potential answers to this, here have a few, the universe appears to be simple. So simple rules, we know that they can create as much complexity as one once and it appears that the universe is simple. So looking for simple programs, it's not an HR strategy for model builders. If the universe is generated by random programs, then then simple programs will are more likely to be the case because they're shorter basically, simple models are unbiased and generalize better. This is part of what comes principle, but they're really also addressed by Laplace and James more recently, and of course, they're also easier to construct. So, from a constructive point of view, starting simple seems like a natural thing to do, then it will be easier to store and reuse for model building as well. Okay, central hypothesis of kt, which is using this background is that an agent or whatever, that agent may be formed by has stronger and more structure experiences to the extending has access to encompassing and compressive models to interact with the world what we call good models here. Now, more specifically, that the event of the structure experience arises from the act of successfully comparing these models these good models with data and then the structure of experience is actually determined by the by the program structure by the structure in the program as I described before. Now, well compressive so good mothers are compressive and uncompetitive. Compressive I already described what they mean, encompasses encompassing means to the amount refers to the amount of data from the world that is successfully captured by the model. And by therefore its, its explanatory potential. And in terms of algorithmic information theory, here we have to refer to mutual algorithmic information. So models that have high mutual algorithmic information with the world. So just a remark that models are constructed by agents from information generated through the sensory motor system, sensors and effectors that the agent may have. And it should account for data generated by the world and also by the agent itself. And therefore self model is a natural part of the model of the world that agent needs to build. The algorithmic agent, it's an abstraction that tries to capture the minimal set of elements needed for homeostasis of an algorithmic system. And this goes back to AI in many disguises. What is special about what's proposed here? Well, first you have this closed loop of interaction with aging with the world. The world it's a simple generator of data it runs simple models, provides input streams to the to the agent DSDM has a modeling engine, which is in charge of creating, running and predicting the future. And the Act. As I mentioned before, the act of extraction experience occurs when these are successful match between the data that's coming in from world and from self. And it's been matched by the model predictions. The agent also needs other bits to function and for homeostasis. One is clearly an objective function, which defines valence, and it's fundamental for homeostasis, and a planning engine that requires the modeling engine to create plans for the future and output actions into the world which are being streamed as well. Then the algorithmic event of structure experience is simulated, I think, this nand comparator they have here between comparing input streams and self generated predictions. It's also a part of a common theme now in in neuroscience, you can you probably know about predictive coding theory which you have bottom up, which is sensory information basically being compared to

Unknown 1:26:16
top down predictions from models at different hierarchical levels. And so the comparators in neurobiology seems to be implemented at different hierarchies. And, in particular, it's also now it appears, this is the dendritic integration theory that suggests that this is happening actually in layer five pyramidal cells. And this this nice theory is now relating this to things like anesthesia and consciousness, psychedelic experience, Alzheimer's, Alzheimers disease, etc. So, the next question that the model tries that we try to address is how do agents build models and we find that because of these different routes to K T that we are led to connect in the concepts of life intelligence and structure experience in doing this. So both life and intelligence represent processes to construct simple models from the persistence for the persistence of algorithmic information preserving systems across time. So starting from resilient building blocks, what we call the first static persistence as an example products for example, but this could also be some amino acids. For example, from our competition my perspective, life is an algorithmic process, program building carried not solely by the individual agent, but by translate that by the transgenerational agent, through evolution for meta homeostasis for preservation of the kind. So this kind of an evolutionary learning paradigm model building paradigm and intelligence, then is the consequence is the is the next abortive jump pressure gives rise to agents that starting from their static models, that they have built higher level compressive models of the world within their lifetimes, using what we call brains. From the point of view of the theory, both static model building static modelers and active modeling agents transgenerational, are not transgenerational enjoy structure experience. But of course, there's the level and the structure of their experiences may be different. And I raise a question here for thought what comes after life and intelligence in this model building hierarchy. Few words about dynamics, there is connection of the theory belief with with with Anamika system theory and more particular with critical with critical phenomena. Here's a little chart that I that they have that tries to relate KT the theory of Kolmogorov complexity described with it and I will not go into this beyond the reference to the program because of structure before and Criticality theory. So here we will focus on this link. There was a revision is that competition in nature is carried out by dynamical systems with very large degrees of freedom. We do seem to observe the brains operate close to the critical boundaries, consistent with notions of self criticality. In fact, altered states of consciousness such an STC our secret derricks, for examples appear to move the brain away or towards critical state. So, this raises a challenge for Katie. So algorithmic agents, dynamical systems, instantiating compressive models of data, which itself has regularities and symmetries this these agents must have special properties, what could they be? So here bring you back. For an apologize This is perhaps the hardest part of the talk So, bring you back to the theory of the moving hands, where you have images generated by a function of some parameters. This could be handy matches for example, and we observe that although the images themselves may be embedded in a very high dimensional space is the space of pixels 1 million dimensional for example, the dimension it's actually very small if the the parameters controlling the hand function is small. So, the state of a dynamical system generating frames of the moving hand your brain for example, regardless of how its natural space state space is, for example, with a large number of neurons, this dynamical system must be lying in a low dimensional space or reduced manifold. So, this low dimensional manifold embedded in a high dimensional system, how could it be erased? So, criticality is one way criticality when the real part of Eigen values of the dynamics of the system are zero or close to zero the dynamics collapses to a low dimensional manifolds, we can think of this as constraint dynamics. So, this can give an answer for this reduced manifolds. Symmetry if if you have a Hamiltonian a dynamical system with constraints,

Unknown 1:31:19
another stream states that the Hamiltonian dynamics is invariant the group of symmetries. So you have here connection between symmetries, constraints and low dimensional manifolds. So, for example, here we have a very simple representation of what this very high dimensional space would be. And the trajectory of the representation of a hands in the reduced manifold would just be around this little circle here. As you move the parameter theta, we're rotating the hand for example. So you're embedded in our space, but you're actually Lanius more manifold. So, here we see that structure symmetry in the data, the collapse of the dynamics to low dimensional low dimensional spaces, criticality, with its associated important features like maximum information flow, power loss, long timescales and enhanced susceptibility to perturbations Kolmogorov complexity and structure experience are deeply connected. And if they are the emerging view will be that the manifold structure of the reduced dynamics together with the mutual information provide respectively the metrics on the simplicity of the models and the amount of algorithmic information capture the agent world of loop this is an interesting observation which tracks world data this this comparator system trying to predict the future keeps dynamics from the reduced manifold on on a structure experience. So, one can imagine that psychedelics meditation, sensory deprivation, or neuro psychiatric disorders may lift up the system from these enslaved dynamics into the higher dimensional manifold and this is the loss of structure experience. Nurofen phenomenology so, you know, phenomenology defines a methodological strategy for integrating the phenomenological neurobiological accounts first person or three person data, we can rely on altered states of course, in that consciousness at secondary supplementation to a context to study the effects of perturbing the mechanisms behind structure experience with meditation is also associated with a global dissolution of the body itself. And for this reason, it can also be associated with this loss of structure experience. And we can we need to find objective measures of structure experience from first person data and one proposal that we have made is to Analyze Descriptive narratives in speech through state of the art, natural language processing, to establish metrics on the structure and semantic coherence of speech, for example, under psychedelics, this could provide some principal way to study the structure behind behind this and in terms of the connection with the theory, then we have this dimensional structure experience the first person parties here, so, this will differ sorry, to this axis here structure of experience, the realism and the breadth of experience are the three axes of the first person experience and in this direction, here you have maximum structure experience and this map to respectively in the in the theory to model breath, how encompassing the model is the model accuracy, and the reduced money for simplicity or model simplicity. So, this is the relation between the first person and third person mathematical version of things so, enclosing the philosophy of KT is known as Moore's Law As revealed in the context of Pan psychism or idealism, which is perhaps a more rigorous philosophical background, where consciousness is a fundamental entity and mind is everything. This is not necessary to study the scientific issue implications of the theory, but it's itself adopted the kept me motivated by simplicity and consistency criteria itself. So it's kind of placed in prison in that way. From the ethics point of view, just some reflections that the theory does not grant any special status of to humans of course, all systems of capture structure experience from the war have structure experience, pleasure pain is associated with the objective function of the agent other they described before. This can be for example, right to morality, we have natural notions of good or evil in computational terms. For example, we may say that an agent A is evil or hate HMB if the objective function of this agent a increases when the objective function of b decreases, and vice versa, you could talk about an agent being good to another or loving or another when its objective function increases when the other one increases synergistic behavior with the marriage when agents adopt this goodness or love for each other, when mutually destructive behavior takes place in the complementary case.

Unknown 1:36:21
Now, I will close. So of course, I just introduced a framework here with some ideas, much work needs to be done. I believe that this framework can provide the unification framework from very rigorous mathematics to different approaches of consciousness. That's it in global workspace theory, the free energy principle dendritic integration theory, this will seem to fit the theme. We still need to work further on trying to map the neurobiology of this algorithmic agent. We pointed out an example with a comparator but there are many things to work out. A challenge that relates to last talk is if we can competition evolve agents this model building agents are this is a mathematical question are persistent patterns unavoidable once you have a computational system, if you wait long enough other types other than the static or the static problem, persistent pattern life or intelligence? How can we discover the structure of reduced dynamics from third person data? How can we how can we discover these manifolds which reduce manifolds? Can we design better neurophysiologic methods to study search experience? Can we design a based on all the above model building agents that mimic life or intelligence? Is AI the next evolutionary model building leap from life and intelligence brain to brain communication? And thank you for your attention and curiosity. And I think it's my collaborator aid and Rosae for the brainstorming this later available in this link and preparedness is is in on its way. Thank you very much.

Joscha Bach 1:38:18
Very briefly, one question. You suggest that we use short programs to model reality, the program being basically in computationally irreducible unit. Right? And why do you think that? Or do you think that reality is made exclusively of short programs? Or is there basically an abundance of short programs and long tail of very complicated ones that it just takes too long to discover?

Unknown 1:38:46
Yes, thank you. So two answers to this one of them is that well, my background is physics, theoretical physics, and perhaps by this the formation of my profession or my earliest color education. I do believe that the universe is intrinsically simple. And there are simple laws that can generate all the complexity that we see. So that's that's one answer. The other answer is that looking for so if you are given a random data string, and somebody tells you that this data string has been generated by a program, you're the best way to make a bet on what what program generated a string is to look for shorter perhaps, this is the so called solomonoff prior. So, these are the two answers I will give to your question.

Joscha Bach 1:39:37
Thank you very much. Today's last talk will be given by artium busine was an analytical philosopher working at Lomonosov State University where he holds assistant professorship. He is looking into how to understand mental states of others when humans understand each other's conscious states works best with a bidirectional coupling. And he proposes to do this with brain computer interfaces starting from the insight that mental processes are mostly about the allocation of attention.

Unknown 1:40:21
Thank you, can you hear me? Yes. Yeah, okay. Okay. Yes, thank you for your presentation. My talk is what brain computer interfaces can tell us about the actions of artificial agents. My primary area of research is moral responsibility and artificial intelligence is not my main sphere of interest. So, I try to apply something from the real for moral responsibility to artificial intelligence. And I'm sorry, sorry, if my understanding of a AI is not very deep. So, I will start with a characterization of an artificial agent, artificial agent is an artificial intelligence that can directly interfere with the environment. I roughly mean this indirect interference involves other agents. For example, an artificial intelligence that only gives commands to human beings is not an artificial intelligence is not an artificial Lincoln, excuse me. And roughly an artificial agent is an artificial individual that can perform overt actions that can directly manipulate objects in the environment and so on. And I have a specific understand understanding of action specifically for the purpose of this presentation and for this research, action is an element of agent's behavior for which she or he can be morally responsible. To my mind, this understanding of action is most clear for us for us human beings, because we are agents, and we know that we are agents we know when we act, we know what we do, and we know that we are responsible for what we are doing. And this is a very precise and clear way of indicating actions, the elements of actions, our behavior, and as far as I understand today, the question that is on the table in this area of research is the question of moral a AI or moral artificial intelligence. And there are many papers concerning the algorithms, the principles of choice and so on many, many other topics. Generally, these questions concern artificial intelligence as a moral subject in the sense of our interaction with the subject and as far as I know, the RAND not not many papers that consider moral consider artificial intelligence as a moral agent itself. I am persuaded that in maybe in the nearest future artificial agents will appear in our world, we will create new persons new moral responsible agents and my question concerns the possibility of such agents the possibility of more responsible particularly show agents and I need to clarify what I mean by more responsible agent to be more responsible agent is to be an appropriate target for reactive attitudes such as praise, blame these two reactive attitudes or dimeric examples, these, this type of attitudes this broad are sown in understanding of moral responsibility and I ask

Unknown 1:45:20
question about action, how can an action of an artificial agent meet the conditions that are usually met by human actions, the conditions for morally responsible, morally responsible actions and I want to stress that the topic of my discussion of my presentation is action itself. So, this is a question about the structure of the actual structure of the action itself. So, I believe that genuinely causal theory of action is correct. Some version of this type of theories is correct. The cause of theories of action generally say that an element of behavior is an action if it has cause of right sort. Usually, it is a psychological cause some mental event and for my personal purpose, I claim that because of theory of action states only a sufficient condition conditions for an action. So, maybe there are some other possible explanations of our actions, maybe there are some actions that cannot be explained by any causal theory of action there are various examples, but, I insist that at least some actions are called actions because they have the right call the right sort of psychological cause, and I take into account only overt actions. And then we have this question about the causal theory of action, what sort of course do we mean do we need there are various options on the table. Some philosophers say that they are intentions or desires or want and beliefs or some something else, but if we consider these these concepts, it becomes clear that we face more questions than explanations. So, if we will try to implement intention or desire on artificial agents, we may simply not know what to do, there are there is something very specific about human intentions, human desires, and the big problem with interpreting intentionality in philosophy of consciousness consult and my idea is to find is to find a theory to find some some way to overcome this overcome this and I think that we can find help in the case of brain computer interfaces. So, I found some definition of brain computer interface the brain computer interface BCI is a computer based system that acquires brain signals analyzes them and translates them into commands that are related to an output device to carry out desired action. I think that this is an audience that do not need the need presentation of this technology, I believe that everybody has some. So, understanding of what brain interfaces are, and here is an example again this is a picture simply taken from On the web, so, here is a support article of excuse me, article, transcranial brain computer interface. So, here a person imagines writing the word the word hello and this word appears on the screen, let us take this action for personal letters agree that it is an overt action and let us take just take it as an example, okay.

Joscha Bach 1:50:45
If we

Unknown 1:50:47
if we consider the cases of actions that are performed with the brain computer interfaces, we can select we can find out that they are specific in many respects and the the specific characteristics of these actions give us give us some advantages in studying action with bring computer interfaces action. So, first of all subjects learn how to use BCI and I will say that they learn how to act they learn in you you if you use the BCI you never acted in such a way before and this process of learning and action in the process of performing an action can be introspectively accessed by the subject and there is a possibility of objective objective control objective access to this process. So, we have some we have a chance to observe the action from beginning to land with reports of the subject and this is a unique chance usually, we learn to act in infants in infancy and infants are not as good as good subjects for search as adults and actions with BCI our actions because they are produced by a cause override source. So, I will return to this claim later, but I say that the structure of action with BCI is is this itself is a support for causal theory, because theory of action and obviously, eight agents can be responsible for their actions before with BCI we can do roughly all the same things that we can do with our let's say bare hands with BCI all these features are important for for the artificial agency, as I will try to show I have tried to for some time to find theory a causal theory of action that will describe BCI action and action will be safe and will not and will not introduce some obscure concepts such as intentions desires, so on and for some time I thought that idea mortar theory and not all theory created by suggested by William James is a good candidate by but then I found Kent box representational theory of action. It is an old theory, it was presented in 1977 paper. And its its main claim is this that action, unlike other behavior said essentially involves experience of what is being done and what is to be done next, and that it counts as action only if it's if this experience called So, we interact with it. And that box introduces two types of representations. Receptive representations that are a burn that are awareness of what is being done in the effective representation to warn us wordless of what is to be done and these types of representations are called executive, executive executive representations. There are some features the executive representations have some features, first, they are not intentions.

Unknown 1:55:45
One difference is that they are more fine grained than right grained than intentions. I will illustrate this later. Executive representations are not propositional attitudes and intentions are often understood as propositional attitudes. Executive representations do not constitute her su generis glass of mental states and all these claims must stress the specific status status of this executive representations According to Box theory, but it will be more simple to use an example let's take the picture that I already used in this case the imagining of writing a word will be an effective representation that will be a cause of some change in the environment and the letters that appear on the screen and the letters of which the agent is aware, they are the receptive they are the receptive representation and the process of writing this word hello. The process of performing this action will be very fine grained According to Box theory look, if we take a different theory, we can say that we have only one action here only one action that is corresponding to the intention to write the word hello, but that can be true this can be true for bar in the sense that this is something that we will prefer to call one action that's what we usually take four actions but according to his theory, this big action is constituted by many many groups many rehearsals of the same movements between the between the effective presentation and the represent the receptive presentation okay. So, I try I try to illustrate this with this picture. So, roughly we can say that the agent first imagines the letter H then it appears on the screen then he represents imagined letter E and so on and at every stage there is a feedback loop the agent controls the action I insist that this is the idea of control that this is the type of control that we need for action. And if this there is some error in this process, so, the then the agent can correct correct the performance or the aid of the action in this way, as I presented this slide is an example what all this can tell us about the effective or executive presentations consider this event this imagining the letters of the word, hello. This imagination this act of imagination is not itself an intention and it is not itself a desire. But I must add that only

Unknown 2:00:30
imagination is not an effective representation in the sense that there must be also some expectation of the effect, but I'm sure that the expectation of the effect can be easily implemented on artificial agents. And I need to add that this imaginations are not necessary for actions. So, a mental event can be different. And in the case of BCI, we can also try to construct a different interface interface that will

Unknown 2:01:20
that will

Unknown 2:01:23
monitor some other type of activity. Outbreak. So, I believe that this scheme can be implemented on roughly can be implemented on artificial agents. But there is a possible possible objection to everything that I have said so far. That's not the agent with the BCI intentionally produced an image is not it an internal action that is the cause of the overt action. There may be internal actions, I can agree with this, but they are not necessary for every overt action. That's what, that's what the case of brain computer interface can have, show us. That the cause, the cause of the action must not necessarily be an intentional internal action, simply because some actions are not intentional, but still our actions. As Bach writes, some actions are not mentioned before, to automatically routinely unthinking when I'm thinking Glee, they are minimal actions. And I believe that actions performed with brain computer interfaces can be routinized, they can be they can become automatic. And if it's true, then the causal explanation the cause of theory of action, that does not evolve original intentions also, is also becomes a true So, the structure of the action can be presented in the following way. If I'm right, so, we can read that there can be different reasons for actions, some reasons can be external, and some can be internal and I want to concentrate on the external reasons. So, an example of external reasons can be the phrase please write hello. So, we can address this to a person with a BCI this phrase can be a reason for action itself and it can cause the effective representation and start the process, this process of circulating through creation of effective and this process of action and another the results in other condition, that can be easily implemented an artificial agents that all of us meet is a Viet veto capacity, capacity to stop performing an action if there is a reason for it. So I don't want to say that if an action is started, then we cannot stop it. We cannot control it. This theory explains it How we can control it. So, control is present on every step of this process. So, what is my general conclusion for the artificial agents obviously, much more elements are required to construct more responsible artificial agents, agents a set of general rules of choice for various situations that are subject to correction through moral reactions, a pattern of distribution of attention that gives preference tomorrow. So, subject of my talk was introduced as a talk on the distribution of protection, but, when I, when I prepared my slides, I understood that I will not have time to discuss this topic, so, excuse me for this and

Unknown 2:05:55
an artificial intelligence artificial agent must have a cognitive system supporting consciousness, many, many other things, but my conclusion is that implementation of a representational system of action their system of action that is described by BA is a part of at least one sufficient condition for artificial agents being a moral responsible agent. So, of course, this is not sufficient condition itself and maybe it is not a necessary condition, but if we want and I believe that we want and we will do it at some instant, if we want to construct to design an artificial agent who will be responsible for or responsible for his or her or its actions, we can use this representational system simply because it is it describes the way we are self packed in the cases when we use brain computer interfaces, and I think that we do not have any doubts that we are more responsible when we use BCI. So, thank you for your attention.

Joscha Bach 2:07:26
Thank you very much for this presentation.
