Joscha Bach 0:00
I'm going to be five to go talk. So if you have questions, jot them down and try to fit some of them in the panel. I'm going to talk a little bit about why I think that ATI requires integrated models or condition. Why graphical architecture is a good idea. And about the macro side framework itself? I'm not going to talk about this a lot, because we're very short on time. And one of the reviewers asked me in his review, why I would think that the graphical architecture would be a good question in the first place. And I think this is one of the points that you should briefly maybe elucidate. But I believe that one thing that we should be thinking about briefly is what's actually consisting the biggest problem AGI research? Is it that we do not have a proper roadmap that we like, towards for innovation. Do we need to understand things like creativity better? Do we need to focus on empathy and communication? is the biggest question of AGI research, maybe the existential risk that comes with it? Or is it a question of getting sufficient hardware? Material armor faster, better CPUs, cheaper, harder? Or is it the big topic of funding? How can we get our research funded and so on? And I don't doubt that all of these questions are extremely important and interesting questions, except maybe for the funding question. But I believe that the really biggest question of all of them is that we don't know how to achieve AGI at all yet. And I don't mean in the sense that we don't know how to achieve, say, mortality, or that we don't know how to achieve turning into godlike beings or whatever, but in a very mundane, secular sense. More like that you didn't know how we can achieve sporting, if you want. But I do think that's entirely reasonable as a computer scientist, and I don't think that it can be difficult because nature gets it right so many times by just keeping you in the right place in our head, and making it or constraining it sufficiently to perform all these cognitive operations that we also cherish. But we just don't know how to do it. We know how to do sorting, we can program this, but you don't know how to do AGI because as programmers if you know how to do something that needs to be expressed in this program. And so the big question is, how do we find out and we already have good ideas in which corners to walk. But still, we are far from knowing how to do it. And I don't think that the promise of AGI is to get a single paradigm perfect, like deep learning, deep learning better for optimality theory, or better logic based systems, or assault to remote language or whatever. But instead, I think that we need to make progress in many areas. And this lead needs to lead to good enough solutions. And these webinars collusions need to cover things like autonomous decision making, and reflection and adaptation, and the acquisition and the control of internal complexity. So how can you build a system that makes itself more complicated, but make sure that this complexity is also regulated? How can we get perceptual processing in such a system? How can we achieve the basic cognitive operations and natural language logic and sort of the them? How can we get such a system to interact successfully with an environment with other agents and so on? Then, questions like Resource Management attentional processing anytime characteristics, so how can we work with limited resources? And I don't think that there is a single paradigm that can address these things like building robots or focusing or the body man, that would probably not be the right level to solve this host of problems, or looking at neurons and brain architecture might not be the right answer either.

Joscha Bach 4:28
But I think that the biggest problems then work into uncharted territories are things that we do not understand very well. Personally, I totally agree with many stock that motivation is a very important topic that is attention relevance content. But I don't think that's a very difficult one. But this might be self deception, because I've been working on motivation for the past 10 years or so. And now I think that if you can set up a system with a finite set of drives, which also Make the system autonomously explore social end domains and cognitive domains. But it is an important question, but it's one I think that can be solved an indicator of research and so on. But the really tricky things are maybe in the perceptual domain, how can we encode percepts properly and do perceptual learning bottom up? And on the other hand, you concept driven hypothesis formation top down. And you do this both with modal a modal representations? And how can we reconcile this this petal and fuzzy processing domains like the associative operations, or memory and analogy finding and pattern recognition and structured Association building and so on? How can we reconcile local representations with these parallel classic processing domains. And we also need these local this combinatorial processing operations to get to planning and language and constructing speaking and so on. And the biggest deficits are not just in these different topics when we do lots of stuff, but in how to understand how these things are intertwined, how they can get them to play together, and how can they be combined to form all those abilities that we have. So as an attempt of getting there 2002 We started work on the computer architecture microsite, which is based on work by psychologist Dietrich Turner, unfairly remarked that most AGI research doesn't use a decent theory on psychology, and might have to do with the sparseness of decent theories and psychology because psychology unfortunately, is not a theory driven discipline, and you don't get any points. But for training, publish a theory in psychology, this is not how psychology works, which is unfortunate, because it means that we have to do it ourselves, we have to do this is not an artificial intelligence, but it's something broader and more cognitive science domains. Unfortunately, cognitive science over the past decade or so has turned into neuroscience. Which brings us to AGI. So, I guess our job as AGI scientists, in some sense, is to come up with a decent theory of what a mind is, that is a decent theory, psychology. And this is baby steps, such as theory. So it's an integrated architecture with round representations. And it has complicated modulation, this modulation can configure the system in such a way that emotional states emerge. And it has a motivational system, which gets the system to set its own goals autonomously, including social economic controls. And the whole thing is graph based architecture

Unknown 7:51
we're currently doing since for about a year and a half

Joscha Bach 7:56
a reimplementation of this system. When we started 10 years ago, software development was very different from what it is today, back then it was all Java and XML today is much more agile, you have the same functionality in a fraction of the number of lines of code, it's much more easy to work with this. But our focus was on making the same readable, understandable and putting together an open source version, which can people can access on GitHub, which they can use in the browser without installing anything. So it's fast. One of the main goals is how can we explore IDs. And the representations in there are built on graphs, strictly speaking on hypergraphs. And they use spreading activation networks as the dominant principle for their computation. That's a paradigmatic difference to other systems, which use logic or rule based systems or which try to mirror biological neurons as closely as possible despite trends and so on, or a combination thereof. But it's not a computationally differently, it's a different way of obtaining perspective on what this thing does. And since our goal is to get models, as researchers of how we can look at the mind, we find this a very fruitful perspective. So, our agents are built on universal mental representations, which are compositional and distributed with the same formalism simple expressions element at the root of this which are combined, it was very activation networks that we can use them to press points and scripts and associative relationships and so on. The basic building block of this is what we call the concept node, the concept node has a number, single slot, and then a number of gates and these gates are tend to mount to the leaf types. So we have a very small finite set of link types. And most of these things are causal, in some sense, and taxonomic there is SL to It's an inverse relationship. And then we have a partner relationship in this part of relationship here, which is in some more general sense attribute relationship. And we have type inclination of different kinds, which is spread throughout the network. And together with sensors and actuators that do relate the system to operations on the environment and within itself, we can build more complex things and complex relationships. So the basic relationships are part anomic and categorial. We also have a simple relationship which is useful for connecting these things to databases, and so on and programming purposes. And then we have a horizontal alternate advising relationship, which is a predecessor relationship and view expressed as possible linkages. These concept notes are a special case of case of notes, you have 10 slots and gates, the gates aggregate function and several parameters. In here, there is no function. And the simplest case, the node function does nothing but transmit activation from the slots, which are the activation things to the gates, but they can do more complicated stuff. So we can hide arbitrary functionality there. And then arbitrarily complex agents with these note nets. And all of our agents are built using these credit activation networks with notes. Activation spreading can be seen as a special case of message passing by a matrix multiplication, and the constraint the activation spreads according to the links. So we don't have runaway activation. Usually, these forms of early spreading activation networks don't occur. We implement structural inheritance as CO activation of concepts, we have different speeds for activation propagation. So this enables us to build simple hypergraph capabilities in the systems because sometimes we need conditional linkage in the graph, we will typically only have connections between nodes, which are independent of the connections that these nodes have to other nodes. But if you think about conceptual domains, very often, you need conditional connections. That is, in the context of one concept, that concept has different meanings than the context of another one. So we need hypergraph properties. Okay, and we allow this allows us to have combinations, neural

Joscha Bach 12:25
learning a social, different tree. I'm not going to go into details just because we out of time basically already, the macro side friend builds of these agents out of these notes, and can connect with one information to multiple environments usually use virtual environments. Because robots at this point, provide very cool affordances it's very difficult to build a robot that can climb a tree down with a bit of wood, construct an x from it, or whatever. And it's not that difficult to build equivalent operations. It comes on discoverability and combinatorial. richness in the virtual environment. The framework is built on a server application with a runtime and the server allows us to distribute this over networks. And people can use it over the internet without having their own installation speed up apply this to research with psychologists which use the model for human problem solving and expressing personality properties as variations of the motivational system. And also commercially, where we have microsite as the core of a platform for acquiring social knowledge by monitoring how people plan in this social domain that's drafting over these plans. This is only possible because of my co workers, especially Dominic Darren, and it's supported by the pot having ag and the Berlin School of my brain. You find some more information on Rakoff side of Comdex mostly coming soon when we put the open source version of the current edition online. And I'll be looking forward to questions during the panel and later on. Thanks

This transcript was generated by https://otter.ai