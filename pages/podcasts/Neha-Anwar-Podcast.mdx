 # Intro 
 **Neha Anwar:** This is my conversation with Joscha Bach, a German cognitive science and intelligence researcher.
 
 Joscha has an MA in Computer Science from Humboldt University Berlin and a PhD in Cognitive Science from Ostenburg University. Bach has taught Computer Science, AI and Cognitive Science at the Humboldt University of Berlin and the Institute for Cognitive Science at Ostenburg. He worked as a visiting researcher at the MIT Lab and Harvard Program for Evolutionary dynamics. Yosha then joined AI Foundation, working as VP of Research. He is currently a Principal Engineer at Intel Labs Cognitive Computing Group. Bach built MicroPSI, a cognitive architecture extending representation of the PSI theory with taxonomies, interheritance, linguistic labeling. MicroPSI's spreading activation network allows for neural learning, planning, and associative retrieval. He has also worked extensively on novel data compression algorithms using concurrent entropy models.
 
 I just want to start off like right from the beginning. You grew up in Germany and you have a very different upbringing from like general public. Can you just give me a little bit background on how this mind was created?
    
 **Joscha Bach:** Well, I grew up in communist Eastern Germany. I was born in 1973. And my father was an artist and sort of a hippie. He did not feel that he got along with society at large. The things that he was interested in were not completely aligned with the way in which society was was run, so he removed himself from it and got a watermill in a very beautiful remote valley and then did his art for all of his life and still does to this day. And I grew up in this very beautiful and somewhat remote place, and I spent a lot of time in nature and enjoyed this very much.
 
 And on the other hand, I did not have many people to talk to. So I got bored and I started to read. And by the time I got into public school, I read many of the books until fourth grade and science fiction and other stuff. The house was full of books. And in some sense, I was lost for the education that my teachers tried to give me. And my intuition was usually when teachers say something that they are either confused or they're lying. And that was because I was quite immature. Of course, I was a young child that did not know how other people operated. And so I was unable to seek my, this demands around me. And until I got into a school for mathematics and sciences in ninth grade, I basically met nobody that I had the impression that I could interact with on any meaningful level. And so I basically was an autonomous mind. I was looking at the world from the perspective of an outside observer, of an alien and that was exacerbated by living in a communist country where the epistemology was weird because it was a highly ideological society in which philosophy was always seen through the lens of a vulgar interpretation of Marxist-Leninism. And so everything was seen from whether it was reflecting the role of the working class in the proper way and having philosophical conversations and so on. with an academic philosophy and so on was difficult.
 
 I was strongly influenced by the ideas of cybernetics, for instance, authors like Stanislaw Lem, Polish science fiction author who basically formed a lot of the ideas of the 1960s in his own mind and then decided that he didn't want to go into academic cybernetic research because the time was not right yet and instead he tried to influence the world by writing stories and philosophical parables. And I also read many of the classics in that time. It was like a sponge. And when I left school, I thought about what to do next. 
 
 I was most interested by the idea of understanding how our own minds work. And that was something that fascinated me from a very young age. I was sitting in the early '80s, in 1983, roughly, in front of a Commodore 64, I taught myself how to code. And I had realized that everything that I could fully understand, I was able to put into this computer, within the limits of the hardware, of course, but I realized that there would be faster computers with more memory available at some point in the near future. And I was wondering what if I could, what is that I would want to put inside of the computer? And the answer was a whole universe. A world that is interesting and fascinating and the universe is populated by interesting minds that I could talk to. And that fascinated me. Also this perspective of understanding that if I can put a mind on the other side of a screen, it also means that I am in a category of things like this, right? Everybody who looks at the world is looking at the world through some kind of screen, our retinas that project patterns from the universe into our minds and our minds learn how to interpret these patterns and make sense of them. And behind the screen, there are other people obviously, and I'm one of these. And so understanding who we are is also probably best done from this perspective, from this constructive perspective, from the causal structure that is embodied in this idea of software. And I thought, okay, I want to figure out how this works. The best way to do this is probably not art because it does not give me the tight interaction with a substrate and the necessary interaction with other people that I need. And so I thought I'd go into academia. And then I studied all sorts of things that were related to this, especially computer science and philosophy and a little bit of psychology and neuroscience and physics. And I found that the perspective of artificial intelligence was the only one that seemed to be productive because neuroscience mostly didn't seem to have causal models about how the mind works and was not so much thinking about minds, but about neurons. Psychology didn't have theories about how the mind works, surprisingly. It was not trying to develop any. And while artificial intelligence was mostly concerned with how to make data processing more efficient, there was this tradition, there's an artificial intelligence right from the start that saw it as a philosophical project, as a practical project where we could implement our theories. And it's also a very difficult project, one where you cannot make, solve domain questions within the three-year scope of a grant proposal, which disincentivized most of people in AI to work on this question and also got the discipline mostly not be concerned about it. But what I liked about computer science is the criteria for what's valid, because I discovered that you can publish and work in computer science regardless of your age or the school that you're in, because the criteria for what's valid are not social criteria. Your program either works or it doesn't. And that's quite beautiful because in philosophy, it's not the same thing. In philosophy, something is valid as long as people are willing to listen to you. So it's mostly social criteria at some level that determine whether theory is popular or not. And I found that philosophy has become almost sterile and unproductive in the last century. And most of the progress within philosophy has been made outside of the field. And computer science, for instance, and mathematics, and some areas of foundational physics. Yeah, so that's how I was formed, yes. - So my question is, you mentioned philosophy has almost become sterile, and then you felt like there was growth in artificial intelligence production. Now, how have you, especially 'cause with your work, you work with consciousness and then assimilation, and then somehow that is our understanding of the world and like how we are, who we are. Where is that overlap? So, it has the question, what are we? How is it possible that we can experience anything? And that was, in some sense, the first question that puzzled me. I thought it's possible to understand how reasoning works, how language works, how knowledge works, how the reaction to patterns works in such a way that we make data compression models over these patterns and connect this with a sympathetic motivational system and so on. But feeling, that seemed to be very mysterious to me. How is it possible that a machine feels anything? So the first question that I asked myself was, what is an emotion? And how are emotions represented? And the first answer that I found that I found convincing was by a German psychologist, Dietrich Dörner. Dietrich Dörner had the insight that emotion is a configuration of cognition. It was not the way in which he expressed it, but this is what it came down to. Emotion is the way in which cognition works. It's not a parameter inside of the cognitive system that says you are now 50 angry and now you're 100 angry, but rather it's a particular way in which you relate to the world. There are basically underlying modulators and these modulators optimize the cognition for the situation at hand. And these modulators direct in the way in which your attention works. So it might go narrower or wider. It might go on to certain things more than on other things. It shifts your behavioral tendencies one way or the other. And it shifts your experience of valence, of whether something's good or bad in certain dimensions. And these parameters together, they create a space. And being in a certain region of the space, that's what we call emotion. So it's basically our classification of a space of modulations of cognition. That was an important insight to me. It did not explain consciousness or anything, but it explains something about the nature of feelings as assessments of the world by a perceptual system that would be visible to a reflexive system. And at another level, the question is, how is it possible that something feels like something? A question that is stamping philosophers. How is it possible that physical things feel something? And the answer that I gradually stumbled upon, and I'm by no means the first person that did this, and once I had this answer, I noticed that much of human cultures and societies had already found that answer and was only obfuscated in my own culture, is that we live in a dream. Our own perception is a dream perception. we are entranced by a notion of being in a reality that can only play out in a dream. Physical systems cannot be conscious. Only simulations can be conscious. Consciousness is a simulated property. It's as if, right? So the neurons don't know what it is like to feel anything, but it would be very useful for your organism, for your brain to know what it would be like to be a person that is embedded in a real world that cares about things in that world and reacts to those things that are happening to it. And so in order to figure that out, it creates a simulation of such a thing. In part, it's only a simulacrum, but this thing is basically a simulated person in a simulated model of the world. And the mind is maintaining that. The mind is basically an AI that runs on your brain that creates a virtual world like a game engine that is developed that you perceive in interactors that integrates your sensory experiences. And inside of this world lives a simulated person that integrates your motivation, emotion, reflection, reasoning, attention, the model of those. And you identify with that person, with this virtual being in this virtual world. - So who is that person? So like our body, maybe I'm wrong in what I'm understanding, is a simulation by our brain to be able to perceive the systems around us. But to perceive the systems around you, you don't need to be a person, right? You just need to open your eyes and try to predict what's going to happen next. And our brain is trying to look for instance at the patterns on our retinas and then try to find relationships to the patterns that appear on the retina to other patterns on the retina at different times or at different positions in the retina. And the functions that your brain discovers are playing out in this three dimensional space because that's the most efficient encoding that you find over those patterns. And what you then find is that there seem to be solid and liquid objects that are moving around in this space. And these objects correspond to control systems. And some of these control systems are agents that have goals of their own. And they might be similar to you and so on. So you build this hierarchy of interpretation that allows you to interact productively with the world. At some point, your mind discovers that the purpose of that modeling is a regulation of an interaction with this world. and there is an agent that you are part of that is interacting with that world. And you start modeling that agent, you start to tell the story of the organism that you are. And yourself is the story about that organism that interacts with the world that you are part of, that your attentional model is generated by and for. - How is this connected to AI? So I know that understanding the human system helps you create artificial intelligence and smarter systems to build in that manner because you can replicate that, or is that not the case? - Artificial intelligence was not started because people wanted to have academic grants or implement machine learning more efficiently. But the reason why we did AI was because people wanted to understand how minds work. And they discovered the computer is a powerful new tool that can implement arbitrary causal structure, which means if you implement a theory as a set of causal interactions and you go all the way down to first principles, you can translate this and express this as a computer program and then simulate your theory. And this means if you understand all the causal structure that gives rise to what we call a mind, you should be able to express it as a computer program in this way, test your theory and make incremental progress until your theory matches the observables and you have an explanation for what minds are. And this attempt to understand minds has always been one of the driving impulses of artificial intelligence. But it's not been the most valuable part from an economic or academic perspective because it didn't work, right? It's something that takes longer than a single researcher generation to get to work. And when you are working on a practical problem, you want to have results much earlier. And so this philosophical project of AI was only ever going to be something that interested a very small fraction of people working in the field. I just have a question related to that. So we don't completely understand how the mind works even now, one. And then second of all, we are creating systems that are self learning. So is that replicating somehow the way the mind works? Or is it deviating from the path? It depends on how we define what a mind is. So what is the particular phenomenon that we want to explain? We want to explain how a physical system can have agency. That is something that is not that hard to understand, right? You build a system that is a controller, that I already understood in cybernetics. A controller has some kind of sensor that measures a deviation from the way in which things should be. And then it has an actuator that can direct the way in which things are towards or away from how things should be. And you measure that difference and you make sure that your actuator is moving at the right moment. So you can have a heating element, for instance, that heats up a room if the temperature drops too low and turns off the heating if the temperature goes too high. And in this way, you can have a single, simple controller. And you can introduce more dimensions of control. And you can also nest controllers. So you have something that controls the controller, for instance, the sensitivity to a sensor. And once you get the system to make a model of its environment, so you don't measure just the temperature right now, but you try to make a model of how the temperature is going to be in the future, if you perform the following actions, then your heating can become much more efficient. Because you can now anticipate how long it will take for the temperature in the room to change after you turn the heating and so on, and how early you can turn it off again, based on the way in which temperature moves. And maybe you can even figure out on by seeing how fast the temperature in the room raises, whether the window is open or not and thereby do a more fine grained control, right? You can get much more efficient. And so as soon as you connect your controller with a model of the future, you get an agent because you have now a system that can model different trajectories of how events play out in the future. And these trajectories are branching based on your own decisions. And that means that if you integrate the reward over those different trajectories, and you make a choice based on that, you now have desires, beliefs, that's your model and intentions, which are the paths that you have committed yourself to by making certain decisions at some point. And so it's a very natural progression from a simple control system into a system that is an agent. You need to have a motivational system, something that is cares about how things in the world should be. You need a general modeling system, which means a computer that can create its own causal model, irrespectively of what's happening around it. So it can model a counterfactual world, one future world, for instance, or can evoke its own memories and so on. And now the question is, how can we build a system that is capable of general learning? What is the right algorithm to build a learning system? And there are many ways in which you can build a learning system. The simplest one is that you can just try to evolve the right set of functions that you want to have to predict the future. But evolution is a very slow, undirected search in all directions. You just randomly change the elements of your function and hope that the resulting functions are more useful than the previous ones. And you find out by testing them all and then selecting those that are best. And then you change those a little bit until you get to functions that are where you want them to be. a very slow process that if the problem that you're learning is too complicated, often doesn't converge, which means you're never going to get to a useful result in a reasonable amount of time. And the most powerful algorithm that people have discovered to date in order to learn is called stochastic gradient descent. And stochastic gradient descent works by first of all representing the problem in a mathematical form that is quite general and the mathematical form that people are using is called a neural network. A neural network is often compared to biological neurons and said to be inspired by them but we don't actually seem to be understanding how biological neurons are working if you compare the two. Basically there is a superficial similarity to a toy model of biological neurons and that the reason that it works like a biological neuron might be almost accidental. Basically a neural network is a mathematical model that consists of weighted sums of real numbers that are arranged in chains. Every layer is made of lots of sums of the previous layer, where you sum up the different numbers that come from the previous layer and you give weights to them, which means you multiply each of these factors that go, each of these summands that go into the sum. And by changing these weights, by changing the factors by which you multiply the value from the previous layer, you can change the outcome of the function. We also introduce so-called nonlinearities, which are basically a fancy way of saying that sometimes you don't have continuous changes, but you have if-then changes in the network. So you, for instance, a simple way to introduce an if-then change is that you say, if the value is below zero, we just set it to zero. And these nonlinearities allow this model to also express if-then relationships. And it looks like a very clumsy way that we should represent everything in the world as weighted sums of real numbers that are chained behind each other. But it turns out that the benefit of doing so is that we can introduce gradual changes into the model. These gradual changes working by changing these weights until the result becomes more similar to what we want to express. And stochastic gradient descent is a way to figure out how to change these weights in very small incremental steps to get the results gradually closer to the function that we want to have. So, imagine you have a function that tries to express whether a photograph depicts a cat or a dog. It's obviously a very complicated function. And it first of all works by disassembling the input into a form that the model can understand, which means numbers. For instance, you can take all the pixels and translate each pixel into three color values. Each of them is a number. And then you arrange them based on their position in space. And then you feed this to the map, you feed this into the network. And then you take a few hundred of these layers. And then each layer, you take maybe millions of these units and you make a very dense connection across the layers. And then on the outcome, you have maybe two outputs. And one is it's a cat and it's a dog. The third one could be we don't know what it is or it's something third, right? And now we feed it in millions of pictures of cats and dogs and do statistics over them. And whenever the result is not correct, you have an error in the system. And for that, we basically look backwards through the network and see how much every of the connections contributed to the error. And then we shift every of these connections that contributed to the error a little bit in the right direction. This looks like it's a very expensive and slow procedure and indeed it is, but it can run in parallel. There'd be many, many pictures that we feed in. And then in 2017, a group of people had an idea. They said, instead of doing statistics over everything that happens in the previous layer of the network, Let's have a second neural network in every layer that learns what to pay attention to, which part of the statistics we actually need. And this model is called, this approach is called the transformer. It turns out that the transformer is a much more efficient way to train neural networks. And so we could train neural networks on things that we couldn't train them before. Also what happened in the meantime was the computers got larger and cheaper. So people could use very large data centers to go through very large amounts of data. And then I myself started to work on things like this in the 90s, was in a lab in New Zealand. And I tried to find grammar in an unknown language, in an unsupervised approach. And the unknown language that I was using was English, because it was unknown to the computer. And it was very easy to get a large set of English documents for me. And I got the largest computer that we had at the department which has two gigabytes of RAM. And I use it as efficiently as I could. So I did in-memory compression with programs that were written in pretty low level C and tried to do statistics of a language. And indeed, I discovered grammatical structure. And what I noticed is that the closer I was discovering the structure, just by looking at data compression principles, how well different words were able to predict other words across sentences. I was also able to get closer to the semantic structure of the text. And I thought if I could do much better statistics, I would find more and more of the semantic structure. And this would enable me to build models that would be able to construct sentences in very much the same way as it was constructed in the original text. But I only worked on this for about a year. And then I abandoned the area almost completely and only looked at it back when GPT-3 came out. And GPT-2 was the first time I really noticed these models in more detail. And I realized that they found a way to make this more efficient by finding efficient ways to do the statistics. And that was actually what this transformer was all about. And now we have a way that learns structure and language by predicting the next word. And then encoding the difference, training based on that difference, violation of the prediction of the next word. And this idea was formulated in neuroscience by Carl Friston that we learn by predictive coding, that we try to predict how things are going to play out and whenever something doesn't play out the way we predicted, we change our model. But I think we can also see that GPT-3 and similar transformer models don't learn in the same way as human beings do. They learn much slower, but they're also able to integrate over more data. They discover first, the stylistic structure and things like grammar and text. And then at the last thing, they discover semantics. And for us, it's the other way around. We first in some way learn semantics, what the thing means. And then we learn more niceties like style and syntax. And so we basically have a motivational system that determines relevance for what we interact with. And we have a cybernetic system that controls the behavior of a baby before it has a very sophisticated perceptual model of the world. Is that the limit of GPT-3? And it's that hurdle that once it crosses that, it will be different? The interesting thing is that we don't know what the limit of GPT-3 is. What we find is that GPT and GPT-2 had different amounts of data in the model and different size of the model. basically a couple orders of magnitude more data and model size and training. And as a result, it got better by a certain factor. And from GPT-2 to GPT-3, the same thing happened again. So again, more magnitudes of training data and model size and in linear increase in the performance of the model. There's not an obvious end to this. So basically, the more data we seem to be putting into these models, and the more we train, the better they get. And as a result, you get a model that is weird because it in many ways is superhuman because can use GPT-3 to have an interview between Hannah Arendt and Albert Einstein. And to somebody who is not an expert on Hannah Arendt and Albert Einstein, this thing is much better than what the person could have written themselves. And it's able to do this across all domains there was text in the data. So this is really, really amazing because it can now generate so many categories more than any human being can do. But it's worse than an expert in any of those areas would be because it does not actually have a grounding of what it's talking about. It's just confabulating in a way. And when it makes errors, it's not learning anything from these errors and it's not updating itself. So of course there are people which work on how to make versions of this that can update themselves and integrate back into this and they get better. - Isn't the issue with AI like this, which has like, you know, it's a, it has a memory thing where it's like within this space of time and it captures that information, but it's not able to go back and forth. Like, you know, when we can switch between tasks. - Yes, so we have an attention window that is our working memory in which we construct a model of the reality as it is right now. And we constantly edit this attention window that we have on the world, which means if I read a book and I read the last chapter, I might recall parts of the first chapter and instantiate them in my mind and revise what I learned from the first chapter based on that. Or I might even physically get up to my bookshelf and take out this book or another book that I read at a different time and compare what I read there and reintegrate this information. And in the way in which we currently train these models, they cannot really recall the original training data and go back to what they learned. But they mostly interact with the structures that they've built over this. But it's not clear that this is a fundamental limitation, or even if you don't revise the algorithm, if you cannot just brute force it. So at the moment, it's very difficult for GPT-3 to make a summary of the book in the same way as we do, because its attention window is just moving through the data and often not even in a linear fashion, because it might learn the book by splitting it up into different portions of text that are then fed multiple times into the algorithm and correlate statistically. I suspect that main reason why GPT-3 is able to do summarizations of books is because it also has read many, many summaries of the books that it looked at in the training data because it's trained in a substantial fraction of the internet. And so it's in many ways, brute forcing things that people do with more intelligence. - How is DALI? I'm not sure if you know DALI is like when it's interprets text into images. So how is this different from GPT-3 and how does this change the paradigm of AI? - What we find is that GPT-3 is only at the lowest level interacting with words. And these words are not sequences of characters, but they are positions in a vector space that they're already mapped into. And in this vector space, they're organized according to the similarity between the words and statistics that we found over the text. That's because people who built GPT-3 thought if we pre-process the text in this way, it's going to be more easy for the system to learn structure in it. And after the first few layers, it goes even beyond that and basically maps this all into a conceptual space. And the operations that GPT-3 does are playing out in this conceptual space and are then mapped back into tokens that can be expressed as words and letters and displayed on the screen in a way that we understand. And it turns out that the space of meanings that GPT-3 discovers has a structure that can be mapped to the space of meanings that a transformer that is trained on images discovers. And it's tempting to think that a similar thing happens in our own mind, that is a similar structure for the space of meanings that we discover behind language to the space of things that we can touch and the space of things that we can see. And it turns out that there are basically two spaces that we discover in this way. One is the world that we touch, which Descartes called res extensa, the extended world, the world of things, of stuff in space. Everything in this world can be mapped to surfaces and volumes in a three-dimensional space, in the same three-dimensional space that is connected through time. And this is the world that we interact with. And everything else is the world of ideas. And the world of things needs to be fully coherent in the sense that we expect that it is playing out definite way. Quantum mechanics complicates this a little bit, but by and large the world that we interact with consists out of stuff in space. The space is flat and the stuff is made of liquids, solids and gases and occupies that space in a coherent consistent manner. And everything else is ideas and the ideas don't need to be fully coherent. But the ideas explain what is happening in that physical space. So you mentioned in the past that the current methods of machine learning is flawed and they seem to choose to go against, you seem to choose to go against the brain. What exactly does that mean? What's so different about your approach? I don't think that there is my approach and the approach of machine learning. I think that machine learning, for the most part, tries to answer a different question than the questions that people like me are trying to solve. there is a substantial overlap. Machine learning is about making statistics more efficient. And traditionally, statistics come out of a subfield of applied mathematics. And the models that exist have properties that have been proven to be correct. But the models have been often very simple, which means there's only a certain degree of statistic analysis that was possible. And machine learning in many ways is experimental statistics. You just try to find out of an unprincipled base and how to make statistical predictions over how things are. And you evaluate these models at first empirically, which means you test whether one machine learning algorithm gives better results than another one without having a deep mathematical understanding of why that is the case. And often you also don't design the machine learning algorithm based on the mathematical theory of what's going to work better, but you just experiment and you follow your experimental intuitions. And then you make a mathematical model that tries to show why something is better than another thing or why something works at all. And that's, for instance, true for the transformer. There's still a lot of debates about why the transformer works so well as it does. And are there better ways to abstract the transformer? Right, and so if you look at the current transformer models of vision, we have, for instance, stable diffusion. Stable diffusion is a model that is generated by trustive learning over images. And these images are organized in a highly compressed embedding space that is represented as the weights of a neural network. And then we have so-called samplers that take a given phrase, a prompt, and then establish the similarity between the meaning of that prompt to an image and use this to guide the generation of an image by this model using a so called sampler. The sampler is some mathematical algorithm that tries to figure out how to move the output of this model of stable diffusion towards something that is more similar to the description of the image that you want until it ends in local optimum. And at the moment, this process is relatively slow in the sense of on my M1 MacBook, it takes half a minute to converge to a result. In a fast GPU in a data center, this goes down to a few seconds. But there are now some kids which have designed or discovered algorithms that are not in the public domain yet that are orders of magnitude faster. So there are ways to construct samplers that will use the same model input, the same regularities that have been found to converge in real time to the output. And this enables new applications. So now we can probably use these diffusion models for vision. We can use image-guided diffusion. Instead of using a text as input, it uses images as input and tries to create an image that is similar to the input image. And the nice property is that in order to do that, it needs to create an embedding space that is basically a space of meanings that corresponds to the content of that image. And if we could directly read out and interpret the contents of that embedding space, it would be similar to having a mental state that represents that world, that we could use it as a perception module. And I suspect this is going to be a major application of these models in the near future. Right, when you've got such a model that uses image-based guidance to converge to the state of what you see in the camera image, and this thing contains a perceptual model of reality around it. And if we can connect this with reflection, reasoning, and knowledge, we enable a large set of new, very valuable AI applications. - What would differentiate these AI applications from a human being? 'Cause it's almost replicating the same behaviors. - It is replicating some of the behaviors, but it's slightly different. It's in many ways, brute forcing the behavior of a human being. If you look at how stable diffusion was trained, it takes several hundreds of millions of images that have no relationship between adjacent images. and you connect them with captions of these images. You associate them with description of the images and you train the model by giving it the correct caption with the image and sometimes the wrong caption with the image or very often the wrong caption with the image. And only do statistics over them until the model converges. It requires dramatically large amount of training. There's many things that for the model don't work, right? And it only works because we have so much compute that we can throw at it. And the model has such good memory that is able to integrate over so many bits of data piecewise until they gradually start to make sense. And I don't think that the human brain would be able to integrate over so many images that are disconnected. Instead, what we do is we focus on very small regularities that we can discover in very little data that only changes gradually, where every frame is connected to the previous frame and very similar to the previous frame and only changes in very particular but important ways. And we gradually built up models about what are the first principle components of the change in which we are seeing. For instance, the neighborhood between sensory neurons on our skin or retina. And then we see objects moving over our skin or moving over our retina as connected blobs. And then we try to understand the shape of these blobs when we move our eyes or when we move our skin, and how they remain static in space. And then we notice how they move through space and we notice the dimensionality of the space and the shape and properties of the object in that space. And then we notice how they're being articulated and how the geometry of movement works, how the skeletons of objects look like, where are joints, in which ways can they change? And then we see the motor forces behind those movements. Why are the objects moving in this way? Is this inertia or is there some control system behind it? And what is directing this control system? And eventually, you notice some of them are agents that have goals that can only be explained by preferences that are built into these objects that direct it into the future. And then we discover ourselves and the interaction with these other objects and we try to account for who we are and how that influences the world. - You posted a video on some thoughts in 2016 about us living in a simulation. What are your current thoughts on this? - So I don't think that we live in a simulation in the sense that there is a nerd in a higher dimension that has a computer that simulates this universe similar to Minecraft. It seems to me that when we look at fundamental physics, that the universe that we are in is not a Turing machine in the deterministic sense, but a non-deterministic Turing machine, which means it's branching, it's a multiverse. Multiverses are extremely inefficient to implement on a normal computer. So if you were to live in a simulation, I suspect that there would not be quantum mechanics. It seems that quantum mechanics is the expected outcome if you were in base reality. So in this sense, I think that the physical universe that we are in is most likely to fundamental level of reality. And quantum mechanics emerges over that fundamental level and the particle universe emerges for quantum mechanics in space time over the particle universe. These are basically all levels of abstraction, of modeling, that our mind learns to impose on reality to understand how simpler causal layers lead to the causal structure that we can observe at a certain coarse graining, at a simplification that is more congruent with what our senses can resolve and our mind can model. And so in this sense, we don't live in a simulation, most likely, I think, even though we cannot completely rule that out. But what you and me experience is not the physical world. We don't experience quantum mechanics. We don't experience photons, electrons, and atoms. What we experience is people and emotions and attention and things that we care about and so on. And lights and sounds, and lights and sounds don't exist in physics. They exist in our own mind. So in this sense, we live in a simulation. We live in a simulation that is generated by our own brains. And what you see is a universe that is generated in your visual cortex and your motor cortex, your auditory cortex and integrated throughout the brain into a game world, a game engine. And in this game engine, you perceive objects around you. - You wrote about going from AI to artificial consciousness, even though we program robots and build into them limited AI capabilities. I think AI consciousness is far-fetched. What are your thoughts, what are your current thoughts about it? What are the challenges and how can we build an AI consciousness if we cannot go or get a good explanation of human consciousness or at the very least if my understanding is correct? I suspect that there are many people which have a good explanation of what human consciousness is but they're not well integrated with the discourse in AI and neuroscience and philosophy and that might be because of the different intellectual traditions and the concepts that they use in the language that they're using, the ontology, the metaphysics by which they try to describe the world. People who are good at understanding consciousness tend to be people who do a lot of introspection by meditation. There might be Buddhist meditators and so on. And the language that they're using cannot be translated into the language that neuroscientists are using. And it's very lossy, right? People think that they understand what the other one is saying, but they do this based on the preconception that they already have, which almost invariably leads to some mistranslation because sometimes you need to forget what you think the concepts are and need to learn new concepts from scratch. And part of what I'm trying to do is to get this translation to work. For instance, in our own culture, there exists the word "spirits". Spirits are an old concept that existed before the Enlightenment, and with the Enlightenment we have eliminated this concept of spirits as a superstition. Spirits don't exist. Every reasonable person knows that spirits are not real, because they've been described as disembodied. And everything that exists must have a body. But what we notice is that people have minds, and they want to do things. And how can they do this? There is some coherence happening, some pattern in the patterns of the cells. And I think that spirit is the old word for a type of software, for a type of agent that emerges over the control structure of a bunch of cells, for instance, that interact with each other in a coherent manner. So you could say that a spirit is an operating system for an autonomous robot. And we are autonomous robots made from many cells, and each of these cells in turn is also a very simple autonomous robot, a single-celled animal. And they link together in a state building organism, a human being, and this state made from many, many cells. And the state is organized in a coherent manner. And this coherent manner is an agent that it's not physically real, right? It's only a pattern that we see in the interaction of the many cells that allows us to model them and use that model to direct the behavior of the cells when our own brain is making that model. And this is what the spirit is. I think it's a autonomous regulation system for this shambling amount of cells that is an agent. And when the word spirit was invented, the only autonomous robots that were known were not made from machines. They were all biological or made over the interaction of biological things. So the only agents that were known were people and plants and animals and nation states and cities and so on. And so we can say that to some degree of abstraction that nation states have a spirit and this spirit is embodied by the institutions that do the cognition for that nation state and its decision-making. And that is a very distributed thing. It's concentrated in its institutions of government and academia, but it's also reaching into every individual that is contributing to the discourse of that system. And it's only coherent to some degree. And in some way, that's also true for our own mind in the way in which it works. So I think that this notion of a spirit is a pretty good metaphor. Now, if you have a system that has a reflexive attention, attention is the ability to focus on something in the way in which a conductor focuses on the music that is being played by the orchestra, by individual instruments. To make, in the role of the conductor is to make the performance of the orchestra coherent, to make sure that everybody is playing the same piece of music and everybody is contributing this piece of music in the right way so it becomes as coherent and harmonious as possible. And once you have such a conductor, whether it's the government of a nation state or the consciousness in a human being, you have the possibility that the system becomes more coherent than it could become in a completely decentralized fashion only. So it's an emergent console structure implemented from the same components as everything else. the conductor in your own mind is not fundamentally different from the individual instruments that play the music of the mind. And also the conductor and orchestra is not different in its substance from the people who play the individual instruments, right? It's just one instrument in a way. And this conductor in our own mind needs to be reflexive because everything in our own mind is self-organizing. So the structure needs to stabilize itself. And the way in which you stabilize yourself as the conductor is that you make sure that you are the conductor and not something else or spacing out and lose your own structure. So our own attention flicks back and forth between the content that we attend to and the fact that we are attending to it. And so we don't just model the things that we filter out perceptually when we pay attention to them, but also the way in which we pay attention and the fact that we pay attention. So our attention is directed on content, on the mode of access and reflection. And these three components are, I think, what constitutes the phenomenology of our own consciousness. It's the self-reflective attention in which the attention becomes its own object. - And what is reality then? - It's a term that is used in different forms. The way in which I use this term, that in order to be real, something needs to be implemented. So the real things are those things that are being implemented. But things themselves are only real to a certain degree. The universe is real, in the sense there is an underlying reality that produces causal structure and that seems to also be persistent in a way. At least that's the best model that we can make of it, that there is some continuity going on. So otherwise we would not be able to model it. And inside of that world, we exist as some kind of fossil structure. And when we try to model the universe, we cannot do this as a whole. We cannot take in the state vector of the universe and predict how it's going to play out. We need to make this manageable. And we do this by slicing this big universe into different subsystems that are more tightly coupled than the connections between them. And we call these different subsystems objects. So we separate the universe into small sub-universes. of those can be described as its own state space and we model the interaction between these objects. And these objects are only real to some degree because the universe goes out of sync with our description of the universe as separate objects. And we call this way in which the universe deviates from this nice description into separate objects entropy. And entropy describes the way in which the universe trends away from our description into things that we can manage and control and interact with. So we have to adjust our model of the universe or we have to adjust the order of the universe to keep it arranged into those objects, which means we have to constantly resist entropy. And then what is the purpose of God, if there is a God? What are your beliefs in it? And And how does this existence of a superior power or God work in a stimulated reality that we perceive? So the first step that we need to make when we try to understand the true nature of God is that we need to understand epistemology. Everything that we understand is the result of a mental state that is represented a particular way. that we perceive, think, imagine can only be as good as our minds can represent it. And if there is a bug in the representation of our own mind, then what we perceive will be buggy, what we think will be buggy, what we represent will be buggy. And that means that we need to understand what it means that we represent something in our own mind. What is the relationship of what we represent to reality? And that determines what can be true. And this philosophical field of of what it means for something to be true and under which conditions something is knowable is called epistemology. So basically we need to figure out epistemology. We need to figure out the criteria under which something is true. And what gets into the way is that the majority of people don't have any concept of epistemology, which means you can tell them stuff and they might believe that stuff based on criteria that are completely unrelated to whether the thing is likely true or not. If you incentivize people to believe a certain thing, they might choose to believe it, even if they don't actually have a rational reason to believe that it's true. So for instance, if you ask somebody why something is true and that person tells you, because I've talked to a burning bush, or because it came to me in a dream, is this a valid criterion to determine the structure of reality? Normally, you would probably say no. But it's very different if that person has enormous amounts of power over you. And once you have a situation where a few people have enormous amounts of power over you and use this to impose beliefs on others, they can also generate social proof, which means you see lots of respectable people around you submitting to the same set of beliefs. And that for most people, it's very, very hard to escape from a belief that is being shared by everybody around them because it separates them from the cultural, intellectual, epistemic universe around them. And they are now forced to form their own notion of reality. And that even happens in academia, where people actually are virtually incentivized to go towards truth. Very soon as academia gets into the grip of ideologies that cut people off from the realm of truth and the human thought space, they drop into this and they will restrain their thinking very often to those parts. So when we try to understand the notion of God, it's important to realize there are many institutions that have propagated the notion of God. often very forcefully, they might punish you if you don't have the right notion of God. But this creates an environment in which it's very difficult to understand what you're actually talking about. I would say that our own self is real to some degree. It's implemented in our own minds, but it's a story about a virtual person. The person does not actually exist. Our self is only real to some degree, it can be changed. It's as real as the echo in the wind, the mountains. This echo in the wind is somewhat real, but it's not the underlying causal structure. The causal structure is the fact that there is wind and that there are mountains, and the mountains are changed by the winds by some degree by the echo. But the structure itself is not clearly visible to us. We only can infer the structure of our own mind by the echo that we are observing. And the self, of course, you would say that to some degree it is real, right? It's a software that is implemented as a model in your own mind. And, but it's only in your own mind. And other people have a model of you, but it's not a self. It's not a model of their own agency. It's a model of your agency that they have in their mind. But what happens if you have a self that spends multiple minds? Not just one. So it's an agent that is able to move between minds and go across minds and get synchronized between them and perform things by running on many people's minds simultaneously, right? Such a thing can exist. And this is what we call the God, this is small G. And cultures before our monotheist cultures had many of those Gods. They're basically selves that were forming as archetypes or through narratives or through observations or systems of agency, like ecosystems and their spirits and so on. And they got implemented in part on the minds of people and that became causal power. So the gods were starting to co-exist with the selves of people and for shaping societies and cultures and the interaction between people and the environment. But this is nothing superstitious about it. The gods are just as real as other selves, but you can drown them out in the same way as you can destroy the self of a person by giving them substantial amounts of drugs, subjecting them to mental torture. And in this way, destroy the self, get them in a condition where their mind does no longer house the original self on only a crippled version of it or completely dissociates and no self is left. We also notice at night when we dream, maybe we don't have a self, we only see a world going on, or we have a different self, we identify as something that is completely alien, has no similarity to who we think we are during the daytime. So this notion of God, this Big G, is a total God. And it's a God that is the God of all Gods, right? It's a world mind, so to speak. And as far as I can tell, based on all the sources that are available to me, God was created about 6,000 years ago. There are no sources beyond that that speak to the existence of God. And God is incoherent because he's been fucked a lot. And most of the distros are not very good. So basically it's a software, it's an AI that is running on many minds. and it is synchronized by a dedicated institutions, religious institutions. And these religious institutions indoctrinate young children systematically to give them the same notion of God. And the institutions try to figure out the aesthetics of the total God. What is the ideal total God that is directing the entire civilization and its interaction with life on earth? And because so many people are trying to find this, this thing exists to some degree, it is implemented. And so I would say that God is real, but God is incoherent. I suspect that there are many layers to God and they don't all want the same thing because they're not completely coherent. Even though the specification of God says that it should be coherent because this is what you want to create. And so God is an agent that emerges over the interaction of many individuals and the relationship of the individual to God is the same as the relationship that a cell has to the organism. and single-celled organisms, which don't have a relationship to something larger. In the same way, there are people which don't have a relationship to God. - And then where does psychedelics come into play? Because that's another simulation that we experience. And there's a lot of literatures out there that says that psychedelics kind of initiated religion. - I suspect that this is the result of people taking psychedelics. like Terence McKenna thought that human language was emerged after people took a lot of mushrooms. And I think that he had this idea while he was a mushrooms. And what psychedelics do is that they change the neurotransmitter configuration of your mind from into different states. And they introduce basically dreaming states, states in which you integrate the same information that you have in your mind in different ways. It's not magic. It just puts you into a state of mind that some people are in naturally. basically explores the space of neural transmitter quantum geocorporations. And the effect is usually one that is outside of the range in which you are conventionally functional. So people that are in psychedelics typically are unfit to make complicated decisions about the future. But they are able to explain lots of things, how they fit together in the past, which means from a machine learning perspective, they're probably overfitting. They create functions that are able to explain all the previous data in the training set, but they're not able to deal with the data and the tests that was actual reality. People that do a lot of psychedelics don't tend to be much better in dealing with the real world later on. And so it seems to be possible for people in psychedelics to have important insights because they are able to deploy more bits for the same function and arrange information a different way. They basically need to get back to a sober state and able to prove that these things are actually true because they have just been dreaming. Psychedelics enable a different way for people to dream while being awake. And so they integrate these dream memories into their real-time experience, things that didn't happen physically, happened in their own experience. And now they learn from these things that didn't happen from these dream events. And that's both very powerful and very dangerous because it also means that they act based on training data that don't correspond to anything that actually happened in the world. And that's an important question. Did people discover God because God didn't exist before people dreamed God? And if I look at the history of religions, it seems to me that people could have gotten this intuition much, much simpler. Basically, we don't just have ourselves that we are serving in the world. We notice that, for instance, when you have a relationship, that you're serving these relationships when we have children we start serving our family and this family agent is a system of agency that we feel to be part of that happens also when we have a village or a group of people that interact tightly and as soon as we formalize this interaction and try to understand what is the spirit of the civilization how can we serve it in a more focused way you discover something like a tribal spirit and this tribal spirit is the tribal god similar to the hebrew God. And over a long process of, I think the Hebrews were mostly sober when they did this, you discover ways to flesh this out and make it more powerful and so on. And while being completely sober, you also notice cult techniques by which you can hypnotize people into serving God in particular roles. And I suspect that's also been an important part of the religions, and that there were basically technologies that were being used to implement ideas on people's minds before they could understand these ideas and thereby drag them into a certain way. So I suspect that basically talking to people at a very young age about how reality works is more powerful than giving adults drugs. You've said that simulations can be conscious, physical systems cannot be conscious. Can you explain that a little bit? Yes, physical systems as we see them are are mechanical, which means that there is causal interaction between parts that lead to the emergence of further patterns that you can again explain as causal structure. But there are representations in physics. It's just these parts moving. Similar to the distinction between software and hardware, if you look at your computer physically, there is no Minecraft, there is no text processor. There's only electrical states and transistors that lead to switches in the transistors. And the software that we look at becomes only apparent at a certain level of coarse graining. We need to take a step back and abstract the causal structure in a certain way. So we say regularity in the patterns. And this regularity, this function that we discover is in different layer of modeling that we can impose on the same system, on the same physical system. In this sense, software is a very specialized physical law. It means if we arrange matter in the universe in this particular way, then we are going to perceive at a certain level, this pattern, and then that pattern, and then that pattern. And this is what physical law is about. It's a certain pattern that you observe for reasons of mathematics, based on the arrangement of patterns at a lower level, we see this pattern emerging at a higher level. And in this sense, the mental states that we are having are software states, are representational states that emerge over the interaction of many neurons. And they allow us to make sense about why the neurons are firing at a certain point and how this relates to the action of the organism. And the mind is discovering itself, these patterns. So the mind is discovering itself, it discovers its representational states as representational states. To go back to this notion of an agent, if you want to go from the controller, from the thermostat to an agent, a system that cares about its future, it needs to be able to represent the future. and the future is a world that doesn't exist yet. So we need to create a causal structure that is different from the underlying causal structure. And this insulator of the simulated causal structure of the underlying causal structure is a computer. That is what the computer can do. It can simulate arbitrary causal structure. That is the power of the computer. And the simplest computer that we know in nature is the individual cell. This individual cell is able to compute lots and lots of different things using DNA and RNA by manipulating the symbols, amino acids, that are written on that tape. We forward on that tape and performing actions, including changing the state of the tape based on those actions. This is very close to Turing's abstraction of the computer as a Turing machine using a tape and a red-white head. That is changing the symbols on the tape. And the cell cannot learn everything. That is a limitation of the cell because it's very difficult for the cell to integrate information over a long time spans and to do something with that information. But arrangements of cells can do that. So at a different level of emergence, you can take many, many simple cells and put them into a more complicated arrangement that can integrate information over larger time scales. And this is what we are doing. - What is the incentive-based scale for human cells and let's say AI systems? because I know with humans it's like survival, if I'm correct. - Well, individual cells are in some sense driven by an emergent survival in the sense of cells that did not act as if they were motivated by survival, didn't stick around, because they had to compete with other cells. So it's an outcome of evolution that systems try to persist. But inside of an organism, cells can also sacrifice themselves and they have to be built in such a way that the skin cells are able to sacrifice themselves for the greater whole in the same way as ants can sacrifice themselves for the ant hill or human beings can sacrifice themselves for the state. And it's a very- - Is that the same way? So is that the same way I think you've mentioned in the past about rats and then the next generation and how they change over, like, you know, based on the environment? - I think what you refer to is that animals can adapt to the environment, to not overgraze it. It's slightly different. If you expose rats to scarcity, or mice, it turns out that the next generation tends to be more scrawny than the previous one. And the reason might be that they have evolved for environments in which you have periods of abundance and periods of scarcity. And if you overgraze in the periods of scarcity, you might destroy the underlying food chain, which means that the mice go extinct. So in order to not go extinct and you are confronted with a period of scarcity, you might have to reduce your population and the consumption of your population. So you just stick around and become sustainable and then can expand again once the period of scarcity ends and you go back into a period of abundance. That's not necessarily related to what I was talking to because there is not an individual decision of the mice to sacrifice themselves for mice food, But there might be a decision for a parent to sacrifice themselves for their children. Or there might be a decision for a soldier to sacrifice themselves for the nation state. Or for their friends. And these decisions where an individual organism is willing to sacrifice themselves for something that is more important than them means that we form agents that are above the self, above the ego, and be a part of those agents. And our notion of God comes out of scaling this notion of a group self, of a group identity that generates purposes above the ego. And it's an adaptation towards being state building. - How is that different from what you've mentioned in the past that we've doomed ourselves by like, you know, signing up for industrial revolution? Because now we're doing counterintuitive to what we should be doing based on the model that you just explained. - It seems to be very hard to control a state that is industrially organized, centrally top-down, we find that dictatorships are very brittle. And if we don't have a dictatorship, but some kind of distributed execution of power, it means that we have different incentive structures. So the regulation in the society with distributed decision-making means that you have lots of local agents that act in a more self-interested manner. And this leads to conflicts, because it means that the individual might not have the same incentives or incentives that are compatible with the incentives of the greater whole. And this can only be solved by integrating the regulation more top-down. And so basically you need to be able to get people to behave in ways that are in the interest of the global system, even if it's not in the local interest. But if you, for instance, are a parent and you have children, you may not want to sacrifice your children to the greater whole. You want to give them the best possible healthcare. You want to have them to eat well. You want them to live well. You want to insert them as high in the foot chain of society as you possibly can, right? And everybody is trying to do that. So when we are confronted with the question of whether we give our children the best possible resources, or to create a world in which we can live sustainably for as long as possible by responsible resource use, we might choose our children, right? And because if you don't do it, somebody else will do it and take the resources that you don't give to your children. So in this sense, we are doomed to using up as many resources as we can. And this means that we get into a situation where we might be extracting more resources from the environment than the environment can reproduce. And at some point we might not be able to find a technological substitute for the food sources that we destroy, for the ecosystems that we destroy and depended on in the past to feed ourselves. So we can create a situation in which the environment is no longer able to sustain our food sources and the climatic conditions for our survival. And this could mean that large parts of the planet become uninhabitable for humans and our civilization, its present form collapses. - How would this end? I know like us as a human form would change, but is there a possibility that we might like, you know, transfer our consciousness into another system? And then that system would not require the tools and the environment that we require? - I don't see how consciousness would be transferred because your consciousness, for instance, is interrupted when you sleep at night and are in deep sleep and don't dream. There doesn't seem to be a continuity of consciousness. The continuity is only projected the science set. You remember some things from yesterday and you think you are the same person as yesterday. So you project this identity, but there is no continuity, right? Your consciousness is recreated again and again and again. And this means if you cannot even be transferred between yesterday and today, but only recreated in a shape that is similar enough that you can pretend to yourself to have the same consciousness as yesterday or the same self as yesterday, with only gradual changes. You can also not translate it or transfer it into a machine. You could only create a machine or a system that thinks it's you. But it wouldn't need to share many properties with you beyond that. So who is the self, then? The self is a story that the brain tells itself about the person. So can that story be uploaded onto a system? - In principle, yes. So we don't have a technology that does do this in the near term by your brain uploads, but I think it's conceivable that just by monitoring the output of a person sufficiently well, you can limit the constraints that direct that person in such a way that we create a software that plausibly recreates the personality of that person to people who interact with this recreation. And that recreation themselves could be organized in such a way that it also thinks that it remembers being that person and identifies as being it. But it's entirely virtual, right? It's a software that thinks that it is real. And that thinks that it's extended over time and that thinks that it's all these other things. And it can also understand that it's not. And it's also true for human beings. We also can get to the state where we realize that our consciousness is not continuous and there's only the now and there's nothing but now. And there will be different nows in the future and at some point there will be not, but it's not a reason for concern. So you can get, once you get to this state, you lose your fear of dying. And then once you lose your fear of dying and you look at what's happening, you notice that it's actually not about you. Life on earth is about the cell and the cell sometimes organizes into single celled organisms that form rich ecosystems and sometimes into multicellular organisms and these sometimes into societies of individuals that reflect about having a self and gods for the societies to organize themselves. And then we realized that this is a temporary thing that things in the universe on earth stick around for a while and then they go extinct. And some stuff that stays stable for a very long time stays stable because it doesn't need to adapt because it's an environment that doesn't change very much. But humans are something that only has limited adaptation. We have very specific bodies and psyche and so on. And it will be difficult to change us in a fundamental way into a very different species. And we also a species that is very powerful. We change the environment dramatically. And it's not clear if our own existence is compatible with the changes that we are going to impose on the planet. But we are not going to be the last intelligent species on this planet, right? At some point, you're going to go extinct. And it's not clear when that is going to happen. It's just how life is. And at some point there will be new species that take our place. - You mentioned before that there's a possibility that trees or other things within our environment are more intelligent than us. It's just a difference in like the timeframe that they are, their awareness or they are there. Can you explain a little bit about that? - I don't see any evidence that trees are more intelligent than us or even ecosystems. So I mean, I don't know whether that's possible. I suspect that basically every cell can send chemically encoded messages to the neighboring cells and it can learn to make the sending and its interpretation of the messages that are being sent conditional, can evolve to do that. Which means that if you have a multicellular organism that gets old enough and you have subjected to evolution and it benefits from coherent information processing, that every organism in some sense is going to function a little bit like a brain, just by the cellular information exchange, but much slower than our brains. Because if you just send messages to your immediately adjacent cells, it's going to take many seconds before the signal is moving more than a few millimeters for the most part through the organism. And so the information processing in plants is going to be much, much slower compared to our own information processing. If that makes make sense of the world than at a much slower time scale. Because plants don't get older by the same factor as we, they are able to integrate information only at a much, much slower rate, which means that they're not going to be as smart, comparatively speaking. So the benefit of humans is they're basically telegraph cells, they send information very quickly over long distances in the organism at a very high price, at a very high metabolic rate, need a lot of energy to be able to do this. So plants are not able to afford this because photosynthesis does not, for the most part, seem to give enough energy to the organism to make this high metabolic rate possible. In order to do this, you need to move around and eat plants that have stored that much energy. So you can process it quickly enough to perform this information processing. And the reason why we do that is because we compete with other animals for the same food sources. So we cannot afford to be very slow, do information processing slow. And so our nervous system is an adaptation that happened due to this evolutionary pressure to process information much, much faster. And then integrate more data into shorter timeframes in a better way. So in this sense, I suspect that plants individually, at least, by far not as smart as mammals are. But if plants stick around long enough and integrate information over long time frames. I suspect that at the ecosystemic level, there could be considerable levels of control in ecosystems that emerge over the information processing of many integrated coordinated plans. - What is the self in relation to the puzzle of consciousness? And how is it related to the simulation our material brain is creating? - The self is a model of our own agency. At its core, it's a model of being an attentional observer. At least that seems to be the minimal self that I can have, that I notice that I'm observing. But for the most part, I'm integrated in my everyday life into a personal self, which means I remember that I'm a father, I remember that I'm a researcher, I remember that I'm a human being that has friends and that might be hungry or thirsty or tired or awake or elated or depressed. And all these things are a model that allows me to understand of how I am going to interact with the environment from now on. And use these interpretations to have thoughts and I store these thoughts in attentional protocol and use them to direct further behavior and future thoughts, right? And the same thing happens with my observations of my feelings that are generated below my reflection in my mind as assessments of my alignment to the world that is being simulated in my mind. And basically this is the nature of the self. It's a model of my own attention and agency over many domains, integrated into a story about a person that interacts with the world. And this story is being, it's not a story written in words, but it's a multimedia story written in the language of thought. And that is being used to direct the behavior of this organism in the world. - What is dualism, materialism and idealism and how are they connected? - They're basically attempts of a philosopher to classify a base in which the world can be metaphysically organized. And in most traditions, idealism is seen as the basic idea that we live in a dream, and that a dream is fundamental, and also means that magic is possible, and so on. And idealism has to explain why things in the dream are so regular, and cannot be changed as much as in a dream at night, or in a lucid dream, where you can change everything around you. And then there is materialism, which is the idea that only matter is real and that we live in a physical world that is made of mechanical interactions. And everything in that world is the result of the emergence of physical patterns. But materialism has to explain why something feels like something and how consciousness works. And dualism is the idea that there are two worlds that interact with each other, physical world and mental world and they somehow exchange causal structure. And the original formulation of dualism in the Western tradition is understood for instance to be Descartes' idea and that is idea that you have an extended world where is extensa, the extended stuff, and where is cogitans, the thinking stuff and they somehow interact with each other. And another thinkers thought about this about this, they saw a problem because the physical world is defined in such a way that it's constantly closed. The physical world is exactly all the things that interact with the physical world, and can be described in physics. And physics works super well, right? And no obvious gaps in physical things are happening for reasons that we don't understand. They, they happen in ways that might be mathematically complex to grasp. But that doesn't seem to be any spooky ghosts, interfering with physics in ways that mess up our experiments in certain and unexpected and surprising ways. And so the question is, how does the mind interact with the physical world if the physical world is closely closed? And there have been attempts to deal with this, for instance, occasionalism, that they just happen to be concurrent and run in the same way and God is somehow synchronizing them. And it's, of course, very ad hoc and awkward and not satisfying. And I suspect that the best way to resolve this conundrum is to realize that, well, first of all, space is not real. Space-time is an approximation, a mathematical model that we are making over the patterns that we interact with. It's approximately true at a certain level of resolution. It doesn't work anymore at very small scales. It may fall apart at very large scales because there are singularities in the space-time, and it might not work at very high energies. So it happens only for a certain way of abstracting the universe in certain contexts. This notion of space-time is basically the set of locations that we can discern in the universe that can hold information and the trajectories along which information can flow using a certain mathematics that models the universe as a continuous interconnected space. And this is only approximately true. Space-time is not completely real. And this notion of stuff in space that we form is also not even relativistic space-time. we don't perceive relativistic effects in our perception. The stuff in space that we are observing is generated in the visual cortex. It's an intuitive model that our brain is generating that's pretty coarse and simplified and just exists to allow us to organize ourselves in the environment at the time scales and spatial scales that we can perceive and make sense of. - What does the future look like though? I mean, how would it evolve itself? Like I know we could just destroy ourselves, but what's the alternative reality of it? - Well, we can wait until something else kills us, right? We can wait until the next super volcano erupts and makes agriculture impossible for a few years and we all starve or freeze to death or a meteor hits the earth with similar result or we have a serious pandemic that eradicates enough people to make survival of the rest impossible or our global warming tips the oceans and we have an eruption of cyanobacteria and the atmosphere becomes unbreathable for us. Such things are conceivable. And they might happen if you just wait for long enough. A catastrophic event that is going to eradicate complex life on Earth is a statistical certainty. Right, in the same way as these events happened for statistical reasons every few million years on the planet. And it could also happen that we ourselves are the reason for these things that are happening. It's not us individually, but the dynamics of our species and the way in which we interact that we cannot completely control and foresee. Might also be possible that we get our shit together and stick it out. But what for? I suspect that the optimal solution for life is not human beings. We are a very particular way of particular ecological niche. We are monkeys that have a long enough childhood to integrate information sufficiently well to form cultures that can understand how minds work. That is surprising. We are the first species on the planet, I suspect, that is able to understand how minds work and build AI and even build machines possibly that can think. And it's going to change if we succeed in this, possibly life on Earth in ways that we cannot foresee yet. - Do you think maybe there's a possibility that there are some aliens out there looking at us like monkeys and watching us evolve and seeing how slow our evolution is? Well, if you look at the cell, it seems to be a search algorithm that has a Turing machine self-replicator adaptation mechanism built into it. So if you give the cell long enough and then stable environment, the cell is probably able to evolve intelligence and consciousness. So in this sense, the cell is a good probe to send around in the universe to colonize the universe. And I don't think that we are especially slow in this regard. It's something that just takes very, very long. It takes as long as it takes. And some random effects play a role and some environmental effects play a role. I don't know if you are particularly slow or fast. I don't doubt that we are particularly slow. It seems that life arrived on Earth very early on. We don't know whether it arrived by meteor that infected it from another planet. another planet is the first few cells or whether the first cell indeed did emerge on our planet. It seems that all life on earth comes from the same cell, from the same source, that there were not multiple origins of life, at least we don't have any evidence for that. And if aliens exist, it probably requires that our current physics is very incomplete, because it might require that there is faster than light transfer of information and physical things in the universe and we don't have generally accepted physics that allows for that. But if such physics exists, which we don't know, then faster than light travel is possible in ways that doesn't destroy the structure of the thing that is traveling. And if it doesn't require exorbitant amount of energy, then it's almost certain that aliens are here and observing us. Right. But we don't know that as far as I know. And I'm not able to evaluate the UFO sightings in any productive ways. Basically there is this very weird phenomenon that every dozen years or so it's like the irregular interval there are a number of blurry pictures of UFOs and retired air force pilots that explain what they saw. So this could all be an op that the governments use to regulate the amount of lithium and the water supply to make sure that their populations don't go too psychotic or who knows what's going on. It's got to be also basically a certain baseline of memes and unexplainable phenomena that always are around or maybe there are indication that something like this exists or it might be new technology that is being from some discovered and that is not made public and it's only kept under wraps by the military who knows I don't know. So if past and night transfer translation is possible maybe Kevin Kelly's interpretation is correct. Kevin Kelly suggests that if faster than life travel is possible, then the aliens will largely not be visible because they will be traveling outside of the night. But if they want to observe us, they need to put something like a periscope into our space. And what we see from the UFOs are basically just the ends of the periscopes, which look blurry to us that move around very quickly. But the actual spaceship or probe or whatever it is, is going to be not visible to us because it needs to move outside of this photonic interactions that we perceive. What are you working on recently? What keeps you up at night? First of all, I have children that direct, command a lot of my attention and they take away my ability to write books, which I would like to do. I'd like to, I think, write a a book on translating and harmonizing these different concepts from AI and philosophy of mind, and also from other traditions and other cultures into a language that we can all understand, in which we are able to communicate across the cognitive sciences and philosophy in a productive way, and make more sense about who we are, and maybe use this to get to a renaissance in psychology and neuroscience that I think we need. And in the near term, also interested in all those parts that AI cannot really do. How to build a system that is able to learn in real time and update itself continuously, that is self-organizing rather than using the brute force of stochastic gradient descent on a neural network architecture with lots of GPUs. How can we build something that learns efficiently with very little data? How can we build something that is entangled with the world and discovers itself and the interaction with the world and can create shared states with people? how can we understand people using such systems and change their states and make them up into different states? And I am very fascinated by the current developments in generative artificial intelligence. And even though I think that they would force what we should be doing with more elegant algorithms, I don't see an obvious limitation to what these systems are doing. So it's very exciting to see how far we can get with these present approaches. - How close are we to that reality though? We don't know. I don't see good arguments that say to me that must be one way or the other because we don't know what's missing. It seems to me that the algorithms that we are using are very different from what the brain is doing, even though some people might disagree with me. But the way in which we learn is really based on change between adjacent frames. Static information is unintelligible to us because it doesn't produce causal structure on our own mind. All the information that we care about is about change. And the paradigms in machine learning are mostly focused on static images. So we look for patterns and a static thing. And we try to correlate many of these static patterns and then try to understand how we can model change using these static patterns. And maybe there's a continuum between those approaches. And maybe we can use the other approach to do this. Obviously, we can. We we can with great effort and so on, try to use generative AI to produce videos. That's not very good yet, but maybe there is something that is much more efficient. And if we want to get there, it's not clear, can we do this by making incremental changes to what we've got or machine learning, or do we need to provide a lot of the algorithms that we are using and start from scratch? And I don't know this at this point. - Do you think there would come a time where an AI will develop itself in such a way that it can understand and create a system which will replicate the way we think? - I think that's almost unavoidable that we are now building systems that learn how to program, right? And we can use them to support programming. But as soon as we can get these systems to understand the place that they have and the world and the things that they should be doing and search for them and improve their models of what they should be doing and find strategies to do these things. I don't see how we can get these systems to surpass human performance. I think to a certain degree they are like with AlphaGo, you have mentioned that they are performing better than humans and soon enough they will take over. Yes, but for a normal human being it's not clear why they should waste much of their life getting very, very good at Go. And so for many of the skills where somebody is really, really good at the mental technique, they need to have an impaired metacognition while having unimpaired pattern matching abilities. So basically, they need to have a form of autism. And the present AIs are ultra autistic, right? They can only do pattern matching in the way which they're set up, but they don't ask themselves, what am I doing here? And what is the thing that I should be doing instead. It's not just this motivation itself, it's how to modify your motivation, how to build it up. You start out with hunger and thirst and boredom and exhaustion and a need for appreciation but at some point in your life you become an adult and you ask yourself what is it that I should want, what should give me pleasure and pain and then you adjust and you become more the thing that you need to be under the circumstances that you find yourself to be in. Is that what's separating humans from AI is that like, you know, soon enough it will create a system where it will require like, you know, this gives me happiness or this gives me pain and then based on that their motivations evolve and that's what the new system would be. As far as I know there is no AI yet. There is in the sense there is no artificial mind that wants something that is able to learn by the interaction with the world in itself and observes itself in real time. There are only models that do statistics when they're being asked to perform a certain problem. And so there are components and there are crucial components. And we don't know if we have all the components, even though I suspect that we already might. And it's not clear how to make these systems continuous real time and self modifying in a flexible enough way. But then again, maybe we can use the present algorithms to do it and build a system that is not human like, but that is sufficiently smart to outperform us in all the relevant areas to build the next generation of AI. All right. Thank you so much for speaking with me. Yeah, I enjoyed this conversation. Thank you for inviting me on your podcast. [MUSIC]