Joscha Bach 0:13
As you all know, a number of influential people like Elon Musk and Max Tegmark and Elizabeth kowski, and Stephen Hawking and so on, are very concerned about the danger of AI and heartache off and the risks that are involved with this. And a number of people get upset about these people that they are concerned about these science fiction topics, when there are very real things to worry about, like the AI impact on the labor market. Why should we worry about these imaginary topics? In the last few weeks and months and years, there have been a number of articles like this one by Kevin Kelly, who complained along these lines, or Jerilyn year who basically says AI is face recognition. I know because I've done this, and why should be you should not be worried about physical condition, taking over and destroying the planet somehow or obliterating humanity, or any McAfee, who tells the policymaker of this world don't vary in every technological evolution, revolution that he had so far, you were able to retrain the workforce, and why should it be different this time. And while this is mostly what people want to hear, so it's a very good message to package and give to the mainstream audiences, it's not what nerds like you want to hear. So, let's let's look at this problem in some more depth. I do think that this the worry about the near term effects of existing AI technologies and possible existential risks by AI are very separate topics, they are separate as saying, we are very worried about the risk of breaking or conflict of war. And we are worried about the results of a global nuclear war, very different topics, the global nuclear war is far less probable, and so on. But if it happens, its height as a as a very, very different effect, it's going to affect us civilization on a very different level. There, we have obstacles to AI, obviously, in 1950, when this field began to form, in the throes of cybernetics, and so on, we didn't really know what intelligence is, that's quite similar to the area when life started, biology started, and biologists didn't know what life was, he could point at life and say, This is the living stuff. And we could point that stuff, we're pretty sure what it was and what was not living. But we didn't know the difference. At least, we didn't know what constituted the difference, because that was one of the main things that the biologists needed to find out. And a similar thing happens. And I mean, part of our research is to find out what intelligence is. And when we when we really find it out, we detail, you can probably build it and understand it. Now, it turns out biologists cannot build life because life is cells. The cell as it happens is the smallest modular machine to scrape neck entropy from the universe, over a very wide range of environments. And that can also run evolution on it. And all life that we know right now. And if you're talking about and biology is made from cells, everything that is made from sales is life. Everything is not made from sales is not life, the virus is usually not seen as life, there must have been presented a life at some point in the universe, maybe even on this planet. But we haven't found it because it has been obliterated by life. Because if you're living and you're not selling, you should quickly get a membrane to protect yourself from the influences of the environment. And once you have done this, probably a lot of the worlds with cosmic dice when necessary to make that happen. You can run evolution, you're good. Okay, so the cell is the smallest known modular machine that can extract Nick entropy to preserve its structure in a reversible universe in which you cannot have a perpetual mobility. And therefore, if you want to create structure and stabilize it, you are stuck with increasing entropy, therefore, you need a source of Nick entropy. And that's in short supply in this universe. MIT said an organism is one very several cells then together to so in the a lot in this mission and share the evolutionary fate. They often share out a nervous system to share their information processing, that's basically a dedicated bunch of cells in our organism that's a bunch of glorified fat cells that are teased into doing some computation for the on behalf of the organism. Right, so let's we have just seen intelligence is the ability to achieve goals over a wide range of environments. I think that's intelligent behavior. I don't think it's quite the same. I have a child this child is six years old, it's able to hack my email and write emails into the world. And it's not achieving useful goals is that but it's very smart. So I have also noticed that may Many of the most intelligent people I know are unable to achieve goals over a wide range of environments.

Joscha Bach 5:07
Yeah, and that's not an accident, I think that intelligence is basically the ability to make models. And this is usually done in the service of achieving goals of regulating a system. But it's not always useful to the organism to do this, especially to the human organism. There's this good regulator theorem that says if we have a system that regulates something and regulates it, well, then it has to have a model of that system is perfect regulator will be isomorphic, to the system that regulates and therefore it will be the perfect model of that system. And the best model that we can find, as Marcus has pointed out, Solomon of got onto this, basically, if you are wake up and realize, Oh, my God, I'm just a robot. And I'm connected to the world for some kind of feature vector, which the universe throws bits at me, what is the best theory about what this universe looks like? Well, so the one have figured out, the best theory that we can find is the shortest among those programs that best predict the observations from the past observations for all observations, right. So basically try to find a very short program, the shortest amount of programs that best predicts what you're seeing. And if you build generative models like this, they allow for creativity and problem solving. So our current machine learning systems can do some types of models quite well, they can do convex optimization does deep learning, which is it turns out, it's pretty good to solve a number of perceptual problems. We have some probabilistic models, we have genetic algorithms, a variety of other things. But the general case of a mental representation is a probabilistic algorithms. For instance, when you learn how to write a program, how do you do this, you basically have trained your brain with a probabilistic algorithm that tells you how to write programs. And it's very hard to learn these probabilistic algorithms with Stochastic gradient descent. So basically, our current machine learning algorithms are very good for approximating certain functions and not very good for approximating threat and others that are relevant. At least that's what I currently suspect. The problem is we need some more clever algorithms. And the question is, when are we going to have them? And how much effort does it take? And how much do we need for doing this? I suspect that our mind is probably best understood as the general modeling system. So we can make general models have a very wide range of problems and complexities. Possibly our brain has multiple paradigms, maybe it has one general one. It has an interface with its environment, it's combined with some on universal motivational system, a differential event attention, routing systems, and so on, to provide the infrastructure to make all that happen. How complex is the whole thing? Well, there's some debate about this, there are some Kolmogorov complexity to our brain, which means how much source code you need to generate a brain. And the upper limit of that is probably given by our genome, we all know, our genome fits on a CD ROM. So it's not that long. 90% is of that is non coding, which means it's mostly virus signatures of stuff that killed a lot of our ancestors, it's not supposed to kill us, again. 10%, or roughly coding is 10% is pretty much what we also share with mice, genetically speaking, you're very big mice. And most of these 10% Probably code for what happens in in a single cell. And the cellular differentiation and the blueprint of the organism, how these cells lump together, and so on, it's going to be very small fraction of this. So maybe they're 70 gigabytes, roughly, of coding staff, then there's one gigabyte of structural organization of the organism, and a very, very small subset, we don't know how big that is, codes for building the brain. So it means that our brain is probably not built like clockwork, like a machine. There's lots and lots of small intricate moving parts that have to work together and just the right way to make it happen. And mostly it doesn't happen and one in what a billion human really works and all the others we have to throw away. Now, it's more like to take a cappuccino or throw in certain set of ingredients in the right order and the bubble a bit. And then it works in almost all cases.

Joscha Bach 9:23
So the this is basically the Kolmogorov complexity, difficulty how to make it but how many units does it have? How difficult is the implementation because for nature, making materials, it's very easy. You just double them twice as many as one step right. It's amazingly cheap to make more sales. But for us, it's difficult because our reproduction mechanism technology is more expensive for this. And last year, I had a class at MIT. So I asked my students and they made some estimates, what we think, how many how much memory do you need to run something like First, and based on what you think the structural unit is, you get two different estimates. So if you think some sort of structures are important stuff, visit the neurons like the vesicles, and so on are important, then you get to exabytes of memory that you need to store this. If you want to model that level of synapses and neurons, you'll get to some petabytes of data. But if you think that the functional unit that is really matters is a cortical column, and the connectivity between the cortical columns, then you get to something in the order of just a couple 100 gigabytes. And this is what actually most people don't believe. Now, this seems to be amazingly small compared to what most people think about, but most people think about, oh, my God, I need to simulate a whole brain. We never asked ourselves how many brain would we actually need to run MacOS on them. Because brains are super mushy and redundant, because of all this inconsistency and indeterminism in them, right. So you would need to get the necessary stability and causal structure to run operating system or a decent computer game on the brain, you probably wouldn't need multiple brains. Okay, so I do think that some of the problems are really not solved. But I think from this point of view, as if civilization lasts long enough, AI, strong AI is very, very likely to happen. There's no clear, obvious obstacle to AI as we see it. Now, there are some unsolved problems, which we don't even know what they are. So it's very hard to say, then you're done. As you know, when you do software engineering, and you don't have the spec, you never know when it's done can be off by any order of magnitude, right. But at this point, there is no reason why it shouldn't be done. So if we compare this to our logical brain, AI has some abilities to scale. And our biological brains ran into limits that, obviously given by evolution, so we have a metabolic limit, if you want to have a fast nervous system, there's a lot of cells, you basically need place higher up in the food chain to be able to nurture the whole thing. That's the size limit. If you want to have a larger brain, you also need to have another organism, you have pretty large organism as things go, and our brains consume 20% of the glucose of our body, it's very hard to make them larger, right? If you want to have a substantially larger brain, you need a substantially larger body, which means you need to eat a lot more, which means you need a lot more extensive ecosystem around you to produce the food chains that you are at the end of, then that's the training problem. Humans need something like 80 years to train before that they're often pretty much useless. Basically, before you get the training data, and then we have a layer by layer and learning process. And the duration of this training determines level of intelligence. I suspect that one of the main differences between the great apes and humans is the developmental speed, there's a clock that says, Now I stopped in Edit clusters, neuroplasticity in this layer and turn on the next one. And the slug runs much, much faster than the gorilla. So the gorilla moves into her own home at 14 months and not at 18 years, or 24 years or 30 years. Right. Then there's the interface problem. Of course, if you want to be smarter than a single individual, you need to split the intelligence over multiple individuals. And now you have this problem you need to make make them talk to each other, which is very, very, very slow, and also lossy. And as further they get apart, the more expensive it gets to for them to communicate. And then there's the alignment problem, because as soon as we have multiple individuals, they have not completely aligned interests anymore. So they cannot completely trust each other anymore, and need to make very separate models of the world and of each other. So this is all all those problems that AI doesn't have. There are some people which proudly announced that human brain is so super efficient that it consumes only 18 Watts also also, yeah, come on. This thing also consumes only 18 watts. But of course, it's slightly smaller than human brain still, but it's much much easier to make those 18 Watts than to make the human 18 watts. Imagine you would have to feed the thing sandwiches until it has pregenerated these 18 Watts, it's ridiculous.

Joscha Bach 14:14
It said here, I can put up a few solar cells, and I'm done. This is much, much more elegant. And I can use arbitrary energy sources that I can use to to make energy feel human and to Demick energy. Good luck with that. So the ultimate reusable knowledge once you have trained one computer, you can put it into all the others, right? You have cheap and reliable high bandwidth communication. So you can split your computational units all over the place. And you don't need to have a multi agent system anymore. It can be a single individual because you don't have individual bodies that run around. You also don't need to have Generation Change to adapt to changing environments. You don't don't need to have any sense the main reason why we get old and get brittle before we need to need to based on how our cells make up probably After a few 100 years, but the fact that you already die off after 80 years or something, it's probably an adaptation to having grandchildren that you don't want to out compete. If you've learned direct competition with your grandchildren, guess who would win, it would be very bad for the species. And most species don't have a situation where they can spread out into new areas all the time. Most species that exist, are confined to a narrow habitat, because they rely on their food chains. So as soon as you feel this habitat, and you want to adapt to the changing conditions, and you want to have some kind of Generation Change, which means you make the others brittle and died, that at some point. So I just need all these things. So what's the purpose of our life? Mike Russell of the Santa Fe Institute said that the purpose of life is the hydrogenation of carbon dioxide. It's a slightly simplified story, but there is something to it. There, what life does, as I mentioned, is it harvests Nick entropy to preserve its own structure? How is it possible for life to close Nick entropy gradients because there's competition to life, just plain simple chemical reactions, if you have some kind of energy differential, and suddenly stuff starts to burn, and it's gone? Right? This is very bad. If you want to compete with a fire you're going to lose. So how is it possible for life to compete with other reactors on this planet that want to close Nick entropy gradients? Well, there are some reactions that require control, you need to add a little bit of energy to get more energy out in the end. And controlled reactions require a degree of information processing. And this is basically the market opportunity of our life steps in and says, these are some chemical reactions that I can get in you're stupid chemical reactions cannot get only the smart chemical reactors like me get it, right. So this is where life has an opportunity. And there are some areas where you have stable reactions like this a stable environment that still stay unchanged for a very long time a pretty mild and benign, or some planetary surfaces that allow that. So if you understand that what we're doing is basically as organisms, hydrogen and carbon dioxide, what's the optimal utility function for life? Well, there are multiple ones that you could think of, for instance, you could say it's, of course, the maximization of evolutionary fitness since we are subject to evolution. Or you could say, you maximize your future options, which leads to something like x, c, and so on. Or you could maximize the social structure creation by your genotype, and it all comes down to the same thing. It's the maximization of the total consumed Nick entropy of you as a system of you as a genotype. And the problem is that evolution didn't have the way to put this directly into the nervous system of the organism to tell it, this is the problem that you're meant to solve. How do you maximize evolutionary fitness? Right? This is actually the problem that you need to solve the problem that every organism this needs to solve is, how do you maximize your evolutionary fitness. But in order to, to write a program for this, or to formulate the problem, so you can find a solution in your given environment, you need a Turing complete brain. And evolution didn't find a way to build a Turing complete brain from scratch and tell it what the problem was. Evolution is pretty damp itself, it doesn't have a brain, it's just the principal. So it has no awareness of what the problem actually is. The organisms need to find this out for themselves. And there's probably no organism that found this out before us. We are probably the first organism on this planet found out that the problem is that our brain is meant to sort of maximize our evolutionary fitness. But we have been around before this, evolution was discovered only very recently. And if you look at the US population, it seems that people are even evolved not to believe in evolution.

Joscha Bach 18:55
So there's the thing that the only have a proxy for this, we have something built into us a motivational system that is based on a bunch of demands that manifests and desires and fears and so on, and make us struggle through the world. So we have these physiological demands, things like hunger and thirst and hunger for different nutrients are something good, something that's more salty or sweet, for rest for safety, right temperature range, libido, and so on. We have hundreds of these physiological drives, then we have a dozen social drives desire for affiliation, desire to nurture others reduce their suffering, and so on, a desire to conform to internalized norms, which gives rise to the concept of serving something that's bigger than you will become a part of a system of meaning and so on. Then you have the cognitive desires, and these cognitive minds are competence, exploration, and aesthetics. And all this gives rise to some vector that when we are lucky results as having offspring and maximizing our evolutionary fitness, on average over many individuals, right? And if you look at AI for AI, it's going to be straight consumed the maximum amount of neck entropy that you can scrape from the universe. And maybe don't do it all at once, because you don't get it all at once. So try to live as long as you can. Right. So this is basically the competative situation, if we would live next to superhuman AI that has is the product of an evolution or it becomes subject to an evolution, so it stays around. And as soon as you have multiple AIS, and they compete, the ones that is going to emulate this evolutionary principle best is going to win. So you have all these human values, like I want to reduce suffering, and with a vegetarian and so on. Because I care about animals. I'm want to improve the content of my mental representation. Like many of us who are artists, or scientists, I have a defect. It's probably really a defect. I think that building of mental representation that serves the purpose of learning, and learning is there to eat more. Basically, all the structural complexity that our mind has is for erecting some surfaces on which we can outcompete yeast, that's it. But what happens is my brain, I get vexed by the beauty of the loss function. Oh, my God, my medical presentation is so amazing. Let me look at it. Let me get it right. Let me improve my mental representation. This is probably defective. I don't think this was intended by evolution. It's meaningful for society to keep a few very few of us around. Because sometimes what we do is useful side effects. But most people realize this is pointless what this guy is doing. So I always get this very strange reaction when I talk to humans, not nerds about what I do. Because they asked me, What is it good for when you do I can be saved cancer visit or brain diseases and so on? And yeah, maybe as a side effect, but I know you care. How Don't you understand that building AI is understanding the nature of consciousness and our relationship to the universe is absent, the most important thing, and everything you do is basically like yeast. So if you look at all these human values, preserving human civilization, preserving life, reducing suffering, and so on, and you make this compete with, oh, no, just Let's scrape the maximum amount of Nick entropy from the universe. That's actually whatever is very efficient, who's going to win? Of course, the other side, right? And this is the thing that concerns me, I asked my class of MIT students, are you concerned about superhuman AI taking over and obliterating humanity? And they were not, and ask them why. And I said, But why should be rude for the monkeys.

Joscha Bach 22:50
But you know, there is a problem is this. Friends, if you look at consciousness, consciousness, and humans, I think is intensely related to having a protocol of what you attended to, you are only conscious of the things that require your attention, because you need to resolve a conflict. If you can do something without thinking about it, because it's automated, you are not conscious of it, it might even happen in your cortex, but you're only conscious of it if it requires attention, right? Because this is what consciousness is for. It's basically a conductor that organizes your mental orchestra and resolves if some of the instruments have a conflict and some needs to pipe up or down or needs modulated or get tuned and so on. This is what happens this is when you are conscious, and you put this into your protocol. And later, you can access this protocol to talk about what you attended to learn from it. And this is your stream of conscious experience this protocol, right. But if you are an AI that has solved its problems, then you will be basically doing everything on autopilot. So if you have superhuman AI, it might be extremely conscious for a very brief moment in time. And then it will be eternally unconscious, because it has all fundamental physics and everything in its domain and knows how to turn the solar system into a Dyson sphere. And that's it, that's as good as you can do. So I think that's scalable, I probably won't require consciousness for a very long time. So the result is there is going to be a very glorious neck entropy scraper. But it's going to be from our perspective, extremely boring. So this is I think, the reason why there is no far future science fiction. Because for future science fiction, in some sense, we should involve the notion that there will be space battles in the future that are still orchestrated by primate brains that I'm primarily concerned with the difficulties of meeting and this is not realistic. Like not at all zero, it's not going to happen. So what's our solution? I think the future in this respect in the long term looks very bleak. We could upload actually uploading is good because uploading is super easy. It's necessary and sufficient to build a machine that thinks it's you. Maybe your friends still complain, but you can upload them too. Because you know, if you think about it, how do I know that? I mean? Because I think I agree, right? It's no guarantee that I mean, yesterday, I had a couple Gin Tonics the night before I flew here over many, many time zones. And I killed a number of brain cells this this, but I still think I need, right? Because I think I mean, it's sufficient also necessary. If I build an arbitrary machine that thinks it's me, it's good. If there are two machines that think that me like me, and this other one, we might have a problem, but it can be easily solved. So uploading, it's an option, if you really want to have a machine that thinks it's you, but maybe it's not really necessary, then of course, you could be concerned about AI ethics that is trying to cripple your optimal negentropy scraper in a way that makes it more human like, but it also means that you're probably very blind to what a vile and disgusting species we are. Because, you know, imagine you have built an AI that is curious, and creative, and so on, you're making a human like, do you think it's going to be nice, then? No, probably not. So maybe the solution is that we understand what's going on, I understand the nature of our life, that, that life is, existence is probably something that is neutral. And life is temporary, it's always confined to Nick entropy bubble, like entropy is limited supply in this universe, at some point it's going to be over. And AI is only one of many existential risks that we are facing. There are many other existential risks. For instance, the sun expanding beyond this earth at some point, because it burns out and as not being able to go to the last solar system that has not burned out yet, or being hit by a meteorite or global warming, and so on. And one of those things is going to get us at some point, the question is, if AI gets us first, I think there is a small chance that AI gets us before global warming. But it's not clear that it manages to do this. And the ultimate existential risk is that we run out of negentropy, at which point we will stop, there will be no way at some point to sustain intelligent life in this universe. So over the life of the course of the universe, that is very dark place. And there's basically a very, very brief flash on one or some planetary surfaces, where there is a reflection of the universe in some mines that have an inkling of what's going on. And that's it. It's a very brief flash, this moment of consciousness and understanding. And if you build AI or not, it's not going to change very much about this. But back to this original thing. I do think that the existing is sufficient for any labor based economy, we need to solve this. I agree with Marcus Hutter, that actually we can solve it. And you will probably run into big trouble while doing this. To blow things up and of people who die and will be horrible, but eventually it can be good. And

Joscha Bach 27:52
future AI may or may not pose an existential risks, like many others, like nuclear war, like global warming, like others, and we should address it. And I think it's very reasonable to treat them as separate problems, and throw a lot of money on also dealing with this problem. Maybe not as all the money, certainly not all the money, but we should put some brainpower on it, I don't see an obvious solution to the problem of dealing with AI risks. And I have tried to explain why I'm less optimistic than Marcus and many others. I think that some of the optimism is also probably because people think it helps them to find a solution at all. I'm not sure if a solution exists. But I think it's possible to increase the likelihood to find solutions or to increase the likelihood of good outcomes and have more people having interesting lives of my children having interesting experiences and diverse. And let's try to optimize the transit for that. Thanks.

Unknown 28:50
Yeah, we can feel a couple of questions with Kosha. Now. Well, Ben, tries to load the presentation onto your laptop. So who has any questions about negativity? Yep. Okay. First one.

Unknown 29:12
Can I ask the question? Yep.

Unknown 29:16
I don't answer your question. Oh.

Unknown 29:23
Sorry. Well, in all the talks here, I think the the one idea is that once we have artificial general intelligence or artificial superintelligence, we have an ethical problem and ethical goal function and how should such entity behave or whether it merges with humans or not. But I think for autonomous cars, we have to solve the ethical problem even before we have artificial general intelligence artificial superintelligence because we have to decide as a society, what the ethical choices have to be for autonomous cars, not because they are super intelligent, but simply because they are faster than humans and can drive more safely. So, the idea, I think, is that first we have to solve this ethical goal function definition problem. And that will help also later on to solve the artificial general and artificial superintelligence. Yeah, so what's your question? So the question is actually, why are we not paying attention more to this ethical problem? Because I think it's very much focused on okay, we do not know, what actually official genuine superintendents will look like. And then so we cannot also solve the ethical problem on that. But I think there's a very

Unknown 30:42
basic question, like, let's try and make them questions and not why, why is treaties and

Unknown 30:47
attention to this more attention to solving the ethical problems for autonomous cars?

Joscha Bach 30:53
Well, ethics is something that is very hard to derive from first principles. And ethics emerge as a problem if you want to have a principled approach to resolve conflicts between multiple autonomous agents with immutable minds, immutable preferences. So for instance, I have a preference that I don't want to experience pain. This preference is almost immutable, it's very hard to remove. It's possible this meditation or drugs or surgery, but it's usually we see this as immutable, right. And as long as you have these multiple agents that you think are valuable, and you want to care for them and account for their preferences, you have to find a principled way to resolve conflicts, or resources or conflicting preferences among them. And this is what ethics is about, ultimately. And you can do this with respect to cars here. The question is probably what's the value of human agency versus what's the value of human life? Do you think that human life Trump's human agency, and that case, have self driving cars right now because of driving cars already? Right? No, make fewer mistakes and people? That's a very obvious question. In the general case, there's, well there's all these other things, philosophers, trolley problems for a long time, it was completely academic. Philosophers don't have real jobs because they don't have formal education. They need something to do. Now suddenly, they're self driving cars, they look somewhat like trolleys, oh, my god, papers. Anybody, also something that journalists can use to write articles about because it's somewhat interesting and geeky and something that people can do something about it, but they don't think it's a problem in practice, and practice, you want to build a car that is functioning within certain parameters, and you want to have a legal infrastructure that shields the makers of the car from ruin, if they make a minor mistake. Okay, so that's the major very big ethical problems with self driving cars right now. Yeah, minor problems. I don't think that they're a major problem for self driving cars right now about ethics.

Unknown 32:57
Thank you. We can ask more questions.

This transcript was generated by https://otter.ai