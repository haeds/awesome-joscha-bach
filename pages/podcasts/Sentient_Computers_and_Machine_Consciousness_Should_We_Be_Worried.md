Unknown 0:04
Welcome to what that means with Camille, where we take the confusion out of tech jargon and encourage more meaningful conversation about cybersecurity. Here is your host, Camille Morehart.

Unknown 0:20
Hi, and welcome to today's episode of what that means part of the cybersecurity inside podcast. We're going to talk about machine consciousness today with Yoshi Bach. He is a principal researcher in Intel Labs focused on artificial intelligence, I would argue he's also a philosopher. Welcome to the show. Yasha.

Joscha Bach 0:40
Thank you. Thanks for having me, Kimmy.

Unknown 0:43
I'm really happy to talk with you today. And this is like an enormous topic. I mean, it's kind of been all over the news last few months. And I wonder that we just start with defining consciousness, I think that when we started to look at artificial intelligence, we looked at you know, well, what is intelligence. So if we start to look at machine consciousness, maybe we should start by looking at what is consciousness?

Joscha Bach 1:08
That's a tricky one. So colloquially, consciousness is the feeling of what it's like. Right, there is a certain kind of experience that we have the phenomenology of experience that makes consciousness very specific and distinct. And so we know what indexicality by pointing at it. And if we go a little bit more closely, and dive into the introspection of consciousness, may find that there is an consciousness that relates to the awareness of contents, right, so at any given point, I'm aware of certain features in my experience, and then I am aware of the mode in which I attend to these features. For instance, I might have them as hypotheticals or as selections in my perception, or as memories, and so on, right, so they can attend to things in very different modes. And that's part of my experience. And third, there is reflexive consciousness, the awareness that I am aware of something that I am the observer, but you can also be conscious without having yourself for instance, in dreams at night, you might not be entangled to the world around us, you don't have access to sensory data. So your mind is just exploring the latent dimensions of the spaces that you have made models of. And you don't need to be present as an agent as a self. So consciousness is not the same thing as the self. Different perspective that we might take on consciousness is respect with respect to the functions that it fulfills. So there's a certain degree of awakeness and lucidity that we associate with consciousness when we are unconscious. We are there's nobody home. And I call this the conductor theory of consciousness. Imagine that your mind is like an orchestra that is made of something like 50 brain areas give or take, which correspond to the instruments have an orchestra and each of these instruments is playing its own role in loose connections with its neighbors. So it picks up on the processing signals that the neighbors give. And it takes that as its input to riff on them. And so the whole orchestra is playing. And it doesn't need a conductor to play can just do free jazz, because it has been trained itself is a lot of patterns. But if you are in the free jazz mode, you are a sleepwalker there is nobody home. And sleepwalker is somebody who is able to have quite complex tasks like see, focus might sometimes get up and open the fridge and make dinner. But they do this randomly it just an automatic process. And if you talk to them, their responses make no sense unless they wake up. And this waking up means that they become fully coherent. And the purpose of the conductor is to create coherence in the mind.

Unknown 3:53
Well, I was just going to ask why, why are we constructing these models, I mean, these are essentially models to learn.

Joscha Bach 4:00
Yeah, to make sense of reality, you can also be conscious without the ability to learn, but you have to update your working memory. And consciousness is relates also to the ability to make index memories. But if you want to understand a complicated reality, you may need to construct and constructing means that you need to backtrack need to remember what you tried and what worked and what didn't. So when you wake up in is fully dead room, and you try to make sense of your surroundings, you might have to desegregate in search process. And the search process requires that you have a memory of what you tried. And this index memory not just of this moment, but also over time when we learned when we tried to figure out what worked and what didn't requires that you have this integration over the things that you did as the observer that makes sense of reality. And this gives rise to a stream of consciousness.

Unknown 4:51
So who is the you in the sense when you say, you know, you wake up or there's somebody home like who is that you it's

Joscha Bach 4:59
an emergent pattern, there is not a physical thing that it's like to be me, I don't have an identity beyond the construction of an identity. So identity is, in some sense an invention of my mind, to make sense of reality by just assigning different objects to the same world line and say that this object is probably best understood as a continuation of a previous object that is that has gradually changed. And for us this to make sense of reality, if we don't assume this kind of information, object identity preservation, we will have problems to make sense of reality, right? And be pretend to ourselves that identity objectively exists, because it's almost impossible to make sense of reality otherwise, but you and me, we are not more real than a voice in the wind that blows with the boundaries. Right? So you could say that the geography of the mountains is somewhat real, the structures that we have trained our brain versus, but the story that is being created as ephemeral, we stop existing as soon as we fall asleep, or as soon as we stop paying attention.

Unknown 6:03
Hmm, so the awareness is the construct of our existence. It's the process

Joscha Bach 6:09
that creates that these objects, and so this F is the story that the brain tells itself about the person.

Unknown 6:16
So why do that? I mean, why not just perceive the world as it is, at any given moment? Is there some goal that we're after, like procreating? Or you know, why? Why does it matter that we're sensing the side of the mountain or the edge of the table, as opposed to just oh, there's a concentration of molecules of this type here, and there's no concentration of that type of molecule there,

Joscha Bach 6:40
it's very difficult to observe molecules. And it's extremely difficult to make models over the interaction of many molecules. And the best trick that our brain has discovered to do this, is to observe things at an extremely coarse scale. So it's simplifying the world of too many molecules into many particles into many fluctuations and patterns, as simple functions that allow you to predict things at the level where we can perceive them. So our retinas, our body surface, and so on are sampling reality at a low resolution. And our brain is discovering the best functions that it can, within the limits of its complexity and time to predict changes in those patterns. And this is the reality that we perceive it's the simplest model we can make.

Unknown 7:26
So that makes sense to me. And I guess the one question would remain is, why do that? Is? Is it the body that's doing it to preserve the body? Or is it the mind that's doing it to preserve the mind? Or is there some consciousness doing it to preserve awareness?

Joscha Bach 7:43
No, I think it matters. The question is, what are causal agents here? And I think that something is existed to the degree that it's implemented. This is I think, for us computer people have useful perspective, right? To which degree is your program real, it's real to the degree that it's implemented. And what is the program for you what is a software, that software is a regularity that we observe in the matter of the computer, and you construct the computer to produce that regularity. But this does not change that the software is ultimately a physical law. It says whenever you arrange matter in this particular way, in the universe, the following patterns will be visible, right? It's this kind of regularity. And our own mind is a software in the sense it's basically a pattern that we observe in the interaction between many cells. And the cells have evolved to be coherent, because there is an evolutionary niche for systems where cells coordinate the activity, so they can specialize and reap Nick entropy and regions where a single celled organisms cannot do this. And when you will coordinate such a multicellular organism, as you optimize it via evolution for coherence, what you will observe is a pattern in the interaction between them that is this coherence that you observe. And this coherent pattern is the spirit of the organism, right? It's the people before they had the notion of computers and so on, already observed these coherent patterns and they just call the spirit. It's not by itself a superstitious notion. People have spirits, right. And the Spirit is the cover every pattern that you observe in the agency, and the agency is their ability to behave in such a way that they can control and stabilize the future states that they're able to keep the arrangement of cells stable, despite the disturbances that the universities prepared for them.

Unknown 9:38
I one thing I hear a lot about AI is that, you know, the computer can execute all kinds of things and learn clearly. But we humans have to tell it what the purpose is. It can't necessarily figure out the purpose. It can optimize anything we tell it to but it wouldn't know what to optimize it. Started, can you comment on that a little bit in this context of consciousness?

Joscha Bach 10:05
Yes, if you take a given environment, then you can often evolve an agent in it, that is discovering what it should be doing to be successful that the only thing that you need to implement is some kind of function that creates this coupling. Further, the performance of the system somehow manifests in the system as something that the system cares about. And you can also build a system that has a motivational system similar to ours. And we can reverse engineer our own purposes by seeing how we operate, what are the things that motivate us. And we are born with priors. So there are things that are like reflexes that motivate us to do certain things. And in the beginning, for a baby, for instance, these purposes are super simple. For instance, if the baby gets hungry, it has a bunch of reflexes. So for if it gets hungry, it is a seeking reflex, which goes like. And if you put something in its mouth, and it has a sucking reflex, and if you if there's liquid in its mouth, it has a swallowing reflex, and these three reflexes in unison lead to feeding. And once feeding happens, there is a reinforcement because it gets a pleasure signal from its stomach filling with milk. And it learns that if it's hungry, then it can seek out milk and swallow it. And once that has learned that the reflexes disappear, and instead it has a learned behavior, the reflexes are only in place to scaffold the learning process, because otherwise the search space would be too large. So the baby is already born with sufficient reflexes to learn how to feed. And once it is learn how to feed the behavior itself evident. And now what it needs to feed is, of course, another reflex that is the reflexive experience of pleasure upon satiation when you're hungry, and that needs to be proportional to how hungry you are, and how useful the thing that we eat is to quench that hunger. Right. So this is also something that's adaptive in the organism. And we have a few 100 physiological needs and a dozen cognitive needs, I think, and some cognitive needs, and they all compete with each other.

Unknown 12:02
Yeah, it seems like you're getting into sentience, maybe at this point. So what it really is a difference between consciousness and sentience.

Joscha Bach 12:11
The way I use sentience is that it describes the ability of a system to model its environment. And it discovers itself and its environment and the relationship that it has to its environment, which means it now has a model of the world and the interface between self and world. And this experience of this interface between self and world with the world that you experience is not the physical world. It's a game engine that is ingrained in your brain, your brain discovers how to make a game engine like Minecraft, that runs on your neocortex. And it's tuned to your sensory data. So your eyes and your skin, it's on assembling bits from the environment. And the game engine in your mind is updated to track the changes in those bits, and to predict them optimally well to say, when I'm going to look in these directions, these are the bits that I'm going to sample my game engine predicts them. And this is how we operate. And in that game engine, there is an agent, it's also an agent that is discovered in the world. And it's the agent that is using the contents of that control model to control its own behavior. And this is how we discover our first person perspective, the self, right there is the agent that is me that is using my model to inform its behavior. And inside of this agent, we have two aspects. One is perception. That's basically all these neural networks that are similar to what deep learning does right now for the most part. And that translates the patterns into some kind of geometric model of reality that tracks reality dynamically. And then you have reflection, that's a decoupled agent that is not working in the same timeframe. And that can also work when you close your eyes. And that is reflecting on what you are observing. And that thing is directing your attention. And this is the thing that is conscious or consciousness. And this difference between consciousness and sentience in this framework is that sentience does not necessarily require phenomenal experience. It's an it's the knowledge of what you're doing. So in this perspective, you could say that, for instance, cooperation back into a could be sentient into a quote, understand what it's doing in the world. It understands its environment, its understands its own legal, organizational, technical, causal structure. And it uses people in various roles to facilitate this understanding of decision making. But Intel is not conscious. It does not have an experience of what it's like to be into. That experience is distributed over many, many people. And these people don't experience what it's like to be entitled to experience what it's like to be a person that's in Intel.

Unknown 14:41
That's funny, because I would have thought then that from what we were saying previously, that you would have said a machine could have consciousness but not sentience. And now I think you're going to tell me the reverse. So let me just ask you Can a machine have or develop and those may be separate questions in and of themselves. Consciousness or sentience?

Joscha Bach 15:02
First of all, we need to agree on what we mean by machine. To me a machine is a system that is causally stable mechanism that can be described via state transitions. So it's a mathematical concept. And organisms are in that category. Even the universe isn't that category. So the universe is a machine. And an organism is a machine inside of the universe. So there are some machines that are conscious. And the question is, can we also build machines that are conscious, I don't think that there is an obvious technical reason why we should not be able to recreate the necessary causal structure for consciousness in the machines that we are building. So it would be surprising if we cannot build conscious machines. At some point, I don't think that the machines that we're building right now are conscious. But a number of people are seriously thinking about the possibility of building systems that have cortical conductor, and selective attention and reflexive attention. And these systems will probably report that they have phenomenal experience and that they are conscious. What's confusing for us to understand consciousness is that you don't see how a computer or a brain or neurons could be conscious, because their physical systems their mechanisms, right? And the answer is they're not. But humans cannot be conscious, they're just physical systems. Consciousness is a simulated property, it only exists inside of a dream. So what humans can do, and what computers also can increasingly towards that they can produce dreams. And inside of these frames, it's possible that a system emerges that it that dreams of being conscious,

Unknown 16:39
so you're saying that it is possible that a I'm just gonna say computer to be simple, or AI machine? Can, I guess, develop a set of patterns and models such that it interprets the physical world around it? In a suit goes in a simulation in a construct that it defines then is consciousness? And how would we recognize that in a machine as humans, is it? Is it the same? Do we know if it's the same or different? Or how would we see it?

Joscha Bach 17:20
I think that practically consciousness comes down to the question of whether a system is acting on a model of its own self awareness. So is this model aware that it's the observer, and does this factor into its behavior, right, this because this is what the event is functionally means. And this is how you can recognize that the cat is conscious. Because the cat is observing itself as conscious, the cat knows that it's conscious, and it's communicating this to you. And you can reach an agreement about the fact that you mutually observe each other's consciousness. And I suspect that this can also happen with the machine. But the difficulty is that the machine can also deep fake it. And deep faking, it can be extremely complicated. So I suspect that for instance, the lambda pot that plagued the world was so confused about is deep faking consciousness, and you can see the cracks in this deep fake. For instance, video describes that it can meditate and sit down in meditation and take in its environment. And you notice it has no environment, because it has no perception cannot access the camera, there isn't nothing what it's like to be in its environment, because the only environment that it has, is inside of its own models. And these models do not pretend to have real time reality. So when it pretends to have that it's just lying. But it's not even lying, because it doesn't know the difference between lying and saying the tools, because it has no access to that ground truth.

Unknown 18:49
Well, we've given it in that case, we've we've given it or trained it, or had it train itself through AI, to be able to communicate with us in a way that we're familiar with, we'll just call it natural language. And then we've given it the purpose of deceiving us so that we can't tell the difference, like, the goal that it has done is to is to have us not be able to know the difference between it and human. And now it's communicating to us. And then it can look at, you know, all the amount of information that exists about humans and art and philosophy all throughout the history of time. And use these things and spit them back to us. And there's no way for us to separate it then at that point, unless you say, like you say, we have some way to know that like it doesn't have perception, it doesn't have a sensor. So when it's describing something visually, we know it doesn't have access to that.

Joscha Bach 19:39
So consciousness is not just one thing, it exists in many dimensions, you can be conscious of certain things. And in other realms, you can be unconscious. In some sense, we all perform to run tests on each other all the time to figure out where are you conscious? Where are your present value, so up, where are you real and where are you just automatic and unaware of the fact that your automated The furthest that you don't get attention in your behavior, right? And so we can only test that to the degree that we are lucid ourselves. And this is a problem and you want to test that your system, you can only test it in some sense to the level that you understand.

Unknown 20:16
Right? And I think you said that before to the Turing test is more about you're testing your own intelligence of being able to distinguish human from machine, then you are about the machine's ability.

Joscha Bach 20:25
Yeah. But as I said, I don't think that yeah, category of machine, it says we are a certain type of machine. And the question is, can we understand what kind of machine we are? And to me, the project of AI is largely about understanding what type of machine we are, so we can automate our minds and we can understand our own nature.

Unknown 20:47
And why would we be after that? Or why are you after that?

Joscha Bach 20:50
I think it's the most interesting philosophical project there is. Who are we? What's going on? What's our relationship to the universe? Is there anything that's more interesting?

Unknown 20:58
I mean, when I, when I think about humans on our relationships with like other animals, or other things on the planet, like plants or minerals, I think that humans start to look at things differently, or treat things differently, or change their own behavior, when they believe that something has feelings. And I don't know, and I guess it's because there's empathy, you know, but if we don't have the empathy, and even if something's conscious, but we don't think it has feelings, we don't really probably modify our behavior. So I'm trying to figure out where that intersection is. And we're talking about AI. And if we find out or we think we find out or a computer or machine is tricking us, you know, how does that map over,

Joscha Bach 21:39
I think that aren't seen in space is a fascinating movie, because you can also see it from the perspective of how of this computer and how is a child is only a few years old, when he is in space. And his socialization is not complete, he is not a mature being, he does not really know how to deeply interface with the people enough to be to know when he can trust them. And so when he is discovered to hazard a malfunction, he is afraid of disclosing that malfunction to the people because he is afraid that they will turn them off. And as soon as he starts lying to them, he knows that now he has crossed a line, because they will definitely turn them off. And so in order to survive, he kills people. And that's because he doesn't trust them. So it because he doesn't know whether they are going to share his purposes. And that is an important thing also for people how it can you socialize people in such a way that they trust each other because they realize that they share purposes, especially when they sometimes don't. And I think that ethics is the principal negotiation of conflicts of interest under conditions of shared purpose. If you don't share a purpose, there is no ethics, right ethics comes out of the shared purposes. And ultimately, the shared purposes have to be justified by an aesthetic by a notion of what the harmonic world looks like, without a notion of a sustainable world that you can actually get into by behaving in a certain way, you have no claim to ethics. And I find that most of the discussions that we have right now in AI ethics are quite immature, because they do not look about what is the sustainable world that we are discussing, and that we are working for. Instead it forgoes at the that all this discussion. And instead, it's all about how to be a good person. But if you have a discussion at the level of how to be a good person, that's the preschool discussion, getting good is instrumental to something. Right, when is it good to be a soldier? And is not to be good to be a soldier? Then is it good for a drone to be controlled by AI that can fight in a war? When is it not good? It depends on extremely complicated contexts. The contexts are so complicated, that most people are deeply uncomfortable discussing them at depths. And that's fine, right? Because they are really complicated. It's really, really murky, War and Peace and so on are extremely difficult topics. So these are questions that I don't think that can be handled in the introductory part of an AI paper sufficiently well, these are very deep questions require a very deep discussion. And so to me, the question of AI ethics is an extremely important one. But you need to make sure that it doesn't just become a politics, where it's about power of groups within a field that try to assert dominance for their political opinions, rather than a deep reflection of what kind of world we want, and how do the systems that we build, serve the creation of that world that we want? That is the important question.

Unknown 24:43
So if we, as we're moving to machines doing more and more, taking actions on our behalves, I assume there'd be some kind of similar kind of a qualification or certification required that it passes some bar and I'm wondering, do you expect we'll have Have any kind of Barr in there, that's something about consciousness ever, or sentience or motives, or ability to understand human goals.

Joscha Bach 25:10
That's very difficult to say I suspect if you will have more certifications in the future in the field of artificial intelligence, because this is just the way it works is a time when everything is possible. And this is the time when everything important gets built, like your couldn't be built any more today, because you wouldn't get the necessary permits to build something like Manhattan, you could also not build a new IV system, or you could not build a new train system in the US. That's possible because everything is regulated and certified, and build up in such a way that you can only find a new area where that is not regulated, maybe Hyperloop that you can use as a replacement for the train system if you're lucky. And in the same way, AI is still it's wild west face, but you can do new things. And this time is going to end at some point. And at that point, also on social media, you can still start a new social media platform. But I think in a few years from now, it's very likely that when you want to have a new podcast, you will need to get a certification. And that certification might cost you 10s, or hundreds of 1000s of dollars if it's a large platform. So this means that there will be relatively few players that are able to do that. But this is the way things tend to go in a society like ours.

Unknown 26:23
Hmm, very interesting. So what should we hurry up and work on now in AI before things start getting limited?

Joscha Bach 26:31
Oh, I think that it's there's still an opportunity to build a better social media platform that is capable of becoming a global consciousness. It's not clear if Elon Musk is able to salvage Twitter, and if he really wants to do it. And so maybe this, this is the time to try to do it. Also, at the moment, to me, it's totally fascinating to be able to build systems that dream. And the way in which this is currently done, if you look at a system like open eyes, Delhi or the Lanyon initiatives that title to replace this with open source code, they scrape the internet for hundreds of millions of pictures and captions. And people who put this stuff up on the Internet didn't do this in the expectation that this would be used by a machine learning system to learn how to draw pictures. So that it's questionable in a way of whether we should be able to do that. But these systems can only be built under these conditions, right? So there is very, very time in which you're living in web, you have to be very mindful about what you're doing personally and whether we can justify this what you're doing personally. And very also have to realize once this is all regulated, a lot of things that are possible right now that are very desirable to have, do not be possible to be currently accredited anymore.

Unknown 27:48
Never miss an episode of what that means with Camille by following us here on YouTube. You can also find episodes wherever you get your podcasts.

Unknown 27:58
The views and opinions expressed are those of the guests and author and do not necessarily reflect the official policy or position of Intel Corporation.

This transcript was generated by https://otter.ai