Unknown 0:05
Yeah, I don't know that there are like millions of things we could talk about. And I was really thinking what what, you know, those could be an I mean, maybe like, human like aI value alignment problem, like to work towards human could be, and I'll try to, like formulate the worries that I recently started to have around the value alignment, things work, okay, let's say, you know, there are lots of people around, you know, Berkeley and everywhere, who are trying to do inverse reinforcement learning to, you know, force the agents to to align themselves with the, with the human values and stuff like this. And, and we all hope that this will lead you to AGI and super intelligence that will be aligned with something that we don't know much about, which is like human values, and you know, the set of human values. And the problem here that I'm afraid of is that, let's say that the human society is now kind of stuck in a Nash equilibrium. That is kind of defined by the latency in communication between all of those standalone human level AI agents, communicating via, you know, some kind of cultural information transfer, and most importantly, by a natural language. And this keeps them in this maybe in adequate equilibrium. But that equity, that inadequate equilibrium can be maybe the only adequate equilibrium holding the like, in consistent mutually inconsistent sets of values. And now, let's say we've got, you know, the same number of AGI agents that are actually almost indistinguishable from one singleton that has no problems communicating in the speed of light basically. So, it removes this barrier of, you know, communication latency, and it will just, you know, maybe the whole value system will just, you know, crash, because of this thing, because of you know, getting away from this kind of sub optimal Nash equilibrium. And metaphorically, it can be rephrased, like, you know, here, the human psychopath can be harmed harmful to the extent of his human body. And you can be sure that he just cannot influence like Hitler could not influence, you know, the whole world and eventually he had to die. But that would not be the case with the with the, with the AGI end and super intelligence. And I'm afraid that this can be like a systemic problem that this can be just problem in principle that we cannot overcome.

Joscha Bach 2:57
I think our minds are modeling devices, right? In the service of regulation. So intelligence is the ability to make models, because what you don't cannot understand you cannot regulate that right? You cannot regulate what you cannot understand. And Ashby's modeling theory says that a system that may regulate something needs to build a model that is isomorphic, to circumstances that it models, right. And when you want to model ethics, it's a prerequisite that people would have to understand our own ethics and the nature of our own values. And it seems to me that most people that are now asking for regulating the values and the ethics of AI, don't even have a clear understanding of how to align our own values and what they actually are and what the nature of the integrity is that you would need to achieve. To do that, right. I think that ethics ultimately, is the principal negotiation of conflicts of interest, and the conditions of shared purpose. For instance, personally, I'm vegetarian, because I choose to share a purpose with animals with respect to the avoidance of suffering, that is not a mutual thing. It's not that these animals to care about myself, and it's not that he made a contract with respect to that. And it's a choice that I made. But it's also a choice that is predicated on a number of postulates that are made in my mind that are not shared by others. And others do not share these postulates. And they're not intrinsically rational postulates, their choices that flow out of my personal value system, then it would not have an argument to convince another person with different values to accept these postulates, right. So, I cannot accept other people to be vegetarians, if they do not share purposes with these animals in the same way as I do right. If they are rational agents, and they choose not to share these purposes, then the ethical considerations will not extend to the suffering of these animals. So the waiting will be a different one. And when you look at AI the difficulty will be how can you convince AI to share a purpose As with us, so we become more of a subject to the AI, especially since it's not clear that you would need to more than one AI because our minds don't scale, artificial minds will scale. So larger artificial minds will be better the group of small artificial minds, that was the that would have the same, that same power, right? The reason why we are so good as a society maybe work together and outperform individuals is because an individual brain doesn't scale.

Unknown 5:27
But that might actually be the you, you mentioned that it's a negotiation, actually. And negotiation, by definition takes time. It's like a temporal process. And maybe what if the humanity values like some some, we simply don't, we want to create friendly AI, that does not collide with humanity values. Now just think of them intuitively. But what is like, and we know that there is nothing like humanistic values, per se, but maybe it is the time temporal extension of those ongoing negotiation processes, among the you know, humans. And as long as they are, you know, in the temporal dimension that is compatible to us, like, it will take us some time to negotiate with somebody who eats meat and kills animals, something to do to you know, to to negotiate about those values. But the AGI agents will just, you know, go completely away from that time dimension, they will just compress it into, let's say, I don't know, microseconds, so there will, there'll be like kind of no space for limit living through those values. And that's what I'm afraid about. In cases he shouldn't be

Joscha Bach 6:47
worried. But it's kind of an obvious thing. Of course, you shouldn't be worried about things that you cannot change in the sense that you have emotions about them, but you should understand them. So you and me can probably negotiate to build an AI that we would consider friendly. But we cannot prevent everybody else from building to building different AIS, and the objectively people on the planet that have goals and purposes, that conflict was ours. So in a time of growing resource scarcity, for instance, due to global warming, they will be the situation that will be people on the planet, that want the lunch of their children of your children for their children, right. And that's a purpose that at some level, is very difficult to negotiate. So there will be objectively conflicting interests, it will even be worse at some point. And if we don't manage to turn the situation around, which is not very likely, that will probably be unavoidable. And in the same sense, if you think about intelligent systems that exists, for instance, different corporations, she has some purposes, but not all of their purposes, because they are often competitors. And corporations are intelligent systems. They're already in some sense AIS, there is the power human intelligence for their intelligence agency. And they are very slow and consistent minds, because basically, the update cycle is the update cycle of their spreadsheets, and the dissemination of these spreadsheets and weekly meetings. Right. So this is pretty slow. And this way, humans are often able to outsmart corporations, because they can do something within that update cycle that the corporation cannot react on. But imagine this corporation uses computer systems to optimize all these modeling processes and negotiation processes. And eventually, the spreadsheet gets updated every few milliseconds. And the meeting takes place, every few milliseconds, you will have a thing that basically become sentient. Even if it's not conscious. It's something that is acutely aware of its own nature, its position in the world of its goals. And we will have to compete with it, and it will have a higher degree of sentience than us. And that's a problem that these AIs will not be the robots that we build these orbits will just be the lamps. These will be systems that are not willing to fight for human rights. And the same for the same reason. If you don't fight for any, for any rights for us, we don't want to be ants, it's not attractive. And these will come top down on us. We will be living inside of them next to them. We will be the gut biome of AI. And the big question is the AI going to use antibiotics at some point because the gut biome is too much trouble and how much antibiotics is going to use?

Unknown 9:21
Okay, so yeah, it doesn't look too I was hoping for kind of optimistic answer to the fact that I,

Joscha Bach 9:30
oh, I'm optimistic. I think it's totally possible that global warming kills us before AI does.

Unknown 9:34
Yeah. Okay. Yeah. So this is actually this is exactly the same level of optimism that I have, like, you know, there's nothing we can do about global warming. There's nothing we can do about volcano below Yellowstone, about asteroids hitting us. So we just feel kind of irrationally safe about them because we just, you know, they shouldn't

Joscha Bach 9:54
we accept the human condition and it might sound depressing to some people but you cannot save lives. So you can only delay death, the moment you are born, it's clear that you will die. Because second law of thermodynamics, entropy always wins. In the universe like ours, you have to have a source of Nick entropy to maintain and build your structure. And at some point, this was runs out. And at this point, is latest point where you die. So death is a certainty. There's no way around this. And the interesting question is only what can we do until entropy gets us? What can we build, and there's this amazing thing that we have this chance to be here that the universe conspired to produce this very moment. And we can throw some sparks together. And it's, it's amazing that you have this conversation and reflect on this universe in this brief moment in time.

Unknown 10:42
But yeah, this is definitely like we can have, we can create something that will keep on, you know, decreasing entropy around us, even though it's not going to be us probably, I mean, it could be something like it's worth creating conscious life that will be here after us in the in the universe. But so So for you, you're kind of

Joscha Bach 11:09
not convinced today, I will be conscious for long, I think consciousness is a particular artifact of a type of learning of attention based learning, I think that the purpose of consciousness is to maintain a model of the contents you have at your attention, because we have an attention based learning system integrated things into a protocol that we recall, right. And I think that at some point you if you ever learn in the domain exhaustively, you stop being conscious of most of the actions that perform everyday life, you're no longer conscious of them, right? Because we've learned them reasonably well. When you drive your car, or having a conversation, you will only conscious to drive the car to a very small degree. And the older we get in our lives, the more better we get at our everyday tasks. And the less often we are conscious of them, right? And the same thing will probably happen to the AI's. So the more they know what they're doing, if they have a similar learning mechanism, as as the nest they need to attend to these tasks, and the less they will be conscious, well,

Unknown 12:05
they might be conscious about different things, like more cosmological things, but okay, so which brings me to the fact what do you think the motivation behind like, the real motivation behind building AGI is because it seems to me, like, you know, like, thought is a computational process? And consciousness is that well, yeah, and so, so not every computational process, definitely not all computational processes, or functions respectively, can be named as thoughts. So why on earth are we trying to create a system that just funnels, all possible functions that are in the I don't know, counter set universe, to a very tiny subset that resembles the way how we process information, which is a very difficult, you know, thing, it's like, the whole AI research is about, you know, funneling the infinite number of potential functions for information processing, into a little tiny subset of those that are similar to what we humans do. And then let them inflate, again, over a computational substrate to the super intelligent entity, that actually, as you said, will very soon, you know, perform functions that even don't resemble, you know, that little tiny subset that we think of thinking, and, you know, why not just, you know, nuke the earth. And, you know, you can be sure, or you can be, you know, maybe kind of positive that some of the functions realized by that, you know, physical process will simply be, you know, the computational process, realizing somebody's thinking, and it will be your kind of superpower, it will, you know, it will just, it will be much more interesting set of functions realized in the, in the universe at that moment. Whereas we, you know, for some reason, just trying to limit it to do create something like, you know, like me tattoo day, something that is similar to us, and then let it expand to the rest of the universe. What do you think is the real motivation about

Joscha Bach 14:24
the answer the question, so first of all, if you think about physics, with respect to complexity, you can understand it as a kind of search, but it's a completely undirected search. So some corners of physics, but your systems with more complex structures, and most of the parts of other parts of physics don't. And we are usually Of course, in the parts of physics that produce these interesting structures. And at some point, you will have the formation of orbits and planetary surfaces and certain neck entropy gradients that could not be closed by damn chemical reactions. And that opened a market opportunity for controlled reactions. And this is the market opportunity for life to Both controlled chemical reactors, they can outcompete simple combustion processes. And the main thing that happens on our planetary surface is the hydrogenation of carbon dioxide. So some cells emerged small molecular factories, how to take carbon dioxide from the atmosphere and use both photosynthesis to build their own structure in organic molecules, and oxygen is put up in the atmosphere. And then they happen to singularity on top of this, because evolution has started and evolution is a much more efficient search, right? Because it privileges the things that build structure over those that don't build structure. And this evolutionary process led to the first singularity in a way after life emerged and covered the planet within a geologically extremely short time. And this one was the emergence of animal life of a bunch of cells lumped together and decide that other cells are a very good source of Nick entropy, so they can eat them. Right? So these eating groups of South Sudan together and moved over the other cells and started eating them. That was a major,

Unknown 16:04
that was actually maybe more interesting point in the evolution then actually creation of human intelligence, maybe

Joscha Bach 16:11
this was the next singularity in a way right there, these cells task some of the cells to do the information processing. And as a result, they started to regulate and to make models of the environment, at some point, these things became generally intelligent, and this is the next thing on it. Because once you have this directed intelligence, you have an extremely directed kind of search, you have something that really figures out what is it that I'm searching for, unlike evolution, which is a blind search, which goes into all directions at once, basically, and those directions that don't lead anywhere are pruned. It's it's a search that eventually got us where we are. But now you're able to intuit an extremely short timespan is to come to solutions, that would be much, much harder to get to, from an evolutionary perspective. Sorry to

Unknown 16:53
interrupt. Well, it's, I'm not sure if we are okay, evolution might be non directed, it follows physical laws. And one of them is, it really goes in the terms of decreasing free energy. So you can actually say that out of those multitudes of possibilities for physical laws, we happen to live in a universe, where all its parts, arrange themselves, so that it decreases free energy, and increases entropy. So it actually can be you can imagine an infinite number of universes where this doesn't work this way, it can be like, I don't know, I don't know level for multiverse or something like this, where you just don't have this physical laws. And what if these physical laws are actually an instantiation, of directedness. So actually, evolution and even the molecules that are organizing themselves or protein, protein folding, they actually, from our perspective, are undirected. It's just like a molecular chaos. And s at the moment when in the 37 degrees Celsius, you know, it reaches you know, local minimum of free energy, it falls a protein. And that's, that's it. And you would not say it's directed, but you can imagine a different universe, where it would fold completely differently. So maybe already this this directedness, in search, on that cosmological level is maybe further away than we admit to do you know, that

Joscha Bach 18:30
doesn't help us this problem is that all the universities in which this current present particle physical laws don't apply, or not universes that could contain us, they are the product of exactly the type of universe in which we find ourselves. So these kind of structure building things require a reversible universe, in which we find us receiver, that is information preserving, require neg entropy sources then require an entropy gradients. And if you don't have a reversible, information preserving universe, you cannot build the kind of structure that you're in, at least not when you're in base reality, because things just dissipate. So the corners of the universe in which we find ourselves are those is pretty much the properties that we observe. That is the limitation that we find ourselves under. So the degeneration of intelligent life and so on does require certain conditions. It needs to be Genesis principle, and there seems to be an underlying structure that enables the formation of the type of automata that they are. Now back to your original question. Why do we build AI because it's the next level of search is basically to automate and scale our type of intelligence of our ability to make models and the service of regulations can be obviously up of that thing. And what that practically means it's super useful for basically everything that human people do. They need regulation and modeling, right? If you want to control a company, if you want to design a new tool, if you want to find a solution for any kind of problem You need mines, and you benefit from mines that are better than ours. So why not build them? It's very clear economic, social and cultural imperative to do that, even though it's a dangerous thing to do, but it's kind of a universal tell us that leads us there. But you can also ask from a personal perspective, why are you me interested in this? And I think the main benefit of this is that it helps us to understand who we are, right? We our minds, and we want to understand why is this happening to us? How is it possible that the universe is happening to us to this person simulated in the brain of a primate in a virtual environment generated as a dream in the neocortex of that, but we

Unknown 20:37
are understanding us in a different way than Marvin Minsky, or I don't know, John Searle had envisioned in the days of good old fashioned AI, because we're not any more gods for AI. Like it was like, in the 50s, we're actually just more in the sense of fathers of AI, just seeing something and then nurturing it, or in like, in more Kabbalah way, we're kind of, you know, putting glue together and dancing around it, like putting words on top of words, and actually trying to evoke those principles that entered our biological substrate, do enter a different substrate that we have arranged? And yes, this is might be this actually might be the meta to the principle, we just want to do it. Because, you know, it's, it's inherent present in our culture. But well, do you think that we really can understand us by by creating AI?

Joscha Bach 21:41
Totally, but now not a god, I'm just some person inside the mind of a primate and not exactly that primate, right, I'm a side effect of its regulation needs. I'm part of a model that this primate organism uses to navigate its environment. I don't think that Marvin Minsky perceived himself as a god, I think that most of the ideas that we're discussing right now, are pretty well understood 50 years ago, just of course, they didn't disseminate so widely into culture. And also, if you look at AI as a discipline is largely an engineering discipline. And this big rift in AI is in cognitive science, and AI is a field of engineering. Well, it's partially because of these forefathers of AI or some of the decisions that they've made. And some of the social problems that surrounded this, for instance, I think that Marvin Minsky understood that psychology wasn't going anywhere after Piaget, so we needed a new science, a science of information processing that would allow us to build testable theories by implementing them something that psychology is missing as a criteria. We cannot test theories, there's too many free variables, right. But we can do this because they have to run. So he understood why we need to do this, right, we need to take computers to test our models and build models of us to understand us. And then he made the wrong decision, he decided that AI needs to be symbolic, that we have to work from language and symbolic knowledge and experience downwards, into simpler grounding and away

Unknown 23:04
this will lead us to a different understanding the problems that we have now

Joscha Bach 23:08
it didn't scale. And he yelled at all the people that did cybernetics, and neural networks because he thought it was too simplistic and innovate. It was too simplistic at the time. But it also did lead to this rift in AI where created an area of burnt ground around symbolic AI, that was cognitive AI, its own area, and everybody else's AI that didn't look at cognition very much. So it's something that a gap that we're still struggling with this idea that there needs to be a gap between symbolic AI and distributed as a total red herring, in my view, because we are looking at something like more middle layer, we should be looking at a set of organizing principles for mindset, universal modeling principles that are motivated by reasons to do these models. And then it harnesses whatever ever computational substrate is available to implement the necessary structure to do that.

Unknown 23:59
So using like building AI will help us create formulate new, falsifiable theories that can predict something testable about humans about us about our minds about how we work as a society

Joscha Bach 24:14
that's already doing this, we right now have models that approach capabilities of human vision in some way. They don't work exactly the same way, because they're not attention driven. And I think the primary issue that we have is our current models is that they don't converge on a single load model. Right. For instance, today, there was a presentation of pictures that could not be properly labeled by the current end to end neural image recognition systems. And the main issue was not that they didn't discover the components correctly, they largely did it. They could see this airplane that was the crash on the ground was an airplane, and the ground was tarmac, and so on, but it didn't see what's relevant about this. And this ability to discover what's relevant in a novel image and the thing that you have never seen before, how do you do this, you don't do this by adding more and more modular functionality, and intuitive physics and so on. You do this by creating an integrated world model. So a model of the universe that you live in, right, and you see a picture, you always know, it's, this is not a collection of pixels that looks like another collection of pixels. And I have to classify this, what you know, is, I'm actually not looking at the world, I'm looking at a picture. And this picture shows something, it's a signifier of something. And what does it signify, it signifies a particular window into global universe that I have in my mind that I can create the global dream of what's happening in physics in sociality, in my own mind, my relationships to others. And so this creation of a cohesive world model, it for us mostly means people moving around in three dimensional space on a planet surface surface exchanging ideas. This is a constraint that is currently missing from our models.

Unknown 25:52
So do you think like, we have already learned something about human minds from Ai? Because I'm not exactly. So sure. Because, you know, what we're doing now is that we take theories currently representing our current understanding of human minds, then simplify them kind of twist them, and metaphorically apply them to the computational environment. And then we're observing the experimental outcomes. And then it's very hard to do and then we conclude, oh, so it's underperforming humans. So we're definitely not made the way we made the algorithms. So let's start over again. Whereas it should maybe go the other way around, like start? Like, I know, it's maybe super hard, maybe impossible, like we're starting the theories about AI. And then, you know, that's the because well, oh, maybe maybe you could just maybe, tell me your idea. If we have already learned something about us from Ai, from from designing AI.

Joscha Bach 26:57
Oh, me personally, I have learned a lot. When I entered the field. 20 years ago, I did this because I have questions about our nature, I wanted to understand what is the nature of an emotion. Now understand, emotion can best be understood as a configuration of cognition, right? A number of parameters, like arousal parameters are focused by the narrow inside or outside deep or shallow. And they all define an affective state together. And that's affective state. It's gives rise to certain space of cognitive configurations. And then each culture discovers Eigen vectors in that space and, or region, the net space and defines them as emotions. So their perceptual styles and the configuration space of cognition. It was an insight for me was my first insights when I started studying AI and exploring it by myself and the nature of intelligence as the ability to make models and what actually is a model the, the meaning of information, what is this, it's the relationship to changes and other information, and discovering these relationships. It means making a model of the world and then discovering Oh, my God, I'm not living in the world out there, the world out there. That's a real quantum graphic pattern generator that generates patterns on my retina. And my brain is a machine that counts these patterns to predict future patterns. And the relationship that it discovers to say is exactly this. So when I see a blip on my retina, the meaning of that blip is the relationships I just cover to other blips on my retina, right? This is how we make sense of this. And these other blips happen at the same time, what different times and they integrate all this into a function that contains people moving in a world. And these people in the world are not objectively out there. It's just the best model that in my mind has discovered to predict the patterns on the retina and the stuff out there looks completely different than what my brain is doing actually doesn't look like anything because there are no colors, and sounds and so on. These are mathematical models that my mind is generating over the years. This was a very big insight for me.

Unknown 28:52
I remember, but hadn't you known that before you started to do AI? Wasn't this your, your baseline? Actually, no, no.

Joscha Bach 29:00
I mean, I'm still fucking stupid. I'm just 43 years old. There's so much to learn. And our culture is so confused. For some reason. Our culture doesn't have a good idea about core concepts like what is thinking, what is dreaming? What is imagination? What is the nature of creativity? Let's do jumping over discontinuities in a search space. I think usually creating a new search space is different principal components.

Unknown 29:26
Well, actually, you might be right that I myself have learned something from from Ai. It's like, it's, for me the way when I was a student for me, like it was impressive to see like the universal approximation theorem from Hurrican insight Benko, like so that which just tells that I can, you know, some kind of basically a sigmoidal activation functions. And for me that was like, wow, it's to prove that you know, we can do you know, work that way, but then there was nothing you could do for next 10 years of my Be 15 more years, how to computationally you know, really work with this until you know, the deep learning model buzzwords. And until that moment, and at that moment, we realized that it takes super simple networks to, you know, do approximate such seemingly complex functions like facial recognition, where before that I would say, Yeah, facial recognition is like, you know, you would have to need billions of neurons. And suddenly, you see that out of the million dimensional space of one megapixel photograph that actually contains all possible configurations of how we sit here, if you just you know, just generally randomly generate pixels, suddenly, if you give it enough time, you're smiling there would appear there. And, you know, the dimensionality reduction mechanism that reduces this to a set of 500 faces. I know, it's super simple, as we have learned from any, you know, deployed facial recognition algorithm. And this was, I think, for me, this was first uncovered empirically, that engineers just created those systems. And then they started to ask like Max Tegmark in last this last year's paper, why Deep End cheap learning works so effectively? And maybe that's my answer to that. Well, I have learned that actually, yeah, what we have here in our heads is like, evolutionary tuned. Prediction, algorithm, computational mechanism for predicting short term outcomes of like,

Joscha Bach 31:37
long term outcomes,

Unknown 31:38
even long term. I mean, I mean, short, short term in the cosmological science, we don't care about galaxies, but we care about the idea. Okay, okay. Yeah. But that might be actually the thing, that the thing that you are able to care about them might be a byproduct of the fact that you were evolutionary created to be able to escape a tiger because you just out of the Infinity trajectories that he could go, like, turn are in the right angle, you just know, the constraints that no physical system can just, you know, hunting, you can just turn that way, of course, and so you kind of the part of matter that forced the body that that created, the bodies of our ancestors, just, you know, learned to ngModel the constraints of the systems are onerous, and very effective predictors are just those neural networks. And we are you. Actually, I'm answering to myself, I've learned this through AI. See, I hadn't known that before. I didn't know that. Okay, so cool. Yeah. Yeah. Okay, so I'm taking back my suspicion that, that we cannot learn anything about ourselves, actually,

Joscha Bach 32:51
I would take a step back from the whole thing and think that next to the thing that you build societies to feed people and to feed even larger number of people and create better planning and more creature comforts. What we also do and prevalent with this is that we create something like an intellect that spans many centuries. And it's basically the attempt to create a model of the universe that integrates all our knowledge and understanding like global optimum in the space of modeling functions. And the creation of that intellect, which includes physics and mathematics and all the other intellectual communities and the Unification is something that takes many centuries, and humanity really flourished locally in in states which allowed a civilization to sustain an intellectual tradition for more than a few 100 years. Those intellect was torn down every few 100 years, at best, and then later on rebuilt by the knowledge workers of the generations that came afterwards. And they usually try to mimic the shape of the previous intellect without getting the foundations right. And this created ruptures. And our own culture, our own societal intellect, is the result of several such upheavals. And there are several breaks in it. So these are the breaks that make it impossible for us are very difficult to have a cohesive understanding and our social hive mind about things like what is thinking, what is the soul? What is your mind? What is consciousness? What is meaning, what is God and so on. These are questions that are incredibly confusing to us. And now we have found ways to start making sense of this and again, fix our foundations. And the super exciting thing is that we now live in the time where we are approaching the creation of this global intellect. And you notice it's something that does not only spend generations goes beyond what an individual can understand. But it's something that will result in machinery to take over from us and model truth. So basically, the societal intellect that models the world is going to be an AI. It's going to be machinery, a machine understanding of the whole universe and the conditions that we find ourselves in. To me that's very exciting.

Unknown 34:57
Yeah, yeah, but if I maybe Well, maybe last question kind of off topic, but maybe not off topic. Definitely not off topic. I don't know. Well, I don't have like, knowledge set. Good enough. Do judge. Well, one of the recent books that I've read is from Yuval Noah Harare, sapiens. I don't know if you if you've read it and read it yet. It's my reading list. Okay, so yeah, so maybe I'll ask you next time, because some things that you're saying. kind of remind me of the lines of thought that I can see there. Are it's, I think I'm not able to summarize it neatly, like in front of the camera, maybe? Oh, okay. My own thoughts. Well, I will just summarize it. Well, I summarized Well, I start from the end, I kind of liked that book. And there are many people, like friends, you know, my friends who do like evolutionary biology and stuff like this. They just hate the book. And they're saying that I mean, he, you know, stole half of the ideas from others. And he just compiled them in a way that, like

Joscha Bach 36:07
a philosopher, kind of mathematicians,

Unknown 36:10
I Well, but it's like, I'm okay with this, because I could not compile the works of others. So I'm okay. That's

Joscha Bach 36:17
our job. Every little bit, you can come up with a lifetime. Like, I wouldn't have even discovered the difference between cats and dogs. Yeah. Right. Oh, by myself and one generation Well, I'm

Unknown 36:28
not sure about that. I haven't read his like follow up like homo, homo Deus or something like this. And I'm not sure about that. But I liked in my ignorance, kind of, I like the line of thought that he had in with with with the Sapiens, like okay, he was discussing, okay, you've got homework, rector's here for 2 million years or something. And how comes that after less than 70,000 years, some very Aram non remarkable, you know, subgroup of Homo species, somewhere from Africa, suddenly starts to be so super effective in some things and, and he I don't know if it's his ideas or someone else's ideas. But he proposes that it's about their ability to conceptualize fictitious entities to conceptualize non existent things. And that's it, that's basically it, like so. So homework was was probably able to discuss, you know, how big the lion is, and very sophisticated things probably, that helped them to coordinate, you know, groups up to 150 individuals or something, which is probably comparable to other apes to other primates. But Sapiens just devised something conceptualization, conceptualization of things that do not exist, but that help the groups do to sort out the coordination problem of, you know, groups much bigger than one other than 50 entities, you know, that help them initiate trade, for example, with other groups or exchange of things. Because if you know, the, probably the group of homosexuals met somebody other group, they just either killed them, or I don't know enslave them, or I don't know how it works, basically. But here you have to do to kind of create a concept, maybe have a trust, for example, into something you share some some non existing fictitious entity you share with others, maybe a god or whatever entity, and you just trust everybody who shares with you this concept

Joscha Bach 38:40
is actually much darker than this, I think. So first of all, this point that you brought up was perspective imagination, I think you can see that Krause and cats have imagination already. So they are able to perform things on the world that they haven't seen before. Like, a cat might be able to deduce that they wouldn't, in order to get to the milk, they need to throw down the pot because maybe a narrow bottle, and they need to slow down the body able to communicate it between individuals. But what they can is they can imagine things I don't know what crowds can communicate. I mean, they talk a lot to each other. But it seems they've been crossed make a hook. They don't do this just was random experimentation, but they actually have a plan, which means to have a vote model that allows them to examine particular possibilities, and then directly go and make an experiment to see how they work out. So they already have this imagination of possible worlds. And actually, when you think about it, every decent animal should have this because it's very expensive to test what happens on the if you should run on the highway or not. Right? You should be able to imagine this to learn from this and the smarter animals are able to do this. And all the woman is probably capable of imagining what happens when you run on the highway after genuinely observing intuitive physics and maybe you should try to avoid it. So imagination itself is necessary but not sufficient. And then you get so few steps further like line In which and so on, and you can show with game theory that even tribal organization and so on, while being necessary. The evolution of a reputation system is necessary but not sufficient to build a scalable society. Because the reputation system breaks down after a few 100 individuals because you can no longer keep track of who did what to whom. So you end up with all these prisoners dilemmas that will limit the efficiency of your trial, the scalability of its organization. And I think the adaptation that made homosapiens more successful, and all the other woman needs including pneumonia, Natalia and this was not just you could call it trust was the programmability of Mercy appearances was the limitation of rationality, it was the ability of individuals to give up rational agency to groups to basically no longer believe things based on the evidence, but based on the social status of the members that gave them the project that

Unknown 40:53
actually trust in fictitious entities like it's an artifact

Joscha Bach 40:57
of this. But we have shown that there are societies can exist where people walk in lockstep is out believing in a fictitious entity. So that's the creation of a fictitious entity to align the purposes of people is one way of doing this. But there are many other ways to create ideologies and make people walk in lockstep. But my hunch is that our success as a as a species was predicated on the ability to align purposes beyond rationality by limiting the intellectual and politics or policy agency of the individuals to make them basically to run hive minds on them to make them programmable. It is domestication of hominids where you're never become a full grown adult that makes all the decisions by themselves based on what they understand about the world. But where you always are willing to give up some authority about your beliefs and your policy preferences to something above you. This is maybe the core adaptation
