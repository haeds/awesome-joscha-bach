Unknown 0:00
The first paper is by Justin Bach. And it's seven principles from synthetic worlds. Okay, so my name is Joshua, I can further complicated science background initially I came from computer science and philosophy, and mostly interested in looking at architectures. When you do AGI what you're basically after is, you're trying to understand the mind as a machine, something which already, when we put a little light in and says stated that if we want to look at the mind perceptively, as an engineer, or as a natural scientist, we capture it as something that requires thoughts, experiences, and perception. And if we acknowledge this, so to speak, we can CSV can have huge memory, you can see the individual parts, however, on each other. And this is, in a way, when you're trying to try to understand mind by constructing what people have in the form theory that works. The problem very, very much saying that as to the excesses, when they tried to form theory of renderings are numerous, and see if we really worked out the explanations. However, this is not the whole story, because like mainstream stuff, didn't believe in that proposal. The whole point is that perception and what depends on it, that is conditioned thinking, the mind cannot be explained, because otherwise it wouldn't be a regime, if it could be explained by natural sciences. And this is not really conceivable that you wouldn't have some parts working upon each other, to to reinforce cognition. And language since then, just that many, many centuries ago, and it's still Saturday, literally terabytes, when these texts or animals basically says the same thing, when he reminds us that understanding and prediction can not be understood and simulated computation. It has to be something which goes beyond computation in the form of theories, and he thinks that maybe it's clammy hands because for some reason, he believes that betting against itself goes beyond what we can capture mathematically. So it's a mystery. And since consciousness is the mind, so on our Mysteries themselves, maybe Hey, it's the same thing. Dr. Shara also goes in a similar direction, when he points out that computers can only do certain tasks, they only can do symbol manipulation and understanding static which has to go intrinsically beyond symbol manipulation. So, it cannot come from mere simple manipulation effects that come from certain different meaning from the intrinsic properties, which only are inherent logic neurons. To pull the similarity the experience of the human being is not transferable into a machine, it's something which computers cannot have, therefore, computers cannot be created. There are many many more angles this seamless dairy eggs and basically we have a cultural consensus in our Western society for an OS that computers cannot and should not, maybe they cannot because they should not and there's been a lot of AI which we are witnessing this far from over there facing very strong cultural opposition. And when we are doing some general knowledge and general artificial intelligence as establishing this, we find ourselves in a similar position as somebody who does genetics and tries to build a to create a cell or organism and the funding agency is populated by creationist it's even worse okay, but is this the only problem? Of course not HDR and intelligence artificial intelligence itself has a lot of tracks that it managed to fall into it's on its own. And it's viruses that that you're you're really discussing what are what we are really trying to do, what are the paradigms for what kind of modules techniques mechanism you want to implement? We actually started with the direction artificial intelligence and then bent in many ways to make it fit from the past and current attend. And we tend to diverge from original aggression and defend multiple formulate, especially in a very good sized timer.

Unknown 4:53
And on the other hand, the fall into methodologies and that is the development method of fighting So, and then you find the intricacies of these methods to create communities, sub communities that you publish in these communities. And eventually these lessons to learn and not get unified architectures, you do not get poor pictures, but when you get individual modules, and these negative examples tend to be ungrounded, they tend to be very often symbolic, they do not confirm this, we will turn to them to many of these approaches. On the other hand, you have too many of the damn robotic approaches, you have lots and lots of interesting robots, which has a lot of value based on just the two volume processing has its own, but not with respect, maybe to get into the goal of terrible intelligence. And to read, for instance, the left to integrate motivation is the representational structures in our AI programs. And also, most of all, AI stalkers, not only from a lack of funding, but from the fiction is go hand in hand. Okay, welcome back. First of all, should aim for punishment architectures. Now, what do I mean by functionalism? What you can see here is something like the car is huge, of combustion engine. infrared image, but that's quick similarity and dissimilarity doesn't stop the colors. If we look at this motion, you see lots and lots of very, very interesting things can you see up here to the right. And we can see how they work, you can see if the same thing goes at different speeds, how this image is going to change, you find very, very strong correlations, it's very descriptive, you also find that there are certain defects in the machine, if the machine doesn't motivate you to either find certain correlations, which are very, very specific to people if there's damage. But the bad thing about this image is explanation worse, because it doesn't identify the functional parts. And what you do in computer science very often these days, especially in neuroscientists, that a steady mistake, such as the urge for an explanation of how it works. But when you run something like this, you want to have such an explanation. And of course, there's a lot of value with the FMRI image, but only in the context of a functional explanation, you need to have functional explanation to impose it upon the picture to the right, and see what these individual marks actually mean. And in order to get there, we need to have a conceptual decomposition of the whole thing that was parts and entities and how they were upon each other.

Unknown 8:06
Second, second, it could be that this question, how to achieve general intelligence define it as that otherwise, it's not going to allow us to have all these different message and then time culminated and what can be applied to different areas of AI. If you go to an AI conference, these days people have papers on description logics, technologies, on robotics, on game theory, semantic web, and many, many different areas. And it's not as if we are someday going to merge all these things into our agents, it's not going to happen. What we need is you have to define our logic according to our questions. Pretty much the same thing except words which are set and should just explain versus say, A for the big picture, not for never solutions, you need to have contextual decomposition. This is an example by Aaron Stillman. And we took as an understanding of the individual parts which we have to integrate in such a system creative processes, prejudice, environment, deliberation, we dive right into it arrangement, our persona, about what the theory of mind really is with respect to other agents, how memory works, how the setting, action are interleaved, and so on. You should also take for granted systems, that is systems which to interact with our environment. But on the other hand, we shouldn't get entangled in the philosophical simple grounding problem, there's nothing mystical, in reality, which we have to touch in order to gain intelligence. We should also remember that many many body systems which we find and put out a plan for the vast majority of them is not capable of technical diligence. On the other hand, you know, very, very intelligent beings like say Stephen Hawking was thought the quarterback was rich. For this very intelligent, sort of environment disappointing, but that doesn't necessarily mean that in touch with the robot environment that we need to have representation. So which our egg that to represent will tend to take on the topic of exploring that comes today, you do have representations to date to how we interact with the world, not just abstractly two volumes purchase. And you should not wait for the rapture or from logic embodiment in order to get there. Providing one domain is very, very costly, not just because, of course, robots to take striking down considerably. But in order to get the robot to do basic things, we are going to spend a lot of time in focus. I've done for instance, robotic soccer for a few years myself. And I've learned that if you do this robotic soccer, we have a very close environment, there's a limited number of permutations, which we encounter. And it's not as if we just arrived at the cotton field. And then this task was on the same day as test result, it's a very interesting task in its own right, because we have a discrete environment, this may not have foreseen it features and so on. But eventually, this is probably not going to end up in general intelligence. But in some kind of insight to the solution, which is very good at playing robotic soccer at TGS. There is considerably less complex to manage virtual environments I've been playing. So, the big challenge to here is to find benchmark programs for AI, you want to have benchmark problems, which need to be aI heart, which require intelligence in order to solve them. And so, they should be incrementally solve this a very difficult task is looking for benchmark problems. You should also aim for autonomous systems. Intelligence is not something which is related to problem solving, it's about finding the goals in the first place, find the problems in first place, intelligence is not the answer to some resource allocation problem, rather, it's very complex control tasks. And we are organisms in the world, which poses many different demands, and they have to satisfy these markets not specified in the moment, but rather the emergence later debate or interaction with them, I will chase us and for this right goals and motives, and this shapes, the revolutionary things, which are relevant things which are relevant and all visitations and so on. So, we need to basically these mechanisms to our AI systems and this is something which I strongly believe.

Unknown 12:50
And also, I don't think that intelligence is something which will appear in our system by some mystical emergence process, if we allow ourselves our computational complexity that insanity for some kind of detergent pops out and then tries to achieve automatons twice wave work and spend time or at least catches on on its own and do all these things before we get to the build the system at least at some level, you have to identify the components which are responsible for things like personhood, that social relationships or federal experience, what we define as being conscious and you have to take all these things and decompose them and then at some level, you have to realize that the system okay. Our own group has taken or tried to take these lessons and come up with a relatively simple starting point architecture course of the past seven years, which we call micro design. It's based on a theory by a German psychologist, Dietrich Gardner, the theory is that small size theory so we call this because it embodies a subset goals the theory microsite and it uses unified versus volume representation integrates motivation, that is the system and it creates the homeless agents which navigate our that floor on which dedicated virtual environments that wanted to pursue their goals. And a whole system is consists of neural network simulator for the living perception, which is really great. This is a special case in the modern animal semantic network formalism which is the which you can find agents and run them. And then you have component which gives us multi agent systems or you can make many agents and have them interact and confirm them this environmental tasks that you go over which they can go through presentations through and more Also we can implement the same structures and use the same structures on robots. That whole thing is built into a big architecture that define both sides is really more or less in this theory ask what are the components of permission what needs something for something to be human intelligence of your design in order to proceed, what modules do we need. Since based on Unified Model numerous symbolic representations, which verifies pending application and hierarchical under the lowest level are connected sensors and actuators, thereby, concepts can encode the structural regularities and patterns which are present by the environment and thereby require that we need the relationships with respect to the environment and the individual parts. For instance, threshold elements are more complex nodes. And the whole thing is integrated this one visual system which learns by different kinds of pleasure and displeasure, this newly learned and reinforcement learning methods, it works by starting out with a basic set of predefined demands system as each of these demands, corresponds to drive to satisfy that demand. So, the goals of the system are not predefined, but the demands of the system are given by the very structure of this system very much as our demands are given by all these are of course, the logical demands for nutrition and for physical integrity for instance, then can social demands the check or interaction with others make us feel interested in others as agents most important objects, and make us conform to social norms also based respect to our own conceptualization of our own personality and ourselves as and we have cognitive demands and is committed to Mozart, for instance, a desire for reduction of uncertainty, and another one to be competent in solving problems in general. And these shape the exploration of the environment, not only a physical environment, but also our internal cognitive environment. And together, they have to be a dynamic system, which constantly changes in schools in order to build more complex representational environments to solve the tasks better to maintain some vital individual. Entity look at an example of representation in that system, we have hierarchical representations. Established relates directly to the political environment with directors and nations over them, and use us as symbols to get access to them, they need them, for instance, and communicate specific situations. And these are events and situations with pictures of the environment which we can acquire requires learning. And again, the relevance by the connection to the individual demands of the system, for instance, by order. In this example, you might be afraid of dogs because if you pick one, by one, and the other hand might be the dogs because they are able to satisfy our on demand fortification. This tool creates meaningful representations of the environment, which helps us in different contexts to only endorse representations which are relevant to the goal and then has the potential to the cognitive complexity. So nobody would care. If you just have an arbitrary benchmark. It's occasional different

This transcript was generated by https://otter.ai