Joscha Bach 0:01
Good morning, everyone. I would like to accommodate you wherever you are in the world. Good evening, I would like to welcome you to today's panel about representations for the next generation of artificial intelligence systems. This is you're planning to make this the first panel in the series on vectors for cognitive artificial intelligence. This session today is sponsored by the Cognitive Systems Research Group at Intel Labs. And we are very happy to have been able to help bring a few very interesting and influential, I think, and, to me, very inspiring speakers together, that are going to discuss a number of questions and the speakers originate and psychology and philosophy, neuroscience, artificial intelligence, cognitive science, robotics, what brought us together is the insight that the representations that we have and current AI systems fall short of what we think has to be done. Human representations are different from the representations that we use in neural networks, our representations are unified, that is they have a model of the entire universe, if you can relate everything to they are universal, you can represent everything that we want to in the same paradigm, they are coherent, that is the world that we are representing, make sense together, the parts are not necessarily in contradiction with each other and be optimized for this coherence. And they are real time. That is, they are connected to an environment in which you are situated and tangled with it. And always give us a result in there. Online, we constantly update them, and learn from them instead of processing in batches. And so there are a number of things that our minds can do that current artificial intelligence systems cannot do or cannot do well. And we want to discuss some of the conditions for such systems. And every speaker is going to give a short position statement of about 10 to 15 minutes. If we run into timing issues. Tiny I'll give a little pointer or reminder to the speaker. And so we have some time left at the end to exchange ideas or to build on each other or to criticize each other and see where the discussion leads. Our first speaker today is Mark Baker, Mark Baker is the Henry Ireland's professor in constant robotics and philosophy of knowledge, the University and he is affiliated with the department's of philosophy and psychology. And he has work in metaphysics and the emergence of consciousness cognition, language, functional models of brain processes, and including persons and social ontologies. And he's going to talk about cognition and truth value. Mark, could you please share your screen

Unknown 3:16
okay, is that working?

Joscha Bach 3:17
It is working. Okay, if you have two slides, then we put them on otherwise we just see you.

Unknown 3:26
I do have a few slides. Yes.

Joscha Bach 3:28
And we don't see any yet. It's not working yet. You have to press on the share screen button below your screen.

Unknown 3:33
Oh, that's weird cuz I did have share screen.

Unknown 3:39
Okay, there we go. Not yet. No.

Joscha Bach 3:47
How about now? Yes, it's

Unknown 3:49
working. Okay. Okay. My concern my interest here is with a very small little bit of what has turned out to be a kind of a systematic model and theory. The small little bit has to do with the nature of representing. And the issue here that I'm trying to address is the nature of representing in mines or for mines, not representing for mines, like from the outside world like with pictures and symbols and so on and so on. So, there's a little abstract, classically representing has always been approached in terms of some sort of postulated correspondence. We go back to the Greeks, we have the signet ring and the wax which leaves a correspondent impression in the wax. One of the Greek slogans was like represents like, so, the signet ring actually exemplifies that because it's the structure and the wing in the ring that is like the structure in the wax. skipping ahead a few years, lock over turned this way of thinking about representing and substituted things like cars. So representing as correspondences no longer structural correspondence like with the signet ring, but a causal correspondence. This actually is weaker than the signet ring model, but nevertheless, it's still a correspondence. Today we talk about information as correlation. And having informational correspondences with things we talk about lawful correspondences? There are many different proposals over the last millennia for what sorts of correspondences are the special ones, the ones that actually give us representing?

Unknown 5:49
There have been many problems with these proposals. Some of which go all the way back to the Greeks.

Unknown 5:58
One that I take to be very central simply because it seems to apply to just about every proposal in the literature is how to account for the possibility of error. If you've got a factual correspondence, whether it be causal, lawful, informational, no matter what it is factual correspondences, factual relations only exist at both ends of the relationship exist. But representational, quote, correspondences unquote, can be with things that don't exist at all or false. And the Greeks actually knew this. They they talked about thinking about something as being akin to pointing to it. And how do you point to something that isn't there? Or how do you point to something that's false. We have technologically more advanced ways of talking about this, but we still haven't solved the problem. It became a minor industry in the 80s, and into the 90s, to try to account for error. None of them really worked. But even if they had worked, they were all from the perspective of an outside observer of the animal of the organism of the agent. The real issue shows up with organism detectable error, and that has rarely been addressed at all. So the problem is that of error. The problem is that error and truth of truth and falsity truth value has always been the fundamental problem for correspondence models. Notice this is a normative normativity issue. truth and falsity, functional, dysfunctional, rational, irrational, good, bad. These are all normative kinds of phenomena. And here we have right at the center of the nature of representing a normative issue, how to represent non existence and falsehoods. Incidentally, it was Brentano, who pointed out rather strongly a little over a century ago, that the factual relations require that both rely to exist, but intentional relations do not. One of the reasons that I take organism detectable air as so important is that it's necessary for error guided behavior and for learning. If we can't detect error, we can't guide our behavior in terms of those detections of error. If we can't detect error, we can't try to modify things we can't try to learn in response to that air. So even if we came up with some conceptual model of observer dependent attributions of air, it wouldn't suffice because the organism itself has to be able to detect its own error. Just to give you a feel for the the seriousness of this problem. The radical skeptical argument that goes all the way back to the Greeks that most people just ignore, has the following general form. We can't check the other end of our representational correspondences to see if they're true or false. Because to do so would require that we step outside of ourselves have independent access to what's out there in the world, know what our minds think we are representing, be able to compare them and determine whether the mental representing fits whatever is actually out there in the world. We can't step outside of ourselves, and therefore we cannot check. There's got to be something wrong with this argument. Because if its conclusion were true, as I point out, we couldn't have irrigated behavior and we couldn't have learning so something's got to be wrong there. But it's an argument and a stance that has been around for a very long time and has not been defeated. One angle on or one perspective on the proposal that I make is to give up correspondence, at least initially, and ask what other than correspondence could have truth values, I'm only going to be able to give some kind of intuitions here. The basic intuition is that future oriented anticipation

Unknown 10:25
of potential interaction can be true or false. So if I, as a frog, anticipate that I could flick my tongue a certain way and eat, maybe that's true, maybe there's a fly over there, and I could grab it and eat it. But maybe it's just a speck, or maybe it's a pebble or who knows what might be. And that anticipation if I tried, it would turn out to be false. Notice that, at least in principle, and there's lots of details that need to be filled out here. This is a truth or falsity that could be functionally detectable. If I flick my tongue, and it doesn't work, there's nothing to eat, then it's falsified, the indication of potential interaction is falsified. Now, this requires some sort of a normative functional notion of anticipating interaction possibilities. And so in order to fill this out, I would have to provide a model of normative function. I don't have time to do that I'm simply pointing to that direction. So the basic intuition of what I'm proposing is that there's some kind of setup or micro Genesis or preparation in the brain to engage in some kind of an interaction. And that preparation. If you actually engage in it might turn out to be wrong. It might be a setup for the wrong sorts of interaction. It. On the other hand, if it works, then the anticipation is correct. And in reverse. If it doesn't work, The anticipation is not correct. There's lots of other issues. What about content? I'm talking about truth and falsity. But what about content of representing? I've talked about normative functionality. Normative functionality either exists in its own metaphysical realm, which is sort of a problem, or its emergence. But does the emergence make it any metaphysical sense at all? Is it possible? I argue it is, but it requires a subsidiary model to account for that. I'm using this word micro genesis for brain processes. How does that work? What's involved in it? I have some thoughts about that. Learning? What about learning notice, if we have a micro genetic setup process, and it succeeds, and success in that anticipation is followed by some sort of consolidation of the micro genic micro genetic process. Or if it fails, there's a destabilization of that micro genetic process, then we have a local variation and selection learning process, the overall system will tend to stabilize on anticipations, micro genetic setup anticipations that are correct. And then of course, there's many other mental phenomena that would need to get addressed within this kind of a framework. And I don't know how much time I've taken. But that's the slice that I wanted to address.

Joscha Bach 13:42
Thank you very much, Mark. That was a very interesting start in the day. And also, thank you for being so succinct. And hopefully we can revisit some of the questions that you started to ask. Our next speaker is going to be a personal hero of mine. Steven Kreisberg, is a professor emeritus at Boston University, and has a long list of things that he has achieved in his life. I'm not going to go into all of them in detail, but I would like to focus especially on the fact that he is responsible for the adaptive resonance theory, which is one of the first big cognitive architectures that is has been spanning the entire bits of phenomena that the brain is producing. And it's a theory of how neural activity is able to produce representation and cognition and the facilitation of an organism that is connected in visits environment and embedded in it. And I would like to give the floor to Steven Vosburgh.

Unknown 15:04
Thanks. Can you hear me? Can you hear? Well, I hope you can. So I've taken very literally what we were asked to do. We had a series of questions, and I'm going to read the questions and then discuss them one after the other in less than 15 minutes. So the first question is how is information represented in the brain, and in other systems that can make sense of the world? Well, essentially all brain processes that are devoted to perception, learning, cognition, emotion and action, are represented by distributed patterns of excitation and inhibition across an entire network of neurons. individual neurons rarely encode predictive information about the world, much as the individual pixels in a painting are meaningless. The patterns of activation across pixels, encode context sensitive information about the meaning of the painting. More generally, our brains are self organizing, and processing devices. These patterns are Moreover, organized according to at least two different computational paradigms that are needed to explain how brains make minds and I call these paradigms, complementary computing, and laminate computing. Complementary computing explains the nature of brain specialization. Many scientists have proposed that our brains process independent modules, as in a digital computer, the brain's organization in two distinct anatomical areas, and processing streams, indeed shows that brain processing is specialized. However, independent modules should fully compute their particular processes on their own. And much behavioral data shows this is simply not true. For example, during vision, interactions occur between brightness and depth, motion and color, motion in depth, text, texture and motion and on and off. Complementary computing asserts. In contrast, that pairs of parallel cortical processing streams compute complementary properties in the brain. Each stream has complementary computational strengths and weaknesses, and several processing stages of interactions within and between cortical streams that are needed to overcome these complementary deficiencies. These processing stages thus, realize a hierarchical resolution of uncertainty. And complimentary computing shows how properties that control observable behaviors arise from interactions between pairs of complemented cortical processing streams with multiple processing stages. lamina computing explains how and why all higher forms of biological intelligence, including vision, speech, language and cognition, are generated by specializations of a single canonical cortical circuit.

Unknown 18:57
My new book conscious mind resonant brain, how we train makes the mind provides unified and principled explanations of how this happens. Well, the second question is what is the relationship between perception and reasoning analytic thought, symbolic cognition and how are these different modes implemented? Well, perception is general purpose because all perceptual experiences whether familiar or unfamiliar can be registered by our sense organs and process. However, recognition and more generally, cognition requires that objects and events in the world whether familiar or unfamiliar, can be I'm sorry, objects and events in the world are first learned and thus familiar. Learning occurs in all stages of being part assessing both perceptual and cognitive were the earliest stages learn simply things like the orientational tuning of simple cells in the visual cortex. And the higher stages learn more complicated things like in various recognition categories in the anterior inferior temporal cortex, and cognitive plans and prefrontal cortex. Recognition categories function much like symbols, a single category, some can selectively respond to a large number of similar distributed patterns of activation across feature detectors in order to use symbols and thoughts, cognition or reasoning sequences of them needed to be stored temporarily in a working memory. And my book explains why all working memories in the brain, whether linguistic, spatial, or motor, obey the same kind of brain circuit. It's a circuit that allows the sequences that is stored in working memory to be learned and stably remembered as cognitive plans that can be used to trigger context appropriate actions. All of these processing stages have both bottom up and top down excitatory pathways, ensuring that learning doesn't experience catastrophic forgetting. They solve what I call the stability, plasticity dilemma. In particular, adaptive resonance theory or art that was just mentioned, explains how multiple processing stages in our brain use these interactions, to learn to attend, recognize and predict objects and events in a changing world. All the foundational hypotheses of art have been supported by subsequent experiments. And art has to migrate the light provided principles and unifying explanations of hundreds of additional experiments. And these explanatory and predictive successes, but maybe because I was able to derive it already back in 1980, from a thought experiment, about how any learning system can autonomously correct predictive errors in a changing world, as Mark mentioned earlier, and the hypotheses that go into the thought experiment, are facts that are familiar to us all from daily life, because they represent ubiquitous environmental constraints on the evolution of our brains. Moreover, nowhere in the thought experiment of the words mind or brain mentioned. So all are, is this a universal solution to this learning problem, which I call the stability plasticity dilemma, because it has helped any system and learn quickly without experiences catastrophic forgetting. It's therefore I think, remarkable that art dynamics also support conscious states of seeing, hearing, feeling and knowing

Unknown 23:28
these conscious states arise from resonances, which occur when excitatory feedback signals between two or more brain regions match signals well enough to cause the active cells to synchronize and sustain their firing long enough to trigger consciousness and learning. That's why I call the theory adaptive resonance theory. Because of the universality of thought experiments, they provide a blueprint for designing autonomous adaptive intelligent algorithms and mobile robots for engineering, technology and AI. Next question, are there efficient general representation paradigms? Or do different tasks require different approaches? Well, each modality of intelligence such as vision, audition, cognition, emotion and action, and models are built up from a small series of shared equations, and just a somewhat larger set of micro circuits. Usually just three types of equations suffice for short term memory or cell activation, medium term memory or activity dependent habituation, and long term memory or learning and memory specializations in these equations and microcircuits and then assembled in modal architectures that carry out different modalities of intelligence. Again, look at my book, if you want an extensive and self contained discussion of this. Well, what is current AI, research and deep learning missing in the treatment of representations? Well, thank you for that class. The deep learning, as you know, is just a feed forward adaptive filter. And it's untrustworthy because it's not explainable. And it's unreliable because it experiences catastrophic forgetting, you can't really use it in life with death situation, because if it fails, you'll be sued for everything you're worth. Or it is not just the feed forward adaptive filter. It's a principle theory of biological intelligence. That's a self organizing explainable production system. That carries out hypothesis testing, that was all about self organized error correction, says self stabilized learning, classification and prediction, and it loves to live in a changing world. Next question, what's the status of qualia in a general representational paradigm? To discuss qualia, you need to understand how we're in our brains? And why, from an evolutionary perspective, nature was driven to this conscious states, where why do I buy we can consciously see, hear, feel and know things about the world? I believe that in brief, the answer is that multiple processing stages are needed to convert incomplete and noisy sensory data into a sufficiently complete representations, where our brains can effectively plan and act to realize valued goals. But if you believe that then you ask well, how do our brains know which of these processing stages compute sufficiently complete representations to control successful action. And my claim is that consciousness lights up such a processing stage with a resonant state. Again, my new book goes into that in detail, but let me just say a few words about why evolution may have been driven to discover visual consciousness. If you look at the retina, where lights registered, before light signals are bundled into the optic nerve and sent to our brains, you may know there's a big blind spot where the optic nerve forms, and no lights register there. And it's as big as the fovea, which is the high acuity part of our retina and we move our eyes with psychotic eye movement several times every second to point the fovea at regions of interest.

Unknown 28:24
If no further processing occurred, you couldn't look at or reach for things, that it the blind spot. Well, but we're not aware of that, because we complete the representation over the blind spot, so that we can look for and reach objects in these positions. And I would tell you a little more about that. But I want to stay within my 50 minutes. But as to how the brain knows at what stage of all this pre processing, representation is complete enough. The claim is a resonance with the completed surface representation. And the next processing stage, lights it up and renders it consciously visible. And we use that look in reach. I call that a surface shroud resonance. I predict that it occurs between visual cortical area and the foreign posterior parietal cortex. By now it has classified six different kinds of resonances, to support conscious states of seeing, hearing, feeling and knowing. My book explains why although all conscious states are resonant states, it's not true that all resonant states are conscious states. The book also clarifies why lots of brain processing is never resonant. In any case, one example of complimentary computing is There are processes proprioception and cognition are complementary to spatial and motor processes. But I'll stop there. And if there are questions later, I'll be happy to try to answer them.

Joscha Bach 30:14
Thanks. Thank you so much, Steve. Our next speaker is a US Attorney. Miss Kaya, you will certainly miss Kaya is since I joined Intel is your colleague now. Yuya is the leader of the applications research team of the neuromorphic computing lab at Intel. And in the past, she has led the neuromorphic cognitive robot scope at the Institute of neuro Informatics at the University of Zurich and the eth and the autonomous learning group at the Institute for neural computation at the world university program. And she's going to talk about memory intentionality and autonomy enabled by neural attractor dynamics.

Unknown 31:03
Thanks, Sasha, for the introduction. And thanks also for invitation to this panel. I'm really humbled MSA to talk after both Mark and Steve. Let me see, let me first check whether you see my slides.

Joscha Bach 31:19
Over there, yes, they're not fullscreen? Yes. Okay.

Unknown 31:22
Great. Yes. So I'm humbled to talk after Mark. And Steve, because most of my research work over the last 10 or 15 years has got a lot of inspiration in their work. And I can guarantee you that in this presentation, this 10 minutes, there won't be any ideas that you won't find in either Steve's or Mark's work. However, what I tried to add, is the sincere desire and work to to show that these ideas can be operational or to make them operational and to actually build systems based on these ideas that can produce behavior in the real world. And can can show that it is both needed, and to show what they can achieve in behaving systems. So my personal interest is in what I would call embodied AI.

Unknown 32:13
And the citation that I want to bring up here is from Rodney Brooks, who had this beautiful article, elephants don't play chess. And indeed they don't. And moreover, most humans also don't play chess, or at least some humans don't. And those who do, playing chess, and doing some some high level cognitive stuff is not the only thing that they do during their lives and days and weeks. Most of the time, what both animals and humans do are other things. Most of it is about moving around, sometimes in very fascinating and ingenious ways. It also by doing such mundane things is right, washing dishes or cleaning up your child's room. And most of them cognitive or intelligent processes, they really unfold in these very practical physical interaction with the physical world that amounts to planning, generating and controlling movement around the world, in the world, in an environment. And if we understand it in very deep, but also very practical way how that is done by biological neuronal systems that could help us to actually make smart robots a reality to humanity has been dreaming of those machines that will do things that humans do for millennia, I guess as well, at least for for hundreds of years. And we're still looking machines that could do what humans or animals can do in real world environments, real unstructured environments, where they are bombarded by no unexpected events where they cannot assume that they know everything about the immediate environment. So the big question, how can we get to know such machines that can interact with real world can behave in real world and can learn in real world in interacting with this world? So for this panel discussion, I have three key statements that I will know bring forward and then kind of substantiate them a little bit with a couple of examples. And then hopefully, we'll can have some discussion, discussion, the end. So the first statement is that representation requires memory. Okay, kind of maybe an obvious thing. And the next thought on top of that is that memory requires recurrence in whatever representation we build, especially if this memory needs to be sustained for any meaningful amount of time. And when meaningful, I mean, anything beyond a couple of milliseconds, the temporal timescale of neuronal systems and the noise that is present either in your sensory system or in your computing system itself. And destructors. And it is true that today, we have computers that have very little noise in their computing substitute with a very reliable, but the question might arise at what cost we achieved this absence of noise and the cost is pretty high with literal energy cost to keep our computers very, very reliable, their elementary elements and then we have zillions of those entry computing elements, those zeros and ones in the computer. And at some point, the question arises, how much do we pay for, for having that reliable computing substrate? If we don't have a reliable computing substrate, then in order to keep memory around, we need recurrence. And then of course, this representation, this recurrent representation needs to happen in some suitable substrate and the substrate needs to be suitable for the content of what you want to represent these sequences of sounds, are there some spatial patterns, temporal patterns, or motor patterns. And depending on the nature of this representation, the nature of the substrate might differ. You might also need to represent some additional things like for instance, value or utility of different states or representations, and maybe also your certainty in the solid representation. And in this aspect, I will kind of disclaim that, I don't have a very good answer yet, what is the best representation for all the different things that we want to represent in our system. The second statement is that if we want our representations to be about some entities in the environment, and this is something that Mark was talking about. So if we want to make sure that whatever we represent is actually related to some object out there no to an apple that you're looking at, or you know, that the representation of the motor plan that you want to execute actually results in some physical movement, then you need some additional structural components, we need an additional structure to your elementary computing unit, and I will call it intentional structure. And then we'll show you that this has some very concrete consequences for the neural networks that we should build, if we want our systems to have these type of representations that can be relatable to entities out there in the world. So, these representations need some some notion and some representation of intention. So, what we want to represent or what we perceive or what we want to execute as a motor action, they need the notion of condition of satisfaction that will monitor whether that representation has been achieved in the other world, they might also require a condition of failure, some preconditions, moment to moment expectations, what will happen next, and this is something that Mark has talked about, that you need to anticipate something from your interaction in the world in order to be able to derive some normative signals that will help you to know when you make an error and know when you have to learn or adjust your actions. So, this intentional structure is required for to build systems that are aware of a environment that can build representations about the environment and can use those representations to act in the environment. And finally, if we have such units that have memory and make informed memory, and they have this intentional structure, then we can start building networks that will be structured, they can be structured as just by us and maybe by some no evolutionary or high level learning process or they can be structured by experience of our agents with the environment. And then this experience, temporal or spatial contingencies and parents can be reflected in our neuronal system can be memorized there. And importantly, they can be learned autonomously. So all the decisions about when to learn, where to learn which part of the neuronal system should be updated and how shall this part of the neuronal system be updated can be taken by the system itself autonomously if it has this intentional structure in place. Alright, so now a couple concrete examples. So the first memory so why do we need memory a very practical view on that so if you want to build some robotic agent, but behave in the real world environment, what is everybody agents it's usually would have a number of sensors. And it can be different sensors it can be you know, camera that delivers images 30 times per second, it can be you know, some neuronal inspired event based camera that delivers event million times per second. It could be an audio, audio sensor, tactile sense, the IMU, all the sensory signals, they bombard our neuronal system at different timescales with different delays in different temporal structure also is different spatial structure. Now, our cognitive and neuronal system here in the middle needs to somehow build some representation of its environment that is sustained long enough for the system to plan some movements or movement sequences and execute them. So to link to the motor system and to different kinds of movements, some may be very quick like the blink of an eye or saccadic eye movements, some may take quite some time if I need to move my arm or to move around in the environment to find a certain object. So in a way, the task of the neuronal system is similar to the task that one would have to do in order to represent an elephant that is, you know,

Unknown 39:45
sensed by by a number of blind people who have access to different aspects of this elephant and they also know sample input with different temporal scale and our neuronal system needs to create that representation. So in order to do that, we need to create those those memories states that can be that can sustain themselves over some periods of time that go beyond just intrinsic neuronal dynamics. And one way to do that or to formalize those dynamics is in terms of attractive dynamics. So when we design our dynamics in such a way that it has those stable states that are stabilized by interactions that we create in our in our system, so that if there's, you know, some noise, or distractors that try to bring our system outside of that attractor, state, the forces in the system will bring it back to the attractor state. And that will help us sustain that memory state, as long as needed for for the agent to engage in some behavior. Now, in neuronal terms, what does it mean to create such a tractor? Or how can we create that, and this is something that Steve has mentioned that and has basically developed or suggested this type of an architecture is when we define a population of neurons, and we interconnect them among themselves recurrently in such a way that they create stable activity patterns, and the simplest such activity pattern could be just a localized, attract the bump. And you can create a network that will create these bombs just by a certain pattern of local excitation and global integration. So if we have such attractor networks that create stable states that can sustain their, their value their activation over those periods of time, then we can start building architectures, neuronal architectures, that could solve different tasks. And in our work, we have shown how you can build such architectures architectures with these properties. What do these new attractor networks enable to first enable memory to maintain that those representations long enough to do action planning, also to do continual learning with credit assignment, or context aware decision making, when you make a decision, not just based on your immediate context that you see a fly in the corner, but also sometime in the past, it also performs attentional filtering. So when you create this stable attractor state, then the distractors will be inhibited away so that you don't switch to them just because at some point is distracted, maybe beginning had stronger security than your current state. It was a great stable state that the required to control motion to control movement, and also to control activation flowing your cognitive system. And it also allows you to have building blocks out of which you can build a neural architecture that will be transparent, where you will know and have understanding of what each module is, is working addressing the issue that Steve has mentioned that we want to avoid architectures, where we don't know what the architecture is doing, we have obtained the architecture that does the correct behavior. But we don't know why how and when it will break. Alright, the second element was intentional structure. And there we have suggested this very concrete module that allowed us to build architectures that we can instantiate in robotic agents, and then make them organize their behavior in time. And this intentional structure, we call this unit and elementary behavior has basically two neuronal nodes, one representing intention. And this can be either perceptual intention. So if I'm looking for object with particular properties, it might bias my perceptual stream to pay more more attention to two objects with these particular features. Or it can be a motor intention, where I generate a motor goal and again, it will bias both my perceptual and motor system so that they engage in a certain action. This neuronal node will have some self recurrent connections so that it creates a stable state, if I generate such an intention, I will stabilize it and we'll just stick at it as long as needed for me to achieve whatever goal of that intention was. And this achievement of the goal is monitored by another node that is connected to the sensory surface and sculpt condition of satisfaction node, and it detects the state when I'm done with whatever my intention was, if my intention was to find object of certain properties, then my condition satisfaction will be active when I have found it, I have created a representation in my perceptual system that responds to that object. If that was a more intention that against perceptual improvement tells me I'm done with that action. So, we have postulated this little module as an elementary behavior and we have shown like early on in work of Montessori stuff in Bo home, that you can now build architectures that you can connect to you know, sensory surface and motor system of a robotic agent. And you can put this agent in an environment and can now let it execute different tasks meaning that result in a different sequence of actions that will be automatically controlled to have some kind of executive control monitored,

Unknown 44:43
and corrected by by by the system. And then at any time. When the system now controls the robot it will stay connected to its environment and to its input so if something changes in the environment, the system will recompute the plan and continue staying in engaged with task now. So so that was the system that was able to generate behaviors. Now what if you want to be able to perceive behaviors, what if you want to be able to parse action sequences one to the same system to observe someone being engaged in a sequence of actions and be able to do parse and understand which actions have been performed, we can use the same notion of elementary behavior for that, we've discovered that we need to augment it with another node or concept condition of failure, so that if I build a hypothesis that I'm currently perceiving a reaching action, but then this hypothesis is not confirmed later, I need the concept of failure. So I have to discard this hypothesis. Because in this framework, I have to actively discard the hip hop receptor actively inhibit the intention that was activated by me, building the hypothesis that I'm probably preceding the switching action. Now. The next element that we might want to do with this system, if we have this system of elementary behaviors, this network of elementary behaviors, we might want to be able to plan sequences of actions that unfold to certain goals. If I have a goal state, I might need to be able to plan a sequence of actions that would lead me from my current state to the goal state. And again, we can do it with the same elementary behavior, we needed to augment it with another note and motivation note that keeps track of possible transitions or transition trends that are required to bring me to the goal, but it's not my planning from the goal state, I have to go over the behaviors, so many behaviors that will lead me to the goal without activating them. And in order to have the representation of the behavior that won't activate the motor action, I need another another note motivational. And again, we have shown them a couple of examples of this works. And finally, when I have the system of elementary behaviors, this network, I should be able to also learn sequences of the sections with different like different ways I could probably learn it from imitation learning when they just observe a sequence and they just form a memory of the sequence with the mechanism that Steve has suggested, for instance, for performing serially ordered memories, sequence memories, or you could use something like reinforcement learning to learn those sequences. So just trying out different sequences, observing rewards once in a while, and then kind of propagating this reward to neighboring neighboring actions in time by updating the value of different transitions state action transitions, so they can learn sequences of actions. And again, we have shown you can actually do it on a behaving agent, autonomously behaving agent. And finally, when we have this this modules, we can indeed build all kinds of architectures. And here, I won't go over the whole list, but we have tried different types of architectures for different tasks. This is one recent example from our work on object learning. So here we have a scenario where we have a robotic agent can be simulated, we also did it with a real robot I could provide, it observes an environment. And we wanted to learn a couple of objects that it needs to interact with the user. It doesn't need to distinguish any car from any cat. So it doesn't help if we train it with no image at all possible images in this world. We wanted to distinguish these three particular objects that the real experience with this user. And in order to do that, so we can design an architecture that would do that, I will only highlight one part of this architecture that we call neuronal state machine. Because in order to do to learn these objects autonomously, we need this system of again, elementary behaviors that will detect different states in which the system currently finds itself versus be in a state that sees an object but doesn't recognize it. So it's a state when No, I need to learn this object, I need maybe to ask the user for a label, and then try to learn this object. If I hear a label I can be in a state when I have heard this label before, or this is a new label. And these two different states will trigger different types of learning. So either I update representation of an already known object, or I create a representation for a new object that they haven't heard before.

Unknown 49:06
Now, this is not the end of the story. If we really want the our robotic agent to learn these objects autonomously. In interaction with a user we need more structures around. For instance, if we want it to learn efficiently, we might want it to learn one object at a time. And then we need some kind of attention dynamics that will direct the gaze of the robot in its attention, overt or covert attention to individual objects. So we need some attentional dynamics that will run now according to its own temporal temporal scales and inputs. We might need different elementary behaviors or those neuronal states that will just trigger different states of behaviors of the robots. Now looking at an object could be one behavior, fixating on an object like doing little micro saccadic eye movements, could be another behavior, trying to learn an object, it's another behavior trying to recognize an object yet another behavior. All of them need to be in instantiated as an intention can be executed and be monitored when they are executed. Another component that we have introduced is a spatial memory, so that you not only recognize objects, but you also build the spatial memory of where you have seen those objects. And again, there's a whole story or structure around this spatial memory, I just flesh, flesh the different components of that. So we kind of call it the slam simultaneous localization and mapping. So something usually an agent that moves around in an environment has to do it has to estimate its position. And it needs to build a map representation of the environment, but could see it as a very general task that the cognitive agent needs to do know, both is aging that the tabletop needs to do, it needs to build a map of its immediate environment with objects on the table, and then also robot that moves around in the environment as well. An important component here coming back to marks talk is this error monitoring and correction. So we need to instantiate the whole module that will constantly or at least once in a while, be able to estimate whether it has some error in its estimation of its own velocity estimation of its own position, or expectation, what to see in different positions. And when he's mechanism to detect the error to estimate this error, maybe magnitudes, maybe direction, and then use this error to update our system to learn.

Joscha Bach 51:24
I think we are out of time. Okay, but if you have a closing sentence that you want to say,

Unknown 51:30
I have a closing slide. So So one, one note. So these architectures, they're really great. And we have been building them for years. Why haven't you heard about them? Most? Probably? And I think the answer is that these architectures in because we've tried to build practical systems for robots, they just running too slow the conventional hardware. And there, I do believe that this novel neuromorphic systems that we now develop at Intel, and those other people develop all over the world, they really help because those neuromorphic hardware systems that follow the structure of the brain network biological networks, they help us to run these metrics, even large architectures in real time that we need for for practical robotic applications. Okay, so yep, so these are synchronous, parallel and event driven computing hardware helps to make these types of networks usable and practical computing model with stability, intentionality, and normativity. So ability to detect errors is required. And if you have this modules, we can build large scale neural architectures. And learning should be an inherent part of the system's autonomous dynamic. So it's not some extra source that we add at some point. It's really just part of the system's dynamics. And the final statement is that instantiating models in real world behaving systems reveals demands on artificial intelligence systems and controls that might be overlooked otherwise, so and I'm alluding to five months statements that if we cannot build something, I probably don't understand it properly. And I think we have experienced it many times in our work that it's useful to build the systems. Alright, Dan, thank you so much. Sorry for taking a bit longer.

Joscha Bach 53:00
Thank you. Yeah. Our next speaker is Jerome Bucha. Meyer. And he's now a distinguished professor at the Psychological and Brain Sciences in cognitive science and statistics at Indiana University Bloomington. And he is the father of decision field theory. And he has a very interesting perspective on the way in which mental representations work, they just not just using probability theory, but actually quantum probability theory. And without further ado, Jerome, the stage is yours.

Joscha Bach 53:48
Hello, yes, can you

Unknown 53:51
okay, you can hear me? Okay. Yes. Okay. So. So what is the goal? Anyway? I've been working on a topic the past 10 years, called Quantum cognition. And what is the goal of this program? Well, it's not a physical or neurobiological theory of the brain. And it's not really a theory of consciousness. I mean, it could be implemented by maybe some Steve's work. Instead, it's a mathematical theory about human behavior, especially judgment decision making. So but why do we want to use quantum theory? Well, quantum theory, you know, other was developed in physics. It's a general axiomatic theory of probability and human judgments and decisions are probabilistic. And the thing is that these probabilities that human judgments make, or you know, form, they don't pay the Common Core of axioms. So quantum theory provides a viable alternative. But you know, there's many alternatives, you know, why quantum theory? And one of the things that's you know, useful about quantum theory that matches human judgments is non commutativity of measurements. So, measurements, they change the psychological process, they produce context effects. So the principle of complementarity, which was a suggested by Niels Bohr, but it was it was the principle come complementarity was actually suggested to new abour by Edgar Rubin, based upon work by William James. And so we want to put complementarity back in psychology. Steve talked about complementarity tune is neural models in the last point is cognitive scientists like to work with vector spaces. And so quantum theory provides, let's say, maybe the optimal way to compute probabilities in vector spaces, as we're going to see in a minute. So how do we use quantum theory? Well, probability theory has been around for a long time. But it was first acclimatized about Comodoro, in about 1932. Now, quantum theory was developed by a number of physicists in you know, 20s, in the 30s. But it wasn't acclimatized until about 1932, by von Neumann. So let's compare the axioms of these two systems. So first of all, classical theory, you know, is based upon idea of sets. So we have a sample space of possible outcomes. So each outcomes a point in the sample space, like if he had a rating scale, and somebody's making rating from zero to 100, we'd have 100 points in the sample space. Now, the corresponding idea in quantum theory is a vector space. So each outcome in in the vector space is an orthonormal vector. And if you had 101 possible rating, Scale values, then you'd have to use a 101 dimensional vector space each, each axis or each orthogonal basis vector would represent one of the outcomes. So we're moving from a kind of a set sample space to a vector space. Now secondly, and this is, I think, one of the most important parts that distinguishes classical from quantum theory, when we talk about an event, let's say the event giving a rating greater than or equal to 75, you know, that's going to be a subset of the sample space in the classical theory. Now, subsets, you know, the the, the algebra of subsets is a Boolean algebra. And so you have to buy all the Boolean axioms, like you have to buy closure, if A is an event, these events and a and b is a event also always have to buy commutativity, A and B is the same as being a and you also have to buy distributivity A, N parenthesis B or not be as A and B, or A and not B. So all those axioms have to be adopted. When you buy you buy into the subset representation of events, like an event getting greater than 75 Greater than or equal something by, but those x, those axioms of Boolean algebra are really strict. And human judgments don't always obey those axioms, fact, they, we have evidence that they violate each one of those axioms. Now, in quantum theory, and this is where I think it's really key quantum theory in events, like getting an A rating greater than equal to 75, is a subspace of the vector space. So instead of having subset subsets of a set, you have subspaces, of a vector space. And so now you're working with an event algebra, that's based upon the logic of subspaces. That logic is not the same logic is bullying. In fact, it's not bullying. In fact, von Neumann and birkoff start developing a whole new generalized

Unknown 58:11
program of logic based upon quantum logic. And so in quantum logic, you might have an event A in the vector space, you might have an event B in the vector space, but there is no A and B, no conjunction, there's an A and then B. And that's because things don't commute. A is A and B, A and then B is not the same as B and then a and distributive axiom also breaks down. So the producing what we call interference effects from quantum theory. So so the event structure is different. And we're changing from subsets to subspaces. Now, in classical probability theory, how we what we do is we form a state like I have a state I, you know, I believe certain things are true. Steve has a state, he believes that certain things are true. And sometimes we agree. But anyway, he's got to state now in classical probability theory with I find that as a probability function P, that P is defined on the subsets of the sample space. And quantum theory works a little bit differently. The state is a vector sitting in a unit length vector sitting in your vector space, and the way you compute probability. So you take your vector and a vector space, and you projected on the subspace corresponding to the event. So and then you get the squared length of the projection. And so that's how we compute probabilities in quantum theory. Now, the basic one, that one of the key things is that separates these two theories is when we look at conditional probabilities, so in classical theory, the probability of event B given A we take our original probability function B and then and then we, we, we renormalize it by the probability of event A, but here we have the commutative conjunction in the numerator. Now in quantum theory, we do something looks similar, but it's quite different. So the probability of B given A in the quantum theory, we start with our state, we first project on the subspace for a And then we project on subspace for being we get the squared length, and then we normalize. So they both normalize. But the thing is, this these projectors may not commute. In fact, the non commutativity is the key part of quantum theory that things commuted, then quantum theory would reduce the classical theory. So in some sense, quantum theory is a generalization of classical probability theory, whenever we have non competing projectors, and that's where all the action comes in, in quantum theory. So let me give you an example. You know why? Why do we use this theory? And we have many, many examples, but I'll just give you one. This kind of a famous example what we call the disjunct conjunction, disjunction, probability air, a human beings exhibit this kind of phenomena. And this was originally studied by twersky and Kahneman, we have a paper and psych review that kind of describe our explanation for this. So you know, this is an example story. And then there's many, many, many examples. But this is just one example. Suppose Linda was a philosophy major as a student at UC Berkeley, and she was an active witness in a social welfare movement. So you're given his prior information. So you're told this information. Now you're asked to rate the property of different kinds of events? Well, what's the property that then is a feminist? Well, that seems pretty likely, given this story. So you might give it a high probability point a three, there's nothing wrong right or wrong about that. Then you're asked, well, is Linda a bank teller? Well, that seems unlikely given the story. And so people tend to give it a lower probability point to six. These are actual numbers from an experiment, some averages across subjects from an experiment. But the key thing is Wonder asked is Linda Linda, a feminist and a bank teller? Well, they give it a probability rating of point three 6.36 is greater than point two, six. So we get that this is where we get a conjunction there.

Unknown 1:01:51
Because you know, the conjunction now is rated higher than a single event. And when they're asked is Linda, a feminist for a bank teller, we get a disjunction here, they give it a rating of point six, but the disjunction is lower than the single event by itself. So we get these, this doesn't always happen. But there's many different situations where people will make these disjunction and conjunction errs. Well, we call them errors. But maybe it's an error with respect to commodore of theory, but in terms of quantum theory is not necessarily an error, because, as we'll see in a second, you know, prevent quantum theory provides a simple explanation from this. So these kinds of findings can fold followed directly from a quantum kind of axiomatic representation of reasoning. Now, first of all, we just want to point out the conjunction there is a violation of the law of total probability. So the property bank teller is the property that she's a feminist and bank teller given a feminist plus the probably she's not a bank teller, probably probably she's not a feminist times, probably bank teller, you are not a feminist. So that's the law of total probability. And of course, this event here, bank teller has to be greater than this event here, because this is just one of these two that are summed up and this has to be greater than or equal to zero. So the conjunction fallacy, so this conjunction fallacy violates this doctrine of probability. And so that's kind of a signal for quantum theory. So quantum theory is kind of famous in physics for explaining violations of law of total probability, like if you read Friedman's book, the double slit experiments, basically violation of total probabilities. So here's the toy model. I hope I have some time to go through this. Now, this is just a toy, but it gives us an example of how the theory works. So in quantum theory, then we have we have events, yes to feminum. You say yes to feminist, that's one event. You say no to feminist. That's another event. So I'm ready representing the event? Yes, the feminist by this horizontal subspace, this this array is representing note a feminist by this vertical array. And they're orthogonal, because you know, yes, is usually exclusive from No. Now the key thing is, in quantum theory, when you have incompatible events, basically, you're changing the basis for the vector space. So this is a two dimensional vector space, but we have many different bases that we could choose. And so now here, we got a basis for the bank teller, the bank teller has rotated with respect to feminism. Here's the subspace for us saying yes, the bank teller, it's this orientated array with the positive slopes. And here's the vector, the subspace representing not no, not bank teller. It's a negatively related line. So so we got these two subspaces that are incompatible for these events. Now, here's our state vector. So this is after the store, you get the state vector. Now if we ask, given the state vector, well, what's the probably that Linda's a bank teller, or? Yeah, what's the probably that she's a bank? Teller was probably yes, the bank teller. We take our state we projected on Yes, the bank teller we get this tiny projection right here. The projection is like point one six. But now if we take our state and if we look at the conjunction, we take our state Project on. Yes, the feminist first because we answer feminist first, and then project Yes, on the bank teller, we get a larger projection, it's point three. So the projection from the state original state to feminist and then the bank teller exceeds the projection directly from our state to bank teller. And so that's a simple toy model. Now we can derive it more generally from the axiomatic principles using this is this algebra this, for the relies on the vector spaces for the quantum model. I'm not going to go through the algebra because it takes some time. And but this model not only explains the conjunction and disjunction fallacies, but it makes a number of other additional predictions that we can test and that are heavily supported by the model. So, in conclusion, quantum theory provides an alternative framework for developing probabilistic and dynamic models of decision making. I haven't talked about the dynamics, but because of limited time, and provides a coherent account for puzzling violations of classic probability finding found in a variety of judgment decision making studies, and forms a new foundation for understanding why they different phenomena decision making, using a common set of axiomatic principles. What I didn't get time to show us, there's some really puzzling findings in psychology that never had been connected together before. And by using the quantum axioms, we provide a coherent axiomatic explanation for wildly different phenomena is in common set of principles. So if you're interested in this, if these ideas, then you can get, you can take a look at our book with my book with Peter Prusa, on quantum models of cognition and decision. So then, I guess I'll stop here, then.

Joscha Bach 1:06:43
Thank you so much, Jerome, that was a very clear talk, given the short amount of time that you had available, I hope that people get the core insight. Our next speaker is going to be Steve Rogers, affectionately, also known as Captain America. And Steven Rogers is Senior Scientist for autonomy at the Air Force Research Laboratory at the Wright Patterson Air Force Base, and Ohio. And he is a thinker in cognitive architectures. And he's very interested in machine consciousness as well. So Steve, the floor is yours. Okay,

Unknown 1:07:26
let me try to share my screen here. I can go ahead and start the discussion. Please. Go ahead. Yeah, yeah. So so, you know, the fundamental research question that we've been addressing is, you know, what are the principles of representations, think qualia, that enable flexibility. And so I enjoy I enjoyed the talks this morning so far, but let me weave a story for you that sort of uses the words of that question, to sort of guide you through the way we're thinking about how to achieve flexible AI. So I start with, you know, addressing, you know, what are the principles? So, you know, I'm reminded of the 1700s, there was a French philosopher, who suggested that, you know, if I might paraphrase principles, you know, compensate for a lack of facts. And when it comes to consciousness, or predicting where AI is going, we certainly have a lack of facts. So so we focus on principles. And then we let those principles guide our, our implementations of AI. With respect to representations. We, our view of representations or representations are a combination of how an agent structures, its knowledge, and the processes it uses to use that knowledge to create meaning and of course, meaning is then what it uses to take actions or update its knowledge. Okay, so given that background, given what are the principles that have representations qualia, I need to sort of talk about qualia. And so that the get there, I just want to back up briefly, it's been mentioned a couple of times, things like Kahneman and Tversky, and, and, you know, Evans and Stanovich, etc. There's a lot of people who believe that we have we have we have two cognitive systems. Sometimes they're characterized as sis one insists to subconscious unconscious, but the idea of that, that way of looking at it you go great, if you just go down to once you get it to full screen if you go to that second slide there,

Unknown 1:09:24
it will be so, so you know, they're you know, some people think of this this one is a good call it your gut response. And they characterize like in the heavens and Stanovich case, this the sort of functional characteristics over here on the right. And I find that that's a fine way to look at this as you can get, you can get a lot of mileage by looking to this as a dual cognitive systems. But one of the challenges I've run into is a lot of AI people. And I admit even Myself, on occasion, have have suggested that our current AI approaches like in deep learning, sort of replicate what goes on in sis one. And and there's some truth to that, I suspect but but, but there's all sorts of examples in the literature I gave you this picture down here on the bottom of a, of a person with blind sight. And what that means is they've had bilateral damage to the visual cortex on in both hemispheres and so they have no conscious access to the visual world, their eyes are fine, their optic nerves are fine, their brainstem is fine. This is particularly meaningful for me today, because I was losing vision Am I right? I earlier today and so I've been at a doctor all day. But this this guy has no access. If you hold up something, say what is that he'll say, I can't see anything the world The world is black, I'm blind. But yet you put him in a cluttered hallway and you'll navigate his way through the hallway, he can approximate an envelope at the right angle to put it in a slot. So he clearly has access without consciousness be involved with the visual world. And I find that those examples interesting. There's others that are even more profound. There are examples where you can show that, that without consciousness being involved, you can be determined whether an image is congruent or incongruent. And by that I mean, what I mean by that is, for example of an images of a grandma pulling out something out of the oven, if the image is of her pulling out a chessboard, that's an incongruent image versus her pulling out a cookie sheet. And it turns out that that the calculation of incongruence is made without conscious deliberation, because you in fact, get those things brought to your consciousness sooner if they're incongruent. So I just want to sort of set the stage and suggest that I'm very focused these days, on qualia insists to but I don't have delusions in that, in that it's the only really cool cognitive processing going on. And it's far more than just what we normally term is perception. You can broaden perception and maybe covers this one. But but let's let's move on to qualia. Let's go to the next slide, please, Kanye. So So to talk about qualia, you know, the metaphor and it is a metaphor, it isn't reality, that that sort of cartoon in the middle of this slide, gives it to you, you have this illusory and I do mean illusory. There's no place in your in your brain, where all the information is brought together, we have this like a little homunculus, sitting there, absorbing the visions, and the sound and the smells and the taste, and having thoughts on his own, you know, but it's an interesting metaphor. But what I want to talk about with respect to consciousness, if you look at the little patch of red there on the right, what I mean is what you're actually seeing there, the mental image that your mind created for you is fascinating. And as Steve Grossberg was talking about earlier, you know, the photon stopped at your retina. But this, this thing you're perceiving in your consciousness is created for you in your mind. And if in fact, the square just to its right, if I change the hue, saturation and brightness of it, you actually get a different perception. So that's, that's why I use the quality word, the quality of what you see is, is evoked in your mind for that redness, as a different quality with a with a square right next to it. It's the quality that you experience in your consciousness when somebody plays a piece of music for you, or you taste a piece of pizza. Most importantly, on the bottom left here, it's what I see and feel and experience when I hold my granddaughter Belle there are even when we when Bella and I walked down to the local beaver pond with Napoleon, our favorite dog, and we're overwhelmed by sensations and emotions are the emotions I had when I buried my father in Arlington some years ago. So that that sensory motor world model is what I'm talking about when I talk about qualia. They're the elementary elements by which you construct this is this experience. It's the phenomenological experience you have, which we call consciousness. Okay, now, next slide.

Unknown 1:14:00
So what we've been after is and I have to admit, before I just dive into this, the details of the s3, two theory of consciousness is, you know, I'm, I'm a fan of traverse key and Kahneman and Gary Klein, etc. And Tversky. Once, when asked about artificial intelligence that he studied studies, natural stupidity. And what that drives home is, I have no delusions that even if I could replicate some of the characteristics of that sensory motor world model that adds moods in a motion, it's not going to solve all my problems. That being said, I do believe it's a source of great flexibility that can be that can be had for artificial intelligence. So what we were looking for is just a small simple set of fundamental laws, which capture the what it is like and and you know, the what it is like from a Thomas Nagel perspective is key. But from an AI perspective, we're also after trying to answer the question, why is it like that? So as I describe these characteristics of the conscious To experience, you know, from an AI perspective, we're trying to suggest, if we instantiate these characteristics in our AI representations, it might show us certain cognitive capabilities that we didn't have without these particular tenants without these particular laws, if you will, and thus, we could probably get some insights into why consciousness is constructed the way it is. So the first is that, that the world is situated I'll start with number two here. And by that I mean, you know, the, the concepts, the qualia, as the plural the singular is quality, the quality or in fact only have meaning with respect to how they're related to, and how they can interact with other quality. Okay, so the red I showed you, the Red Square, and I showed you there has it, it's only value is respect to other colors with respect to where it's at, with respect to when it occurred. And, and so all concepts talk while you're situated, okay, the fundamental unit of conscious cognition or situations. All right, the third one down here, I'll go to the next simulation is, it's a simulation. And again, as I'm going to have to read Stephens, new book, but but the idea that there's there is a decoupling between the world and in fact, even from your sensory data. In this conscious experience, this representation is decoupled from that. And in fact, we like to think of it as a as a simulation composed of qualia, but simulation, and thus even the things that I perceive my perception is an imagined present, it's not reality, what is out in the world, it is a simulation, which creates a representation, which for me, is stable, consistent and useful. And in fact, when I recall something in the past and bring it to my consciousness, it is in fact, we consider it is creating a simulation of what might have happened to me, not only in terms of experience, but in terms of everything that's happened to me in between. So I have imagined past it's not a replay of an AVI file. And most people can buy off on the fact that when when your consciousness is evoked, you're trying to imagine something in the future that you imagined that because you clearly haven't experienced it yet. So simulation is that is that is that tenet. Other people have talked about the idea of, of simulations, Larry Barcelo, for example, for example, down at Emory, it's a great source. And then the third s and the s three, two is structural coherence. And by that, what we're trying to say is when I reach over, and I grab this glass, because I'm thirsty, you know, there's, I consciously perceive its presence. And there's enough mutual information between reality and my conscious representation, to allow me to interact with it in a coherent manner. But it isn't just in terms of physical interactions, you know, my representations of color, if you look at what things are perceptually, similar on a conscious sense, colors, and you can create a color will with captures, actually some physical characteristics of wavelengths, etc, how the colors can interact, those can all be captured there, they're certainly represented in your conscious experience. And also in the, in the, in this structurally coherent world, this coherent world that's out there. And then the last Q, the Q, and s three two is qualia, we believe that vocabulary of the conscious experience or is made up of these is bottlenecked, reduced order view of the world, it forces the the forces nature to create a simplified model of the world to allow you to have this stable, consistently useful representation. And it's situated these things into these narratives composed of the qualia. So that's sort of the way we define consciousness. Now I'll just take a couple of minutes more to explain to you why we're focused on this, you can go to the next slide on you. It's all about remember, I defined representations as is how an agent structures, its knowledge. So the thing we've been trying to seek is, where does the knowledge come from? How is it structured, that causes the creation of this, this quality experience is consciousness. So if you look at Peter Dominguez his book, you know, he suggests that knowledge historically has either come from evolution, experience, or culture, and AI, we sort of follow the same route that is initially, all the API's that that we've been building, you know, we just put all the knowledge in it from the beginning. It's not unlike, you know, providing your DNA. What we found more recently is so by allowing our AI bots to interact with the environment, them to have experiences, capture those experiences, and store them as knowledge that's much faster and more flexible, in fact, than just pre storing everything. And then what we figured out is now that by with by letting machines interact with not only the data world, but the real world, both with autonomous cars and with aircraft, drones and spacecraft, that that in fact, most of the knowledge going forward from this time forward in the future of the earth, will be created by machines. Now, the problem I have with that is the knowledge that's being created by the machines are currently only usable for that machine to create the meaning that we programmed it to do, or if we're slightly creative. It's some variation thereof, but it's very limited applicability. You know where we're headed is is as we make these, Steven run get up earlier about Explainable AI and deep learning issues. You know, I'm not as much a fan of Explainable AI as Alignable ai ai that can align with not only the human but with the other agents it's working with, whether those are robots, or drones or cyber bots are my little Microsoft's you know, where I'm headed is when these machines can share their knowledge in a form that those those other bots can use to help them solve their challenges. That's machine culture. And that will be even faster than what we've done so far. And then my last slide, Tanya, if you go to that, this is this is my driver problem, if you will. So so as as we instrument the world more and more with our bots, the amount of available knowledge is growing exponentially, that sort of orangish, yellow, whatever color you see there curve is. And the problem is, is, as all my my I have a great team, we're filling great AI to solve real problems and a wage in a range of diverse tasks. But the problem is, each one of those applications is on the green curve. It's very narrow, it solves one problem, we structured the knowledge or we structured its experiences to do it. And so therefore, I have this gap. Jeff Jonas, who I got this idea from, he's an IBM Fellow calls this enterprise amnesia, that is on a given challenge, I have it all the knowledge I shouldn't be able to bring to bear to make that decision better. I can't, because I'm in some sense amnesic to it, I haven't created the knowledge in a form that I can exploit it at the moment I need it. And it's because of our approach to this very narrow AI. Now, I contend that I quoted the Melanie Mitchell on the bottom of this slide. You know, I'm not at all convinced our current approaches. And I think that's the purposes of Yoshis panel here is that these current approaches, I'm not at all convinced they're on the path to general AI, if there even is such a thing as general AI, if you go down the Francois chalet approach, maybe you don't believe it is such a thing. But certainly to make AI more flexible than it is, I think we got to break off of that curve. And so I'll stop there. Thank you, Tony, for projecting for me.

Joscha Bach 1:22:03
Thank you very much for this presentation, Steve. Let me share my own screen. Hope this is working, and Nashoba. I'm a cognitive scientist. I've worked in the area of cognitive architectures in the past trying to understand the relationship between cognition, perception, and motivation. And I'm currently working at Intel Labs, as a principal research scientist in the book of God using AR and I'm leading an effort in understanding cognitive systems.

Joscha Bach 1:22:52
The representations that we are currently using the majority of AI applications can arguably has been started with Frank Rosenblatt, perceptron architecture, and there are almost unchanged in many ways to his original work. And Frank Rosenblatt work has suffered a setback when Marvin Minsky pushed against it because he thought it's too simplistic. And we need many, many more structures and layers and so on to understand them and even wrote a book together with Seymour Papert, arguing that neural networks cannot learn even many simple logical operations, because he didn't see yet how backpropagation would be able to enable this. And now we are at the point where we feel that even backpropagation is probably not sufficient. And we need new approaches to representation and neural networks. And there are in some sense, three perspectives that I would like to present one is the idea of the neural circuit. And indeed, We can argue that the neurons in our brain form circuitry, and also that our neural networks in some sense, emulate functionality and functional division of circuits. There's, for instance, excellent work by open AI, especially Chris Ola scoop, that has been analyzing a number of computer vision networks and shows how the superposition of features leads to the representations and image net representations in existing neural network classes. And he argues that features are the fundamental unit have neural networks and correspond to directions and an embedding space. And the features are connected by weights forming circuits. And there's a universality phenomenon that you can observe that analogies features and circuits will form across different models. So if you take different neural networks and put them to the same task, and they have flexibility in forming their structures, well enough the architecture well enough, we're going to model another group features And this perspective of neural circuits corresponds to state machines in computer science. And there is another one that is the one that we can also think of the brain as something forming like a neural ether. So which saves of activation propagate. And the state that we are interested in is not the state of the individual neurons, but rather it's the state of the base of activation that has been propagated along the neurons, and perspective that people would be taking instead of brain as representing things very much like a synthesizer would do. So you start out with patterns at your systemic interface, and your brain is trying to make sense of them, by making them predictable. And if you know, in your will, how synthesizer works, you have a bunch of oscillators with potential meters that changed the oscillations, and you connect them in a certain way, with a little bit of experience, you figure out how to make almost arbitrary sounds in basically every sound that you want. And you can parameterize that is the generation of the sound by changing the oscillators. And this principle does not just work for the auditory domain, but you can also use it for spatial frequencies and for stationary frequencies that has colors. And you can make then metal oscillators that basically try to find the patterns between the oscillators. And when you work with synthesizers, you know how that works, you can basically create combinations of the different patterns and abstract for instance, the pitch out of a number of sounds as its own parameter, and so on and find principal components at higher levels of abstraction. And then you can basically merge these percepts until you get to dynamic simulations of environment and all the modalities meet. And you can even extract this even more until you understand the generality of all these simulations and get to conceptual abstractions to an address space of all the objects in your simulations to make sense of the world. And this perspective of the neural ether is best described from a dynamical systems perspective.

Joscha Bach 1:27:18
And the third perspective that I want us to look at today is exemplified by Jerome's work, which takes the cognitive representations as something akin to what we are describing in front of mechanics, this does not necessarily mean that the brain is a quantum computer, but that the mathematical formalisms of quantum probability are the most suitable to describe the way in which representations work out in the brain. That is, they don't necessarily in a definite state, but they can be in an ambiguous superposition, because they're constrained in a particular way. And you will have to collapse these ambiguities when you want to reason but otherwise, perceptual representations are very often in superposition, not state. So if you have this potential collapse of representations, you will have to have a bipartite mind that you have generative perception, which is distributed and geometric and ambiguous and just aiming for coherence. And you have an integrated attention system that in some senses is looking at this perceptual representation. And this integrated attention will have to be localist and disambiguating. And aiming for correctness. And the purpose of this integrated attention system is to fix perception where it doesn't work and to interact visit. So you can see a tension inside of the cognitive system acting as an agent, similar to a conductor in an orchestra that is interacting with the perceptual system, and is parameterizing the states of the perceptual system to collapse them into useful states. And so certain operations like reasoning, and planning and so on, can be performed on them.

Joscha Bach 1:29:04
So this quantum probabilistic brain perspective that Jerome has presented basically gives us an insight into dual process mind that you have an observer inside of the mind and face space. How can we create a unified, coherent model of the universe? And I think this question of how we can get to unified representations that are total that are universal, that are fully connected, are all the domains are connected into a cohesive model? That is the biggest, most important, unsolved problem with artificial intelligence today. And when we think about how representations work in general, how our model works in general, we start out with patterns. And these patterns are don't have meaning yet, right? They're the patterns that you perceive from the environment. They don't contain sounds and colors and so on. sounds and colors are function means that your mind is generating to make these patterns intelligible. There are hidden states, there are variables inside of your models. And a variable is a set of possible values. And these feature variables are related to each other because computable functions that's constrained some of the values depending on the values of the other variables. And they also contain constrain the next future states that we can observe, or they reduce the uncertainty and the next set of observables. And these relationships are not probabilistic, they are possibilistic, they represent which features are compatible to each other. So when you are representing a nose, and it has the certain posts, that is a certain direction and alignment in space, there must be a face nearby. This is one of the constraints that you learn that has the same direction and space, and that is directly adjacent to the nose. And if you do not discover such a thing, then you will have a constrained Foundation, maybe you're not looking at a nose, maybe you need to change your models, maybe the features that you took to be a nose are actually not a nose but something else. And you have to come to a different parameterization of your model. And to get find the right parametrization of your model, you need probabilities. So given the current set of constraint violations in the model and the current state of the model, there is a certain direction in which you should move the current model of what it's the case and this probabilistic model and these probabilistic relationships that you learn bias the model towards convergence. This is the reason why we perceive many optical illusions. For instance, the optical illusions are the result of such biases. And they also need to have Leyland's because not all representations. And not all uncertainty that we reduce has the same value, the reduction of uncertainty is expensive. And we need to have a way to evaluate what we should be doing first. So these representations that we present our mind are connected to motivation to set point generator that tells us which things are important at every moment. So the goal of the model is to predict the next state from the previous state. And each step, we try to find a configuration that minimizes constraint violations. This leads to a more coherent model. And we try to minimize the uncertainties to maximize the ability of the system to control its environment. At every moment, we have to do the most valuable thing. So we need to have an estimated reward. And this estimated reward needs to get distributed throughout the system. So all the components of the system, or the individual neurons are organized and self organizing in such a way that they globally approximate to the most valuable thing. And once we take this perspective, we can go to a developmental psychology and we now notice assimilation and accommodation notions product vision Piaget, assimilation is the modification of the model state to make consistent with the sensory input. This means we don't change the way in which we understand the world but we change our assumption of what's currently the case and accommodation means that we have to modify the model structure. So we can allow the assimilation of all sensory data. This means you have to extend the model, you have to change our understanding of the world.

Joscha Bach 1:33:14
And now if we look at our cognitive system, we noticed that there's basically a perception agent, this perception agent is richly entangled with the environment and situated in it and this perception agent is going to create a set of dynamic functions that predict the next set of patterns in your environment and its own environment, part of your mind is also the environment or the perception agent. And we have a motivation agent that is trying to evaluate what the system is doing in the world and generate impulses that tell you what signals anticipate a report and what you should be doing. And you have an attention agent that is interacting with the perception agent itself not providing a motive force, but it's directing the motive force in such a way that the perception agent is finding a representation of the world that makes sense at any given moment. And I think that this attention agent will have to have pointers to perceptual objects, the relationship between them and it will have to have a mode of attention that is are we looking at the counterfactual representation at a memory at an explanation or something that is currently sensory. So, you have the content awareness and you have the reflects the attentional awareness and you have reflexive attention that is the system is also ensuring that it knows that it is indeed the attention agent and the self organizing structure of the mind. And this attention agent will have to maintain an index memory, where the perceptual system is just converging to states the attention agent will have to have a memory of what it has tried before because it cannot just follow a gradient and it's discrete state of interpretations and this gradient This memory is going to be accessible as a stream of consciousness. And I think that this attention agent is responsible for different neurology of consciousness than the human mind. So, to sum up, what we have seen today are several perspectives the perspective of neural circuitry, which corresponds to state machines and the perspective of a dynamical oscillating system in the that you can describe as dynamical systems. And you've seen the perspective of a quantum probabilistic brain in which you have the mind divided into a dual process system where you have a space free space of possible things that could be the case, and the conditional observer, or the observer that is conditionally collapsing these representations into concrete interpretations of what is currently happening. And these perspectives are not necessarily in conflict with each other, there are different projections of this whole uncertain thing that happens in our mind when you represent ideas and knowledge and is this I would like to open up the floor to a discussion for the last remaining 20 minutes.

Joscha Bach 1:36:22
So have you seen several positions you have seen? Mark? Starting out is the problem of reference? How can representations point to two facts in the world? And I think that the possible answer that we might agree on is in the direction of we don't point to facts in the world, what we point to is a unified model inside of the brain, right? We don't live in the physical world out there, we live in a simulation that is generated by our own brains. And this simulation contains a coherent model of the world. I don't know if this is something that mark would agree with, and maybe you want to address this.

Unknown 1:37:09
Okay. I, I don't think pointer work pointers can sing it light some properties of reference and representation. But if you ask what is pointed to and how does the system know, what is pointed to, and how could it ever detect that the pointing is in error, you run into the same sorts of problems. The the, the, one of the questions that I left with, had to do with content. And I think that's what's relevant here. If there's an indication in the organism that a certain interaction might be possible, in general, that will be correct, under certain circumstances in the environment, and incorrect, under other circumstances in the environment. So to have an indication that it's possible presupposes that this is one of those environments that would be supportive. And in that sense, you involve a presupposition about the environment in the indication that some interaction is possible. presuppositions of that sort, are what I propose, as a model of content. It is those presuppositions that can end up being true or false. This might or might not be one of those kinds of environments that support it differs from standard encoding ways of thinking about this, including pointers in that there is no representation per se, there is no explicit content, about what it is about those environments. So that's going to be support support, the presuppositions are implicit, not explicit. And they turn on a notion of implicit definition of dynamic implicit definition where implicit definition really only got introduced into formal logic in the late 19th century, and generalizations of implicit definition to dynamic frameworks is even more recent than that. So what I'm claiming is that yeah, content is what we want to be talking about, and the sense in which content is presupposition gives a dynamic way of doing that. But it gives us content that is implicit rather than explicit, it gives us content in the sense an indication of a potential interaction thereby presupposes that the environment will in fact support that interaction. So you can see a bit of, you know, intuitive convergence there with pointers, but pointers have to be explicit, they have to point to something. And that explosiveness encapsulates all of the basic epistemological problems or so I would claim

Joscha Bach 1:40:01
Thank you bar. Steve, do you find that our current discussion is something missing, missing something important? And when you are trying to look at representations as they're currently done in AI systems? Is there something that people should be looking at when they are building representations of the world and of the actions of the agent or of the agent itself?

Unknown 1:40:32
You'd like me to respond to that.

Joscha Bach 1:40:35
Yes, please? Well,

Unknown 1:40:38
in my brief summary, I spoke about thought experiments. You know, when you can do a thought experiment, that means you've been lucky enough to get sort of to the foundations of the subject. Einstein, as you know, the ride both special and general relativity, from thought experiments. My thought experiments are about how any system can what honestly correct predictive errors in a world that's changing, and not under control. I didn't start by thinking in this generality, but when I didn't start with thought experiments, I always start with data. If I have any talented seeing, to the heart of data, the meaning of data and being able to take hundreds of data curves and converting them into dynamics that can then explain and predict those data and many more data as emergent properties of the dynamics with these thought experiments. The first is cognitive. How a cognitive system can what economists we learned to correct predictive errors. And that leads to adaptive resonance theory, which I think, has successfully explained how our brains learn, to attend, recognize and predict the objects and events in the changing world. There's also a thought experiment, which leads to cognitive emotional models, where emotion and motivation play the role of constraining your predictions based on what's valuable to the individual in a given context and at a given time. So that all the foundations of cognition and cognitive emotional dynamics can now be explained as necessary consequences of universal thought experiments where you don't need fancy data to derive them. The hypotheses are trivial facts that everyone accepts, from their daily life, but they act together and our environmental pressures or constraints on the evolution of our minds, to like, you know, to derive cognitive emotional dynamics that among other things, years ago, explained and went beyond explaining Kahneman and Tversky data and had nothing to do with decision making under risk. It was foundational, cognitive emotional dynamics. The hypotheses are more or less that I'm trying to learn that a predicts D A predicts B, I could do it within a range of time delays. No one would doubt that. And secondly, that that learning is possible. And that leads to foundational models that can explain comments versity data, as well as a wealth of other data about normal and abnormal cognitive motional learning, or recently, what goes wrong in our brains leading to behavioral symptoms of Alzheimer's disease? What is amnesia, post traumatic stress disorder. Attention Deficit Hyperactivity Disorder, visual and auditory agnosia problems with slow wave sleep. And I say these two as a challenge. If you don't know this, and you are believing that you're saying something about how our minds embody intelligence, then you're simply not competitive anymore. I grew up in a traditional, scientific mathematical environment. And

Unknown 1:45:43
you know, the people who could explain and predict the most stuff, those are the people you have to study. I still believe that. And while I'm alive, you might want to exploit me, because just the remarks I've made now, you can't get around them. If you believe these trivial hypotheses, you have to believe the theories or give up your faith in logic and reason. And the scientific method, therefore, many of you willing to do that. Thank you, Steven. Let me say one more thing, my new book. You know, I've been working like a dork for 64 years. And to write compelling articles. in peer reviewed journals, you have to throw in all the technical details that reviewers will accept. But my new book is over 700 pages written in a self contained non technical conversational style, as a series of stories to the general public. So if you're ever curious about these things, now is the time to find out. And I also made it possible to sell this big book for less than $30 in hardcopy on Amazon, and $19, in Kindle, so it's affordable, it's accessible. And you can't get around things like thought experiments, no matter how you may wish to

Joscha Bach 1:47:32
agree. People should read your book and many pointers that are in it, I think that researchers are many. So neglecting, especially for students in cognitive science, think it's important to get the next generation into these ideas. Yo, yo, do you want to respond to this? Or do you want to tell us why it's necessary to have recurrence of memory is not reliable, it didn't understand that particular point that please feel free to respond to anything that you think is pertinent.

Unknown 1:48:05
Okay, so maybe I will take this idea of the thought experiments. And I would like to maybe ask Steve back, but I will just state my state, and maybe Steve can get an answer. So we see value, I saw value in my research of not just thought experiments, but the actual experiments of trying to know first, simulate the models that were built to see whether the behavior is what we expect, but then at the end to actually instantiate them in this embodied agents, and to see whether I can actually generate behavior that unfolds autonomously and can detect those errors. What does it mean? What does it take with real sensors that know might have certain temporal characteristics or certain certainty uncertainty characteristics that I might have not thought about when just thinking about my my model as a thought experiment? So I just wonder whether Steve sees that there is value in such real world experiments, or whether we can stay in only in thought experiments?

Unknown 1:49:08
Well, our models started getting embodied in mobile robots over 40 years ago. First, people that MIT Lincoln Lab developed several generations of mobile robots. And I'm all for that. But I also feel very much I don't know if you remember the shamans story when I think the first time the astronauts came down to earth in their capsule and they landed safely. The first thing was it Neil Armstrong said when he got out was the bump was there, the bumped in there to stand And that was a scientific mathematical prediction. And everything that I've done with over 100 gifted collaborators is rigorous. And we have simulated many hundreds of experiments in quantitative details showing that things work the way we say they do. And in fact, before computers were able to simulate data that may include the interactions of millions of heterogeneously, organize ran circuits, I had to face my fears and learn how to prove global limit and oscillation theorems. And in fact, in adaptive resonance theory, Gail, carpenter and I, and a number of us students that prove theorems about how our models are one or two or two way or three, these are incremental, the fuzzy yard, fuzzy ArcMap is really how these things actually learn, classify and predict in unpredictable environments. So there's a rigorous foundation here, the bump will be there. If you want to build your own rubber, but others have preceded you by maybe longer than you've been alive. I'm not sure if they're willing

Joscha Bach 1:51:30
to cut this short, but you're almost upon the hour. I know that

Unknown 1:51:34
thing about robots. funding needs to be sustained for a while to generate an effective robotics program. What we found, given our university funding was that oh, yeah, in the university, the grant would run out before you could really deploy the robot in an industrial setting. That's why we were so glad that MIT Lincoln Lab ran with the banner because they didn't have to work in the university, although unfortunately, a lot of air grants only lasted for three to five years, too. So the key to this development is getting sustained funding, that's not a victim of every change in political administration.

Joscha Bach 1:52:29
Before we end, I would like to ask Steve orders, please. Do do you think that computers will have quality as soon and that we can make models now already, or there's something missing before we are able to build a system that has the equivalent of phenomenal experience?

Unknown 1:52:51
Well, in my own work,

Joscha Bach 1:52:56
there was a question to Steve Rogers, obviously, Brian? Yes. Because he's about to drop out. I know that he has another meeting coming up in a minute from now. So

Unknown 1:53:08
yeah. Steve Grossberg. It's great to see I haven't seen you since two years ago, I was in your house. I had the pleasure of hearing Gail and Jerry both small. It was great to see you too. To answer your question the OSHA. As you know, you've been you've been interacting with us recently, I believe strongly we are, we are already seeing benefits from taking engineering characteristics of the quality representation and putting them into AI solutions. I do believe we will come up with what I'll call quest consistent phenomenological experience that is, it will not feel the way you and I feel or the way my puppy dog feels, or or anything else. But it will, it will extract an engineering advantage from from replicating the phenomenology because I do believe strongly that consciousness is not an epi phenomena. It's in fact in the in the decision cycle. And you're right, I have a commitment with NASA and right now, so I'm gonna sign off and I will listen to the video later for anything I missed. And it was great to see you guys.

Joscha Bach 1:54:12
Thank you very much. I would like to thank all of our speakers today for participating irritation. I would also like to thank intellects and Gaddy singer for making this event possible. I'm very grateful that we could make it happen. I would like to ask Tatiana Greenberg for helping is the organization and I am very, very thankful that this happened. We have recorded today's session. I hope that the recording has worked out and we are aiming to make it available soon on the femoral website. With this, I wish everyone a wonderful day, evening, night afternoon wherever you are in the world. And we hope to see many of you again in the next Seavus of This panel Thank you very much

Unknown 1:55:03
Thank you Yasha for organizing the hill thanks it was a great pleasure
