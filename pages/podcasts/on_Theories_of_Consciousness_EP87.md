Unknown 0:00
Howdy this is Jim rut and this is the Jim rut show. listeners have asked us to provide pointers some of the resources we talked about on the show. We now have links to books and articles referenced in recent podcasts that are available on our website. We also offer full transcripts, go to gym rat show.com. That's Jim Rudd show.com. Today's guest is Yoshi Bach. He's Vice President of Research at the AI foundation. Previously, he's been a research scientist at MIT and Harvard. He's the author of the book principles of synthetic intelligence, sigh, interesting kind of joke at multiple levels, and architecture of motivated cognition. And he's published many papers. I should also point out he has one of the most interesting and entertaining Twitter feeds I know of at purlins, PLI N Z, check it out. This is the second time we've had Yoshida on the show. Last time, we talked quite a lot about AI. We talked a little bit about his micro sigh project. Today, we're going to kind of split to the very high side and then get down to some details on microsites, I think we're going to start off talking about the biggest of questions, or one of the biggest questions in this field, is how does mind arise from brain? And can it arise from an artificial brain? You OSHA?

Joscha Bach 1:33
I think it can, obviously. And so the question is, of course, what is mine. And there, sometimes we need to clarify our terms, not so much to replace them, but to make sure that we are talking about the same thing. So typically, when I say mind, I refer to, essentially the software that runs on the brain, and that enables everything else. And what is software, I think that software is best understood not as a physical thing as something that has an identity. But as a physical law, it's a very specific physical law that says, Whenever you put things together in the universe, in this particular arrangement, the following thing will happen. And the following thing is a description in terms of certain macro states that have causal structure. And in that sense, you could say that, for instance, the text processor in your computer doesn't have an identity. It's a physical law. And a physical law says that when the gates in your computer are in this, this arrangement, they can be described as the text processor. And when you interact with it, the following thing will happen when you when it's currently in this in this state. So the same category of things is mind, it doesn't have an identity. It's a principle. It's a software that runs on your brain. And it doesn't run on your brain because it is corresponding one to one to a configuration of atoms in your neurons. But because there is a coherence causal structure, emerging over the activity of the neurons that you can use to describe it, it's a lens to look at the activity of many neurons. And there is not a reason why this should happen, should not happen and other substrates that are under the same functional constraints and can implement the same principles. The same way as we can implement software on many, many different types of computers. As long as brains are Turing complete, you can build minds on different substrates if you are implementing the same causal principles.

Unknown 3:35
Of course, people would argue that human mind the human consciousness, and we'll make a distinction later about consciousness and mind, but for the moment, keep them together, is strongly embedded in its bodily substrate, and that, in fact, some people like Antonio Damasio would argue that the actual root of our conscious being is deep in the brainstem, and is more related to our bodies, actually, than it is to our higher brain. And if that's the case, then while we could create artifacts that were analogous to the mind, in some sense, they essentially can't be the same if they're not embodied in the same kind of structure that the human mind is embedded in.

Joscha Bach 4:16
If I change the interface that you have to the universe, so you are situated a different way, for instance, you are not no longer located in your body, but you are connected to sensors all over a city. And your incentives are aligned with the incentives of the city and you also identify with the schools and so on, you might turn into a city at some level, right? And this still means that you have a mind even if you change your affordances. In the same way if I limit your affordances to things that are internal to your own mind as a teapots, for instance, your wing dreams at night, it's not that you lose your consciousness in that moment. And I think that you can also be perfectly conscious while playing Minecraft and even if you're embedded in In a VR, that only gives you access to Minecraft, that probably has enough complexity to this loss of generality be sufficient to be conscious and to form a bond to the external world, you can take Minecraft and implement this on a chip, we can implement this on neural tissue, in principle, at least, and implement this visit the brain itself. So you never really need to leave your brain and go into your physical body. In order to have the experience of the body this is your embodiment can be entirely virtual. But you need to have as an implementation, the implementation needs to have a realization on the physical substrate layer of the universe. But it doesn't mean that you need to have a physical body that is exactly human like you only need to have human similar affordances. If you want to end up as a human like mind.

Unknown 5:50
Yeah, the old brain in the bottle argument, right. But particularly your own work has put a lot of emphasis on emotional states and emotional valence is, et cetera. And there's a substantial school of thought that says that the reality is that the physical reaction of emotions such as heartbeat, and respiration, skin tone, etc, actually happened before the feelings ie the cognitive states associated with emotion. So you'd have to also essentially have inputs that directly at least for humans, if you wanted to try to extend them to a non body, state brain and a bottle, you'd also have to essentially fake out the physiological emotional responses wouldn't you

Joscha Bach 6:33
probably noticed that consciousness is not in charge, then we control our interactions with the environment. So meditators say that consciousness is like this little monkey sitting on top of an elephant. And it can direct the attention of the elephant by providing it. But eventually, in the at the end of the day, the elephant is still going to do what it wants. And if the monkey thinks is in charge, then the monkey is often in for big disappointments when it notices that it makes decisions, but the elephant is not reacting to them. And this elephant part of our mind seems to be much older, especially with respect to the high level concepts that the monkey is developing to analyze its own behavior. The attentional control system is old. But the analytical systems that the monkey employs, as a result of its analytical and attentional reasoning, and so on, they are rather young. And I think that feelings are a way in which the emotions and motivational impulses of the elephant are accessible to the analytic monkey. And so in this sense, you could argue that there is a lot of stuff that precedes the feelings because there are ways of communicating between subsystems in the brain. It's the perceptual system that does the majority of operations and something like a distributed neural network, this nearly continuous dynamics, and then this analytical grammatical engine, how do you interface these two systems, and you basically need to take the features that are being computed in this distributed perceptual architecture, and translate them to the localist discrete analytical model. And you do this by projecting them into a space and the only space that is consistently available is the body map. So feelings are projected into the body map to disambiguate them. You could also take the perceptual space just outside of the body mat, but this changes all the time and we would not self assign it. You could also have a specific emotion space that is distinct from the body map by Why don't we have feelings outside of the body? It's a weird thing. And I suspect it's because feelings have been implemented as an afterthought and evolution so they had to be mapped to an existing brain region. So when you feel love in your heart anxiety in your solar plexus is not because your gut is involved in these computations, your gut is completely occupied with hurting bacteria. I think that's simply a projection that happens. And if you are a paraplegic, and you don't have access to sensations in your body, you still feel emotions in your body.

Unknown 9:08
Yeah, the linkage between emotions and feelings is interesting. And at one level, you could say they kill both ways. You know, again, there's been some very interesting psychology lab experiments where, for instance, like one of the more famous ones is a person who's walks across a dangerous bridge versus a non dangerous bridge and then meets a attractive member of the opposite sex who asked him some questions for a questionnaire and the probability of a male to ask for the questionnaire person's phone number to ask them for a date is way higher if they go across the risky bridge than if they go across the not risky bridge. So in sense, the bodily sense of fear or challenge, operating unconsciously, well below the conscious mind, through the body itself seems to program the mind itself into a different state. Yeah, it's

Joscha Bach 10:01
interesting to speculate why that is. It's simply that an achievement to deal with subjectively life threatening situation leads to higher sense of competence as the size theory would predict. And it's basically would lead to greater risk taking behavior in the case of that agent, but to see if that is actually the reason we would need to look into this in much, much more detail. I've read about these experiments myself, and they haven't really made up my mind what the actual cause is, and but which systems crosstalk, and so on. Why is it that for more easily enough, after we have been in a life threatening situation? I don't really know. Even though I can easily come up with a few hypotheses, which is the dangerous thing.

Unknown 10:46
Yeah, exactly. Well, why don't you let's do it, come up with one hypothesis, love to hear the OSHA Bach take on that famous result.

Joscha Bach 10:53
Yeah, so the easiest hypothesis would be if you enter, basically, Master risk, then your willingness to deal with future risks increases momentarily, because you will have a higher self ascription of competence. It's also the case that you put lesser value on the perception of risks that you would normally fill your attention, basically, you normalize risk. If you are, for instance, find yourself has cancer in hospital, and you are threatened with possibility of death, then all the things that normally would give you existential fear, like losing your job, or losing the friendship of a person that you don't particularly like, and so on, suddenly are no longer important. And as a result, you might start ending relationships that you're no longer interested or asking somebody out that you would never did asking out, because the risk is just not that high compared to the actual existential risks that you have just been facing. So it's basically a rescaling of your perception of what's important. And what the valence of the important things is.

Unknown 12:00
That makes a lot of sense, actually. And again, I know in your architecture and other architectures, those states decay relatively rapidly. So and that's why it may manifest in that particular experiment were more or less immediately after having a challenging experience. So the test is given if you came back two days later, might not be the case,

Joscha Bach 12:19
yet. So in some sense, the prediction would be that for most people, that is a range of emotions, that adapts to the situation that the agent is in. So you have roughly the same emotions and roughly the same proportions, as long as you don't suffer physical pain or actual threats day to day, that put you under real stress. As long as you're existentially safe, you feel roughly the same things that most of the times, and your emotions will adapt to the range of events that happened to you. Right, so it's a normalization that happens in this way. And if you're introduced new events into the stream of events, or if you change the stream of events, then for instance, the amount of anxiety that you experience about things might still be the same, even though the events are quite different. In which means you assign different, different valence to this same events, if they are in conjunction with others. It's probably an interesting lesson, therefore organizing your own life. So for instance, as many people suggest that you always make sure that all ages you will introduce a lot of novelty into your own life. So you will always remain young, you always store new experiences and information and put value on them. And that you will undergo frequent changes of your environment.

Unknown 13:41
Yeah, I think I've been pretty good at that in my life. I'm always getting into some shit or other right, I never say never stand still. Right.

Joscha Bach 13:48
Congratulations.

Unknown 13:51
Let's see what else we're talking about. Next. Get your thoughts on some of the other theories of mind? I'm sure you have them. One theory that was prominent, I don't know 20 years ago is that mind was essentially a brain wide set of interlocking frequencies, some of them phase locked, and some of them rhythm locked. Any thoughts on that theory?

Joscha Bach 14:12
So when I was confronted with singers theories about that, that consciousness is something like a frequency in the brain that was also adopted by Christof Koch. I didn't understand it. And it was mostly because it didn't make functional sense. It didn't explain how this is necessary and sufficient for producing the phenomenon. I thought that oscillations in the brain are necessary because humans are not able to be in a constant state and make that constant state to each other they need to fire if they are connected in some causal structure, they need to fire in synchrony, right then we'll have to be an A wave of activation that passes through the brain this wave will have to be periodic. So you will look at cyclic activations in regular frequency And these regular frequencies that if you can pick them up on EEG, it means that a large part of your cortex are firing and sync, so they don't disturb each other. And you can see a common signal. But the signal would be the result of the synchronization that leads to consciousness. It's not that the synchronization itself causes consciousness. And is on another hand, some interesting insight from the perspective of neuroscientists who thinks about how is it possible that neurons have the binding problem? How is it possible that they talk to each other at all. And then you can think of neurons as something like for instance, like a neural letters, that is acting like an ether. And the signals that are being broadcast to the brain are moving like electromagnetic signals would move to an ether? This is a model that you can use as a crutch to imagine what's going on. And if that happens, the question is, if the signals propagate to this neural lattice, how do the individual neurons tune in to the different programs that are being played out in the brain and parallel, and it's tempting to think that there is something like our am or FM encoding, which basically some encoding where the neurons know at which frequency to tune in to become part of a certain computation. And maybe this is a viable model. So it's a viable model for how the brain solves the problem of signal transmission and synchrony of different processes over large distances in the neocortex. It's probably one of the mechanisms that plays a role. But it's not necessarily the best metaphor, or the best engineering principle for architecture that you would implement in a digital computer. Because in a digital computer, we can have random access that we can relate every single regardless of spatial proximity in the computer. Because we can have a bus with a very high throughput of data. And the individual memory cells in the computer can hold arbitrary values without any need for installation.

Unknown 16:55
Yeah, no, I always think that's an important point when you're trying to map from the biological to the machine. You know, Mother Nature solves problems in a very odd way, you know, look teeny step at a time through evolution over billions of years. And it's amazing, this stuff works at all. And if we're gonna design one from an engineering perspective, in many cases, we would make very different decisions that we might get some functionally similar results back to Brain wide phenomenon, one of my favorite cognitive models, and you know, he very honestly says, I have no idea how the mechanism works is Renard bars global workspace theory, where he believes that the representation of the sense of consciousness for instance, the sensorium, that we live in the movie that we live inside in is somehow broadcasts to wide areas of the brain and the various functional areas of the brain are then able to process that information. What are your thoughts about bars and his global workspace theory, even though he himself admits he has no neurological argument to support it? I think

Joscha Bach 17:59
it's partially born from intuitions that are the result of introspection. And I think that introspection is not given enough credit is an important tool in neuroscience and philosophy of mind. And psychology, there is no real methodology for introspection, I think it would be great if you could develop that further. And it does exist in the different of meditation schools, of course, is an important tool. I think that we don't distribute consciousness across the brain into the different areas. But it needs to be the other way around. The core feature of consciousness is that we remember what we paid attention to. If you don't remember that we paid attention to it, then we are not conscious of it. In hindsight, right. So it needs to be integrated into a common protocol, where it can be accessed later on. And this integration means that it's somehow local. So it's the localization of information that before existed in a distributed way in the brain. So it's the protocol that pulls from different regions of the brain. It's interesting to contrast this and to compare this, this the notion of attention in machine learning that is currently gaining and prominence.

Unknown 19:13
Yeah, that is interesting. And in my own work that I do in the area of scientific study of consciousness, the tension has always been absolutely key in my own model. And as you say, it's quite interesting that in the newest neural net architectures, they're using something they call attention, it isn't quite the same, but it's at least analogous

Joscha Bach 19:31
is something that is very close to the attention that we have in our own mental attentional learning. The difficulty is, how can you train a system that is not organized into neat layers? Our brain is not assistance was organized into neat layers. So we could train it as a whole using back propagation with Stochastic gradient descent. And even if you could, it would not be the right way to do it because it's not very efficient, right. It's takes an enormous amount of time to train it. Since then many many operations to get it to do the same thing that a human being can do. So, arguably, human being is much, much quicker and converging on a model of interpretation of visual stimuli and the neural network is,

Unknown 20:15
in fact, I use that as a very important probe in my own work, which is the way the human brain does, it must be rather different than gradient descent or other similar methods that are used for learning in neural nets. Because I may have mentioned this in the last show, one of the things that really got me interested in thinking about this problem of learning mechanisms. That's when I was playing a very advanced than difficult war game, I don't know, this was about 2013, or 14. And I realized that I was able to learn the game well enough to beat the pretty good AI really quickly, like after playing seven games, and I realized that was using transfer learning from the other times, I'd played war games of different sorts. And I started adding it up every time that I actually played relatively sophisticated war games to a conclusion of a game in my life. And the highest number I could come up with was 5000. And probably the real number was something closer to 2500. So that remarkably tiny number of examples of analogous games that I could, presumably mine for transfer learning, and then apply it to this new game, and then learn to play it at a level of good enough to beat the AI after seven games. And we know that today's artificial neural net architectures, not even close, even the new good stuff, like Alpha zero, still take hundreds of 1000s to millions of planes to learn even a very stereotype game, chess, or go or something like that. And these games are vastly, vastly higher dimensional space and those games so high, you can't even contemplate how high they are, I figured at least 10 to the 60 of move possibilities per turn, just to give a rough sense of it. So yeah, I think that the next frontier in thinking about learning and cognitive systems, and what we can learn from humans is that the rate at which we learn as animals and not just humans, of course, animals learn very rapidly as well have to be somehow fundamentally different than what we've seen so far from the world of artificial neural nets.

Joscha Bach 22:14
We can learn things super fast if we can pay attention, right? The difficulty is, how can you learn how to pay attention in the right way, there is a famous video on YouTube where a coach trains somebody who doesn't know how to play tennis, to play a relatively decent tennis within the space of half an hour. And this is because he's able to direct the attention of the subject in very great acuity and tells her exactly what to pay attention to. So she quickly updates her behavior in the parts that count. If you don't know what to pay attention to, you have to in some sense, brute force the problem by applying the arrow function very broadly in your system, and hoping that the errors accumulate eventually, in those parts of the architecture that make the biggest difference, but it's a very wasteful way of doing it. When I was in New Zealand in the 90s, I've worked at invitons laboratory, it's the same one in which Ben Goertzel studied for a time and chain leg of DeepMind, which is a happy coincidence, we never met there. But in Britain, I think has a great and underestimated influence on modern artificial intelligence, he is the father of arithmetic coding. And he takes a data compression perspective on cognition and while he never permanently took a stance in terms of cognitive science, his main motivation was to understand how minds work when he studied these issues. And so, when I was working for him, he put me in the lab and asked me to find structure and language automatically. So what I did was first look at Engram statistics and I realized that adjacent words in English language, there are so many different words in the English language, that you cannot really do good statistics over for instance, trigrams, or quadrille grams, or pentagrams. You mostly are restricted to by grams, two pairs of words. And this means that for instance, you'd lose the predictive power that an article has for a noun if there is an adjective between the article and a noun. And if you have multiple adjectives between the article and the noun, the earlier adjectives are not going to predict the noun anymore, so you forget your survey, most of the structure. And our own minds don't work like this, they do identify the structure over the entire sentence. So how can I discover the structure? How do I do the statistics? And I started using ordered pairs of words that didn't need to be adjacent and try to go beyond that. It was very difficult because you have such large statistics that you do to do on in a way, you need to do curriculum learning like our own learning that so we give children very short and simple sentences that are short and don't require a lot of pointers to maintain, so the child knows what to pay attention to. And then we go to the next one. And the child dismisses sentences that are too complicated to understand for the child. And if you try to do this in your learning system, you typically don't have these nice data sets where a human being interacts with the system and teaches it language as we teach our children and make sure that the problem is learnable. So instead, we need to have a mixture, where the system is trying to have candidates for meaning from complicated sentences, and then build up from there. And back then the memory that I had in this computer and the cleverness of the algorithm that I had available, were very, very limited, I was able to discover grammar eventually. But I was not able to discover style and the way SgPT three can do this. And when I looked at the transformer algorithm that underlies GPT, three, it was fascinating to see to me that they basically found a solution to the same problems that I had to deal with in the 90s and couldn't solve by myself. And this is to basically make statistics over all the parts in the working memory, and find out which ones of those are related to each other. And it's still too simplistic because it's a fixed working memory window, it's not able to sort things out of its working memory and put things in it. Instead, it has 2048 adjacent tokens in the working memory no more not less, in this implementation that are used in GPT, three. And so for instance, you can use this algorithm to interpret images, but only very, very small images. So those that don't have more pixel elements than those that fit into this fixed working memory. And so this algorithm in its current form is not able to comprehend large images or video, it's not able to relate the early parts of a book to a late part in a book, it's only able to keep two pages in memory at a time and relate concepts visit two pages to each other using massive computational power.

Unknown 27:07
Yes, I played a little bit with GPT. Three. And it's both remarkable and frustrating, right, you go, wow, what it can produce in terms of, you know, emulation of styles and plausible completions, etc. For, you know, as you say, you know, and it's within a small domain, and I frankly, I see it not working well, though, if you try to get it to generate 2000 characters of text, you're pretty much out into garbage land pretty quickly. But you know, in a shorter frame two or three sentences, it's remarkable. And yet, and yet, it really doesn't have any language understanding, it's just, it seems like it's just the biggest, so far collection of associative statistics,

Joscha Bach 27:47
I think we have to give a little bit more credit. So I wouldn't say that it's a real shot at ATI, at least not in its present form. But it is able to understand certain things, or at least consistently act as if it understands it, which means I can give it a task to today that are given the task is weird, right? It's all autocomplete. So I give it the beginning of a text about the completion of the task. And then it's able to continue that text. So for instance, I could say the following is an extraction of sentiments from paragraphs, then you have give it two examples. And now I give it new paragraphs, and it's going to extract the sentiment from these paragraphs. And if that is simple enough, this system is quite reliable. So I can ask it, for instance, to have one of six key emotional categories in a part of speech. And whenever I prime the core request to it this, telling it that it's currently continuing with text that is extracting the sentiments from paragraphs, it's going to do that just fine. And that's in some sense, a semantic operation. It does understand, at some level, what it means to extract a sentiment from a paragraph. It's only not related to a global model of the universe, a global sense of meaning that we have, right? When you and me understand something, it means we have found its place in the universe of that thing. Of course, the universe that we are talking about here is not the physical universe, which you and me probably don't really understand. Physics doesn't really understand it yet. Or at least those physicists, which don't understand it, yet have not found a majority that lets everybody of us understand it in the same way. And so what we mean by the understanding of the universe, or the universe itself is our model of the universe that we have in our own mind. And this model of the universe means okay, there is this roughly three dimensional world that has gravity in it and four forces and most of the interesting juicy stuff is electromagnetic, including light and sound. And it's all organized in aggregates of matter that we can interact with under certain way and it implements us and we are contained in it. And it's implementing the following set of mechanisms more or less, right and in this big, interconnected model. connects everything was everything about the University of your part of the fighter relationship, the causal structure that explains how that thing comes into being in a certain way. And gptc is not doing that it does not have an idea about the universe that it's part of. Instead of, it's just babbling. But it can create plausible universes to some degree, it's not fully coherent. At some point, the coherence falls apart, and much faster than it has in our own minds. Most people are not fully coherent right? I think maybe no human being is fully coherent. But the incoherence of GPT three is much, much more blatant than ours. Maybe we can improve this by putting a coherence loss on its learning, instead of just a surprise, minimization loss.

Unknown 30:46
Maybe. But of course, what I think you're talking about here is the famous symbol grounding problem. GPT. Three doesn't have its symbols grounded at all, no, it has to be grounded in language. Yeah, just say the red self referential back into language, it's not grounded into

Joscha Bach 31:01
its own language. But it does have a proper grounding in language for many of the operations that it performs. So when it's able to do two digit arithmetic quite reliably, there is in some sense, a grounding by treating the symbols that it manipulates as symbols that are subject to arithmetic operators that are properly evaluated, right. So as a functionalist mathematician, you should be satisfied. It's not mapping this onto the same understanding of X mimetics, as proper mathematicians do it. But arguably, not all mathematicians do have the same understanding of, of mathematic. axiomatic this doesn't make them completely important as mathematicians. Yeah, though,

Unknown 31:43
of course, we know that GPT three, while it does fine on two digit starts to fail at three.

Joscha Bach 31:48
Then again, it's trying to predict likely texts, whether or not that many human texts that have good arithmetic overlong numbers, because we don't write text about this is not a human activity. And so it would be in human to do that.

Unknown 32:03
Yeah, does not understand, for instance, the idea of the algorithm to do the arithmetic. Right. I one time as I think it was just for fun, I wrote the algorithm to do multiplication, manipulating text strings, right, just because I wanted to make it clear in my head how to do it, it took me a day to write that it wasn't that hard. But GPT three doesn't have anything like that it has no ability to generate anything like that.

Joscha Bach 32:28
If you ask a machine learning system to find a decently short state machine that explains all the arithmetic that we throw at it, then the system is quite likely to discover the operations if you wanted to discover it quite quickly. There are deep networks, which are able to do better integration and differentiation than Mathematica and able to solve things in a short amount of time. Or things that can that Mathematica symbolic system that is handcrafted, using proven mathematics cannot do, right. So it's able to learn things that mathematicians can do. But mathematicians don't yet know how to do in the right way. And it can learn a few things that mathematicians maybe cannot do yet. And DBT three is just not trained for doing this. And the implication might be that conferencing is wrong, that the free energy minimization which comes down to surprisingly, minimization translated into the terms of physics is not the right principle to build a mind, it could be that use could still say that you can minimize the appraisal better if you are able to assign the right value to your meta learning algorithms. But you cannot make those proofs if you're a simple algorithm. And I think that's the reason why simple algorithm organisms are not using a single principle to organize themselves, but have multiple needs. And these act as reflexes that satisfy many, many dimensions of needs of the system and the environment. And they are evolved. And we can probably supersede most of them, their reflexes. And Though

Unknown 34:03
interestingly, and I think this is important when thinking about how to get the AGI is what I'm getting one of my own little design principles is that when you look at things from a symbolic perspective, at least, attempts to solve these high dimensional problems, and even a very relatively simple organism has to solve, inevitably run into the combinatorial explosion of options problem. One of the things about these simple minded algorithms is they essentially collapse all that and I'm relatively convinced that our attention algorithm is essentially a hack to get around the combinatorial explosion of options problem. Let's move on to another topic. We're talking about ways of thinking about how mind emerges from matter. Another one that become popular over the last eight or 10 years is to note ease integrated information theory and you know, Christof Koch has also been a supporter of that, you know, it's a head scratcher. It's kind of interesting at one level, but it's relatively easy to create high scores that are clearly not a mind, what do you think about the integrated information theory with respect to mind emerging from matter?

Joscha Bach 35:07
I think if you go to the workshop of the integrated information theorists, it's a little bit like going to Climate Analysis conference. The fascinating thing is you look at the conference program of the people that don't believe in global warming is that they don't agree with each other. There are some which will argue that there is no global warming, others will say that global warming is not manmade. Some will just point to errors in the way that the mainstream of the scientific community talks about global warming, and so on. And they all accept each other's papers. Why is that even though they are so fundamentally not on the same page, there is much more disagreement between them than there is this agreement that they basically have as the mainstream, and it's because they are defined by the opposition to the mainstream. And the same thing happens with these workshops about the IoT theory, that is the information integrated information theory. They disagree about how to compute fire, for instance, this measure for integrated information on what the status of fires on how relevant it is, on how to interpret the concept of integrated information in the physical universe, and so on. They are mostly opposed to functionalism. And they try to find an alternative to functionalism. And I suspect that's the main driver for the popularity visit the IT community. Within the physics community, it's probably the reason that Max Tegmark has decided to make this one of the his planks in his boat that tries to integrate over everything in the universe from the perspective of a physicist. And so in some sense, the adoption in physics that this is taken seriously is probably a little bit political. I have great personal respect for Tononi. He basically is a neuroscientist that earns his money and respect as a sleep researcher. And he is also a philosopher and an autonomous intellect. So it's somebody who is not part of a particular school in philosophy, and as such would be a hard time being accepted in philosophy. He has a new solution to an age old philosophical question. And if that is the case, it's usually bad news. Because all the good answers that you can find without labs have been around for a very long time, you just need to find and translate them into the modern language or into your own concepts. And so if he has found a new solution, the question is, what is the solution for what is really express and this suspicion that I have is that the core of his theory is not really been published, that was also the impression that I got from personal interaction. The core of his theory is not fine. It's not this measure. This measure was introduced as, indeed to produce a theory with quantifiable statements that can be mapped to experimental predictions, because that's somehow seen as the gold standard in the sciences, but not in philosophy. For him, it's more an attempt to find a theory that explains the talks about pen psychism is reintroduces it. And it's mostly because he doesn't see how functionalism can solve the problem of consciousness. And he does latch on to this idea of a distributed information in the brain. But the distributed information that happens in the brain is probably not the same thing as it happens, for instance, in quantum mechanics, and I don't think it's non computable. And it's a little bit ironic if you try to make information theoretic theory and fires an information theoretic measure for consciousness that is not functionalist, because functionalism information theory are strongly intertwined. It's an oxymoron to make an anti functionalist theory, using information theory, I think it'd be

Unknown 39:00
helpful for audience if you could describe what functionalism means I could try it. But I think you could do a hell of a lot better job than I.

Joscha Bach 39:07
So very simply put, and very cursory is that functionalism treats a phenomenon as a result of its implementation. So, for instance, what is a bank? A bank is a thing where you can have an account and you can have an interface to it, you can store money in it, you can extract the money from it, it is conforming to a certain legal interface, it's giving you certain guarantees that are compatible with the legal and monetary system and so on. And if an institution is fulfilling all these principles, then it doesn't make sense to not call it a bank. And Dennett uses this as a metaphor to explain his opposition to the concept of philosophical zombie asst that is identical in all its features except not having fundamental experience to us and And as on bank would be a bank that is basically a thing where you can store money and retrieve it again, and you can ask for your account level and so on. But it's not a proper bank, because it lacks the essence of a bank, it's a true bank, goodness, the thing that you cannot really touch, and that has no causal influence of its interaction with the outside world. And this, we would reject this notion of his own bank as nonsensical. It doesn't make sense to distinguish between a bank and his own bank because there is no essence of a bank beyond its functional interface. And why would this be different for consciousness is there some kind of a hidden essence? And so, in some sense, functionalism is the rejection of the notion of a hidden essence of something that does not have causal properties that you can observe. Everything can be explained in terms of its causal properties, and all causal properties ultimately, can be explained as functions that are implemented, which in my perspective means computable realizable in physical system.

Unknown 41:01
I must say when I read Dennett, Consciousness Explained, while I didn't necessarily buy as pandemonium theory, I did find his rejection of philosophical zombies reasonably convincing. What's your thought on that,

Joscha Bach 41:15
depending on your theory, by the way, goes back to Selfridge and has strongly influenced Minsky over surfer student and has led to the society of mind. And I think it's a very beautiful theory, it says that the mind is basically a bunch of agents that implement different behaviors. And you can train your mind with new demons. And demons is a good metaphor in computer science. It's programmed without a user interface in a way without a user facing front end. But it's an automatic behavior that basically can be instantiated can run multiple of them, they can interact with each other, and produce combined behavior in your computer. And in this pandemonium theory, you have a stage where you have the active behaviors that are currently populating your working memory and perform certain tasks. And there are others that are sitting in the audience and can basically evaluate what's happening on stage, and they can pull others on stage or boo, some off stage. And those that are on stage can also call up the Ellie's le behaviors and get them on stage to enact a certain scene. And so it's, I think, a very powerful metaphor to imagine how the self organization of behaviors in the brain might work. It's not really much more than this. It's not yet an implementation. But it's, I think, a good start to basically see a self organizing system that is adapting itself to a variety of unknown situations in the world, by composing a team have behaviors at any given time. So this is the power of the pandemonium theory. For Dennett theory of consciousness itself. I was always wondering why Dennett seem to be so ineffectual among philosophers. And among philosophy students, a lot of philosophy students don't like Dennett very much. And also a lot of philosophers don't like Dennett, very much or analytical philosophy for that matter. And I suspect it's because Dennett seems to miss the problem that people try to explain. He is not talking about that problem very much. It's not because he cannot explain it. But because it doesn't seem to think that fundamental experience is that important at all. It doesn't take center stage in his philosophy. And I think that could be because Dennett is such a nerd. Maybe Dennett doesn't have that much fundamental experience. Maybe Dennett is somebody who is extremely constituted on the conceptual side. And I can relate to that, because I am also very constituted on the conceptual side, it's only that I started observing this difference in people who are constituted on the side of feelings that are constituted in the sight of what the people of science mean, our intuitions, maybe pre scientific intuitions that scientists are very, very off. I think that a scientist or a true philosopher is born as an aberration. It happens when a child is so desperate, that it decides to permanently trust its ideas more than its feelings. And this is often the case when you are NERT you have the best of intentions are very pure feelings about how you should interact with the world. You have compassion for your environment, maybe but you might have poor empathy. Because some of the virus in your brain are not via properly during your development. And so you cannot guess the mental states of others. You don't have a sense for the mental states of others, it's difficult for you to figure out what you should be believing that you should modify your beliefs to fit in and so on. And it's maybe repulsive to you to modify your beliefs just to fit in, or to pretend that you modified them, because you don't like the innate sense how important that is to play ball with most humans. And so as a child, you fail in your social interactions with non nerds and because nerds fewer and far between it's probably less than 6% of the population are on that spectrum. You learn that you cannot trust your feelings when you want to interact with other people, you fail if you do that, right. So what do you do? You need to build rational models for interacting. And I suspect that's one of the reasons why so many people that are very, very good at rational analytical models are nerdy, and end up in the sciences, and are not very good at intuition and intuitive empathy, and the resulting social interactions, right, it's almost a trope that good scientists pour into social interaction. And it's a disaster. I think that we are cherishing scientists is a lifestyle archetype. And is the goal of the educational system that every child should somehow become a scientist know, a proper scientist is very useful to society. But is it effective human beings, if they are trusting analytical tools, which is very brittle and hard to prove? more than they trust their feelings and intuitions? Right? Science works like this science works by always finding analytical tools, and only believing in what you can prove. But this is not viable for life. Because almost everything in the world is more complicated than you can deal with human logic, right? So science is able to deal with the edge cases. But it's not as good in dealing with the mainstream of problems that you have in life. Scientific theories don't have as most of the everyday problems, they only help with the difficult edge cases where your intuitions fail. So in some sense, science exists to deal with our darkest emotions in a very literal way. Because our darkest feelings is those feelings that are extremely difficult to disambiguate. That's why they're dark. They don't help us there are feelings in these darker regions. They don't tell us what to do. This is where we need logical reasoning. And so I do think that scientists are important, they need to be protected, but they are the result of a particular mindset. And is this also true for people like Dennett, then it is not able to talk to people very successfully in the same way, as Deepak Chopra does, because Deepak Chopra might not be analytical, completely clean and pure. But he's able to resonate with people at a very deep level, where he can tell them what they what they really care about what they want to have explained their everyday life. So in some sense, if we want to make computational this philosophy attractive to, to mainstream, we have to explain to them how functionalism is dealing with intuition, this fundamental experience, this sense for the greater whole, with our sense that we are not confined to a single self in a single organism, but that we stretch far beyond that, that we are somehow distributed in the universe exponentially. That's true for normal people, it's probably not true for most scientists, most scientists are confined to their own intellect at some level, including myself.

Unknown 48:04
Yes, I mean, I know a lot of scientists, and certainly in the scientific community, the kind of folks you're describing are over represented, in fact, probably the greater ones are even more over represented, but it's by no means 100%, probably not even 50%. Probably the greatest scientists that I personally have met and spent time with was Murray Gelman. And he's an amazingly social character, right. He has social intuitions, you know, he knows a lot of things that aren't about science, and he can weave them into an extraordinarily interesting conversation. Unfortunately, he's now passed, he was a great guy out in our Santa Fe Institute. But, you know, he was certainly an amazing scientist who nonetheless was a quite complete human being. So I would make sure we don't over Oh, absolutely. Well, I agree, statistically, it's true.

Joscha Bach 48:49
geniuses that are very good at all the fields state, they do seem to exist, but they're very rare and far between. It's basically people that are able to integrate so well over so many areas, that they reach that stage of development in all the areas right. I also would not say that scientists are in a moral or exponential sense in complete human beings. They are just different from your run of the mill, Homo sapiens, or many of them are even somebody like Noam Chomsky, well, very deeply respect, or Marvin Minsky, there are people that have very specific minds, right, they are very good at some things. And there are other things that they don't specialize in. Basically, they have indications of Asperger's, of a mind that is hyper focused on some areas at the expense of others. And there is this suspicion that this hyper focus is the result of a compensation that you get super good at writing books, and Chomsky is probably one of the greatest minds of his generation, and also in terms of writing. If you ask him something he's going to respond. There's a complete chapter in Putting footnotes and references most other people that I know only think of paragraphs,

Unknown 50:05
and they actually it's rare only good thinkers can think in whole paragraphs. I know some that do. So let's get back though to the philosophical zombies and Dennett. That's an interesting perspective that Dennett doesn't see the problem. And you know, maybe the real payoffs you go one step beyond philosophical zombies and go to David Chalmers hard problem, which then it also rejects this sort of being a non problem. And I think that's relatively close to his rejection of philosophical zombies. What's your position on Chalmers claim of the hard problem?

Joscha Bach 50:37
I suspect that charmers in part of the time sees ways out of the heart problem. But he's still mostly on the side where he doesn't. His current philosophy is often focusing on the question that we don't need to explain the heart problem itself, we need to explain why people think that there is a heart problem. Right? So we need to explain the psychological certainty. And we can come up with theories that explain the psychological certainty. So we don't have to explain the phenomenon how a physical system can have conscious experience, the need to explain why we think that we have conscious experience. That's interesting, right. And I think that this goes in the right direction. There is something that struck me when I was looking into Parapsychology and I, in some sense, grew up. There's a part of parapsychology, for instance, in the 70s. And 60s, when LSD was around, and still not illegal, a lot of people at the CIA experimented with LSD. And incidentally, during the same time, there were a lot of experiments that were similar to if you've seen the show Stranger Things, the MCAT Auto Program and related programs, were looking also into clairvoyance and into fire sensing where people would focus their mind on different parts of the world and photon use out of body experiences to sense what's going on there. And the number of papers and books came out of this by people that were not completely loopy and that had proper state funding. And when I read this stuff, I concluded this is probably incompatible with physics as we know it, it's certainly not compatible. This the Standard Model, because we need more than the four forces that are compatible as the standard model to explain what's going on here. And the quantum nonlocality, explanations that come up here, are not explaining it, because quantum nonlocality does not allow the transmission of information that normally would require a photon to send. Its, it might be possible that there are different regions of the universe that share state, but you don't know which regions of the universities are. So you would need to have a classical back channel to that region to know about the entanglement between these different regions. And so it's not obvious that there is some kind of quantum mechanical explanation for upside, phenomena. And as a result, either significant parts of physics that work extremely well in the lab are wrong. Or these phenomena are not correct.

Unknown 53:19
You know, our mutual friend Ben gertle, is a fan of these SCI type things.

Joscha Bach 53:24
Yes, he is also incidentally, a big fan of psychedelics.

Unknown 53:28
Yeah, I'll confess to have done a little psychedelics in my day, though, not 40 years, and they are an interesting altered state.

Joscha Bach 53:36
So there is a correlation between psychedelics and the ability to look into the future without winning the lottery. Right? There's a weird thing about SCI phenomena that don't seem to change the physics of the universe, or the statistics of the universe very much. If we could evolve the ability to reliably fire sense or look into the future. We should see a lot of animals that do this and predator prey dynamics, the should have a small subset of the population at least that quite reliably wins the lottery without cheating. Get the backstory, how is that possible? It's an argument that, for instance, has been made by Stanislav Lem. And I think Summa technolog er in one of his purely philosophical books, that don't make a concession to the reader if it's a very beautiful one. And where he argues against say, in this regard, also a very good friend. At my time in Harvard, she started what friend, I hope she had prophetic dreams every night. Lucid dreams. It's very difficult to deal with this, because she was often able to look into the future during the streams. And she started writing this down and she noticed that when she was experiencing things that were quasi irrelevant, that basically would be the equivalent of winning the lottery that would change the future in any way. So She was unable to write them down. And it's like the men and black basically it prevented her from doing that. I think there's only one good explanation for that the explanation is that your memories are changing retro actively. So you don't remember when your memory changed. You don't remember when your construction of reality change, she was not able to write it down, because she didn't know it at the time that these lottery numbers would come is not a real numbers come. And together with the knowledge about these lottery numbers, you instantiate the memory of having foreseen what these lottery numbers were? Right? This is the easiest explanation.

Unknown 55:38
Yeah, and in which case, you actually did not know the lottery numbers, right? You had the memory of knowing the lottery numbers without actually knowing them?

Joscha Bach 55:45
Yes, but there is a deeper phenomenon here. So if the Spiritist and upside theorist and power psychologists are right, and I'm not completely ready to dismiss them, and Turing was also not doing so vital is if you save his 1950 papers makes explicit and affirmative references to the high probability that telepathy is real. So he even asked the question, Could an AI system be truly intelligent if it's not telepathic? or would this be a sufficient intelligence if it's not telepathic, there is a deeper implication that it doesn't discuss, if telepathy is real, if there is a possibility to use unknown physics to send information across minds, that are not in any kind of known signal in relationship to each other. So there are adjacent so you can observe the other one and entangle yourself as their own vibrations in their mind just using your visual sense or other known senses. Right? If that is true, if there is something like telepathy using an unknown, physical causal mechanism, how can you guarantee that your consciousness and your mind is computed in your own brain? How can you guarantee that your brain is not merely an antenna? And the consciousness is computed elsewhere? Maybe outside of the universe, and you are apoptotic in this universe, right? In this consciousness?

Unknown 57:04
Yeah, that's the Penrose hypothesis, right? But the microtubules are somehow quantum antennas that allow us to get signals from across the multiverse perhaps

Joscha Bach 57:14
I don't really understand why Penrose is affiliating himself is that so much? Right? It's it's difficult because his theory is different from Hameroff theory, I've never seen Penrose actively endorsing the microtubular in the same way as Hameroff. Does it make you really, really like Hameroff as a person Hameroff is building a psychedelic Sculpture Garden. This is theory, it's a theory that is making more predictions every year or more explanations every year using the same mechanisms. And so it's a theory that explains how anesthesia works, how psychedelics work, how consciousness works, and now also how evolution and emotion work, all using the PI resonant quantum underground in a way that I was at the science of consciousness conference sitting in the audience, and he was putting stuff on the screen. And I felt that my basic understanding of physics was sufficient to understand where the physics ended, and ask the people next to me, are you not concerned that you don't understand what he's talking about? And it's physics about the pirate resonant quantum underground. And they said, Don't worry, be shut down. As soon as you mentioned that. It's not the important part of the theory.

Unknown 58:32
I truthfully have not checked his stuff out, I will have to. It's super

Joscha Bach 58:35
interesting to read it. And it's really beautiful. I like it. It's it's also very poetic in a way, but I think it's more art than science. Because it's integrating observations. It's projecting things, it's connecting loose ends. But it's not doing this in the same way as a scientist will properly do it, which explains its lack of resonance in the sciences, right? There is almost nobody outside of the pen of his circle, who is setting these papers and working with this and thinks that they are in any way real and verse discussing. The reason I think why Penrose is affiliated with this. It's, you know, if you are excluding all the viable explanations except one or the other probable explanations, then the improbable explanation has to be the reason and Penrose thinks that computation cannot explain all of consciousness. And I think it's because of his way he interprets girdles proof girdle has in some sense proven that computation is insufficient to do all of mathematics. There are parts of mathematics that cannot be done in a computational paradigm, assuming so leads into contradictions. And Penrose believes that human mathematicians can do these parts that computation cannot do. I think the resolution works the other way around. These other parts of mathematics were never real mathematics was ill defined the true theory of class SQL semantics is wrong, you can only claim that something is true. If you can actually compute that truth, you can only claim that something has a value, if you can actually compute that value, it's basically constructive mathematics and constructive mathematics is roughly the same thing as computation. And what girdle has shown in my view is that constructive mathematics is, is real, or can be real in the sense that can be implemented. But classical mathematics that contains infinities cannot be implemented cannot be real. Nothing in the physical universe relies on having known the last digit of pi, right? There are mathematics that pretend that they are that this works. And some of these mathematics are even used in physics, but they're not real. They're not computable, they cannot be implemented in any physical causal structure. So this was the implication. And this is an implication, I think that Penrose has not seen. He believes that the ability of our minds to be conscious are related to the uncomputable part of mathematics and the uncomputable parts of mathematics go beyond known physics, which in some sense, is all computational, right? Even quantum mechanics takes a bunch of numbers, and performs some expensive computations on them, and then you get the next bunch of numbers. And that's how we explain the universe. And the only part that is not explained in this paradigm so far is quantum gravity. So the culprit must be quantum gravity, right? It's the only thing that's left from Penrose perspective. And the only one who tries to offer a theory that uses quantum gravity to explain consciousness in the brain is Penrose using his microtubules. And I think this is how it comes together.

Unknown 1:01:38
Interesting. Yeah. And I've truthfully read it when it came when he came out, read a couple things on it. And I just put it aside and says, Well, I'll know that it's out there, but I'm not going to pay it any mind until somebody comes up with some useful experimental results. And opera, check out this Hameroff character, I don't know about him. But let's

Joscha Bach 1:01:54
go slightly back to this big distinction between physics and the parapsychology, which leads us into a different world. I think that there are only two possibilities either we live in a mechanical universe. And this is the hypothesis of physics, it says that everything is at some level, there is a cold, causally closed mechanical layer. And this doesn't mean that the closer close mechanical layer needs to look like what the universe looks like to us, right? The Einstein space. And our theories of quantum mechanics are probably high level descriptions that don't describe the causally closed, lowest level, but it's still mechanical, it's still built in such a way that can be expressed as a computer program. And the alternative to that is that we live in a dream. What's the difference between a dream and a mechanical model? A dream has magical interactions, they're symbolic interactions, which means you sacrifice the cat. And the comment appears, or the comment appears, and it predicts your career. There is no known physical force, or plausible physical relationship that we could discover that would explain this kind of interaction. It just means that somebody is messing with us there is a conspiracy, right? The same conspiracy that exists in Minecraft, if you open up a console and set times that day, and the sun suddenly rises in Minecraft, that you basically supersede the basic low level causal mechanics of Minecraft using a higher level of causation. That is outside of the basic game dynamics. And this would be a world in which Psy is possible in which say, in the esoteric parapsychological sense, that you can use telekinesis to overcome physics, or that you can use clairvoyance to overcome the limits of information transmission via subliminal photonic transfer of data between different regions of the universe. Right. So how is that possible? And I think the reason that this is so attractive that the theory that we essentially must live in a dream is that we actually live in a dream. You do live in a dream. It's it's possible to have these experiences it is possible to experience telepathy, it's possible to experience clairvoyance and so on. And it's because we live in a dream that is generated by a mind on a different level of existence. And this different level of existence happens to be physics. And our culture is a little bit confused, because it assumes that the world that you may see in everyday life is physics is the physical world. But there are no colors in physics, and no sounds and physics. What we perceive is still the dream. And our brain that is out there in physics does not really look pink and squishy in physics. It's only a thing that is this is what it looks like in the dream, the dream that the brain is generating to get this right. That is an important thing. Yeah,

Unknown 1:04:54
we all know we've known for a long time that we don't experience actual reality. We Have an interpretation of reality. And then we have meaning maps, right?

Joscha Bach 1:05:04
So we don't experience a simulation of reality, it's a simulacrum of reality. It's completely as if it's not isomorphic to the world out there in physics,

Unknown 1:05:13
and it's a different levels of abstraction, right? I mean, the human perceptual capability can't tell us about atoms, for instance, right, without instruments. And you know, the idea of colors even as we know from anthropology, that different cultures divide the spectrum up in different ways. So some tribe in the Amazon might not describe the brain as pink and squishy it might be blah, blah, and squishy, which is a very different, narrower part of the spectrum than pink, for instance, or probably much broader. I think, actually, we have a more fine grain color than many cultures. So yeah, it's their meaning maps. And I think that's where people get confused. I still remain a naive realist at heart, I do believe there is an objective universe out there can't prove it. You know, as we well know, I can't logically prove the universe wasn't created five seconds ago with all the objects in it, including our memories and all ballistic objects in motion. But if only for parsimonious reasons, I assume there is a real universe out there. And as you say, we have some physical laws, but they're a very long way from the clocks scale. So there's probably a shitload we don't know right? And yet, so far, they have shown themselves to be remarkably lawful. We haven't been able to detect at least with a high fidelity signal, any sigh any, any real even deviation from deep lawfulness. And so, at least tentatively, I side with the physicist, though I do encourage Ben, which he does regularly send me new papers on site, because, as you say, if we're wrong, or if I'm wrong, and sy is real, then we have to reevaluate. Is the universe actually lawful? And maybe it's not just a dream, maybe it is a simulation. But for the time being, I reject that on grounds of parsimony of no other reasons.

Joscha Bach 1:06:54
Yeah, so the only reason to not do it is to not accept this theory is because you don't think that scientists are able to explain the things that need to be explained. Which is why does reality appear real to us? It from a machine learning perspective, it's pretty clear that if a learning system does not in some sense, implement the belief that the universe is learnable, then it's not an effective learning system, right? You have to believe that earnable universe to learn it, it's implicitly. And so the weird thing that we have to explain is, I think, not the qualities of qualia, the qualities of qualia are easier than most people think. Because this is just the geometric calculations that your perceptual systems are making is basically the dimensions the parametrizations of the geometric architecture that is computing the perceptual models. And in some sense, this has been neglected for a long time, because scientists have focused on the linguistic the analytical models too much. And only this the widespread take off of the machine learning paradigms and deep learning paradigms. I think it has gotten more into the common consciousness of cognitive science, that perception is more akin to the deep learning systems than it is to linguistic systems to symbolic systems. But the paradox is, you can of course, implement the deep learning systems on top of symbolic systems that we actually have to completely implement on top of symbolic systems. But there are symbolic systems that are very different from our symbolic reasoning, or symbolic reasoning is arguably limited to a very small stack size and to very few elements at a time, we cannot hold more than like five to seven elements in our focus of attention at a time and relate them to each other. And like GPT, three, that is looking at 2048 and relates them all to each other at the same time. And it's doing this in many, many dimensions, and with extremely high resolution and availability and no lapse of attention. So this is a very different way of doing symbolic operation that our mind is doing. It's doing this on the level of low level automata. But you have to explain to people how the property of realness comes about. And the property of realness itself is paradoxically not a feature of physical reality. Physical reality doesn't feel like anything, there are no feelings in there. Physical realness can only be experienced as part of a model because it's itself a model property, right? It's a label that the mind attaches to some of these parameter dimensions. And if you look at them, you distinguish the non real imagination from the real world that you experience by this label that your mind says this is indeed predictive of your net next batch of sensory patterns as far as I can make it. And this includes the internal sensations that you have about your own self, and your own thinking processes and reasoning processes and your exponential processes. Your experiences are real experiences, because they are predictive of your day. experiential features, right? There are models of what you experience. And the realness itself is a model of, of the fact that they are predictive.

Unknown 1:10:09
Yeah, though, of course, it's true that for humans again, as I said before, the resolution of our measurements and perceptions are pretty gross. And we were able to go way beyond that with our scientific instruments. Yes, we evolved as if we are in a lawful universe because that's what works right. But it may well have been that we were so coarse graining, the universe due to our low resolution vision, you know, the acuity of our feeling the ability to decompose matter, that we could have easily been fooled by something going on deeper down, right. At least unless our instruments are lying to us, we can probe I don't know 20 orders of magnitude smaller than we can get at as unaided humans and 20 orders of magnitude larger. And yet lawfulness still seems to prevail in both the microcosm and the macrocosm, even though, our formulations of the laws are almost certainly very substantially incomplete. That seems to be much stronger evidence for reality, deep reality than merely our sense of reality, though, I do take your point that, you know, reality is not an attribute of the universe itself, but it's rather an experience of a conscious agent living in the universe. But I would say we should have more confidence in reality than merely our naive animal consciousness because of the fact that we've been able to extend our probes a long way in both the microcosm and the macrocosm.

Unknown 1:11:34
I think it's important to hold it somewhat tentatively.

Joscha Bach 1:11:39
I think that to have an enlightened relationship to reality, it is necessary to realize that what you perceive is a representation is includes yourself and your relationship to the universe. This is all in some sense, a representation that you don't perceive as a representation, but as an immediate reality. And you need to make it visible as a representation, you need to pay attention to that. You also need to pay attention to attention in such a way that the attention itself becomes visible to you, that you can notice how your attentional system works and how it's constructing your reality. If you're interested in that, and for a normal human being, the only reason to be interested in that is because it doesn't work. And since these attentional processes tend to work very well, most people don't pay attention to them and just take them as given the processes of reality construction. And the people that are familiar with these processes are those that are naturally in altered states of minds, because they fell down the stairs headfirst at some point, or have different issues, or because they are in an existential crisis, for instance, and this existential crisis makes it necessary for them to understand their own relationship to meaning and their own self construction.

Unknown 1:12:55
Yep, let's step back a little bit. We've been talking about mind emerging from matter. We've talked about a lot of the leading theories. We've talked about some of the out there theories, from your perspective, what's important for the next step, and the science of explaining mind emerging from matter? Where should we be looking next? What's the next 10 years look like?

Joscha Bach 1:13:19
So it's, of course, always very difficult to make predictions, especially about the future.

Unknown 1:13:24
Yes, thank you, Yogi. Yes.

Joscha Bach 1:13:28
I suspect that one of the very interesting areas that we need to look at is attention based models. And the transformer is only the beginning. From my perspective, there are at least three obvious things that are wrong with GPT three. Interestingly, the fact that GPT three is not an agent is not one of them, that can be easily fixed, right? It GBTC is obviously not an agent doesn't have a context. So if you tell GPT, three, Hey, you are GPT? Three, what are you, then GPT three will produce a relatively random sentence, because GPT three has been trained on statistical statistics and language until October 2019. And GPT threads didn't exist in October 2019. Right. So unless open AI has built something into GPT, three explicitly to teach it about GPT. Three about its would say, it doesn't know what it is. And if you will tell GPT three, I am talking to the AI GPT, three and so on, then it will assume in some sense implicitly for some meaning of assume that it's talking about some kind of science fiction context or a technique context in which a human is communicating with an AI system. And then it might guess a number of things right, but it doesn't know which ones are right or not. It also doesn't know whether it relates to any kind of reality where there is no sense of an underlying reality. There is no fixed context. But you can add this fixed context by I'm making additional commitments. And you need to make these additional context commitments. I think that is an implication of loops theory, which is an recasting of girdles proof, a system is not able to basically break out of its own axiomatic system and make statements about axiomatic systems above it. In order to have a system to reason about itself, it needs to recreate itself vision, its own formalisms and then make a statements about this recreation of itself. So, for formal systems that you have created, of course, you can build them in such a way that you can recreate the formal system is in the form of system, so the system can make proofs about itself. But strictly speaking, you cannot make proof above yourself. You can solve this problem of agency and reproduce we I think by building in principle, Vision tool, speech module, vision to text module that is interpreting the camera images of a robot, sends them to GPT three, and then GPT three tells a story about a robot in that world. And then a parser is reading these statements and translates them into motor actions of the robot. And we continuously play that game. And now, arguably, language is not the right level of resolution, we want to have something that is sub conceptual, we want to deal with perceptual stuff. But GPT three is able to deal with that right? This image DBT, which is able to learn the statistics of images in the same way as a gun what or even arguably in a better way than a gun, but within the limits of its small attentional window. So the main issue, I think, is the creation of a larger attentional window at the moment. If YouTube is free, has massive retrograde amnesia, it's not able to remember anything from two pages ago, except the extractions that it made from that it's able to change with neural network as a result, even though it doesn't do online learning is about during training. It is of course, it takes something from it has seen before. But it's not able to re establish the context in which it took it took these insights, right, when I read a book, I might, in a later chapter, we call an earlier chapter, and create a context. This is merging the current chapter with the previous chapter. And then rewrite everything that I learned from the previous chapter in the new light. Because I can reconstruct what I learned from the previous chapter, and I got this knowledge from and so on. Right? So for instance, I have an idea that I misunderstood or the diverse, too simplistic, and I need to revise this idea. And I asked myself, where did I get this idea from why and how I'm revising this right now we're creating a new working memory context in which I direct my attention. And this working memory construction is a thing that GPS fi cannot do yet. So we need to extend attention in such a way that it's able to change working memory context actively and construct working memory contexts. And we need to change the level of representation from language to multimodal representation that is agnostic to what it represents and addresses.

Unknown 1:18:08
And they are you mentioned something in passing, which I want to call out, which is rewriting, you know, one of the things that we know humans do is our memories are not only low fidelity, but they're also quite subject to be rewritten, right. And certain kinds of linguistic processing may be implementable as rewrite rules. And, you know, I think we both have had some exposure to the opencog system and the opencog system, a lot of it's based on the concept of both local and global rewriting. And that seems very far from GPT. Three, I mean, it is what it is, it doesn't have any essentially dynamic rewriting capability going on in it.

Joscha Bach 1:18:45
Exactly. So this is the second thing that is wrong. distributees three, in my view, the first thing was the working memory window, which is limited to 2048 adjacent tokens. And basically, there are hard constraints in there in which the working memory is used, that don't exist. In our own mind. It's not necessarily our working memory is larger, I suspect it's much smaller than the ones of GPT. Three, but we are able to construct the contents of our working memories was way more degrees of freedom. So that's the first thing the second one is online learning GPT three is only doing offline learning, which is good for an industrial industrial production system that is meant to behave in pretty much the same way. But if you want to build a system that is working like us, it needs to continuously learn GPT three has stopped learning in October 2019. So it doesn't know about anything about COVID-19 or George Floyd. It lives in a different universe. And we need to have an agent that is constantly learning and re tracking reality in real time around it. But this is something that also can be overcome, right. It requires massive changes to the algorithms that are being used. You cannot use the same neural learning algorithms that are currently implemented in GPT. Three, but it's nothing that is completely out of this While this can be done, and the last thing is relevance GPT three does not care about relevance, the relevance sensation that GPT three has is because humans don't write everything into texts, they write things into text that are relevant to them. So by minimizing this appraisal on all the available texts that have a decently good scoring on Reddit, means you are probably learning something interesting, it was interesting enough for a human to write it down, right. So just by learning from this, this is way better than random stuff. But this is not sufficient for a system that is interacting with the world and takes in rich sensory data on many modalities at a higher bandwidth that you can process in real time. So in this case, you have to focus on those parts of the model that are most promising. And for this, you need to have a motivational system. And I think that in practice, you will have way better results from systems that are able to assign relevance to learning and meta learning in ways that the current GPT three is not. Right. So GPT, three, and two styles learning in ways that no human being can do because it learns all the stuff which we find is relevant. When you read a textbook, you don't care about the style, so much you care about the content. And gptc does not think that the content is more important than the style, it only looks for style and goes for so deep style that ultimately it often bottoms out in content,

Unknown 1:21:27
it's a little bit more about the effective part, and how that might be added into GPT. Three, or the

Joscha Bach 1:21:35
effective part is in this messy something where the size theory, I think is still one of the best theories today. The size theory posits that you can describe an agent like us using homeostasis as a guiding principle. So there is a homeostatic balance that keeps the system stable in the face of an environment that disrupts it. Our mind is in some sense, solving a control problem in many dimensions. And these control dimensions are given to us is needs that when frustrated, produce pain signals, and when satisfied, produced pleasure signals. And this, when we act on these needs, we act on purposes and models of our needs. And we have strong biases on what kind of models of our needs before. And this hierarchy of purposes that we formed that becomes coherent is in some sense the structure of our soul. It's it's not a random set of behaviors that is just sitting next to each other and randomly arranged. It's a system that strives for coherence. And the more coherent it is, the more our mind appears to be as a singular solid thing that has a definite structure. And in some sense, you could say that our true soul is the Platonic form the ideal form of all these purposes arranged in the right hierarchy including our transcendental purposes that go beyond the individual and its present time and space that it occupies.

Unknown 1:23:08
Okay, I think there we're about out of time, we didn't get to any of the low level things I wanted to talk about but that was all right. I think our conversation was good and rich and went a long way and I hope the audience appreciate it. I know I certainly did. So your shot love to have you on the show again sometime to talk about your thinking about microsite three and things in your own workspace. But I really want to thank you for this amazingly broad range and yet deep discussion.

Joscha Bach 1:23:37
Hey, thank you too. I really enjoyed talking to you again. And let's do it again sometime.

Unknown 1:23:46
Production Services and audio editing by Jared James consulting music by Tom Mueller at modern space music.com

This transcript was generated by https://otter.ai