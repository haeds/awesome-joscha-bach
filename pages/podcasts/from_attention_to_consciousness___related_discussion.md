Joscha Bach 0:09
So I hope this works. My intention was to discuss the progression from attention concepts that we currently developed in artificial intelligence, to notions of consciousness. I think that AI is more than the attempt to automate statistics and develop better ways of automating data processing. But it's also a philosophical project. And this philosophical project was the original starting point of AI, even though it is in practice only a very small fraction of what AI research has been doing so the last six decades or seven decades, and the philosophical project is a little bit older, actually, then the form was ratified artificial intelligence. In some sense, philosophy is the realm of all theories that are possible. And it's mostly conducted in natural language. So it's very hard to say something in philosophy. That's true. And mathematics is the realm of all languages, it starts out with the formal ones. And it's relatively easy to say something true in mathematics, it's very hard to say something meaningful about the real world, about the world that we are interacting with, that we want to talk about in mathematics. Because it's languages are too simple. So in some sense, when we wanted to combine philosophy and mathematics, we would need to find a different approach. And this unification of mathematics and philosophy is, in some sense, a very old project. And you'll find this in like nets, attempt to come up with a discourse of calculus where you translate every statement into something like a sequence of numbers, and then established formal ways of evaluating these values to come to a conclusion and an argument and figure out who came up with the Lipsky crew and try to, in some sense, continue with this project. And the most important attempt in the last century, I think, was Vidkun Stein's Tractatus who pursued the same project, basically find a programming language for thought, and Vidkun. Stein will preempted the logic, this program of Minsky's artificial intelligence by several decades, also preempted its failure at the end of his life included in the investigations that he was unable to deal with images was the grounding of the symbolic language in the real world. And I think that the solution to this problem is, in some sense, the mathematization of perception, the mathematization of the way in which our minds directly interface with reality, not just the matter, the structuring and memorization of natural language. And this complete mathematization of the mind is, in some sense, part of what AI is currently attempting to do. And so if you look at the eras of AI, classical AI started out being mostly symbolic, which means we analyze a problem and we find an algorithm to solve that problem using a symbolic algorithm. And now, we are in the era of deep learning, which uses compositional function approximation to design no longer algorithms that directly implement the functionality, but algorithms that learn to discover the functionality to solve a problem, right, so we basically transform the problem one layer higher. And it's tempting to think that what happens next is going to be a meta learning revolution, that is algorithms that learn how to learn a solution to a problem that learn how to deal with a given formally unknown domain. And it's also tempting to think that our own mind is in some sense, a meta learning machine. It's not just a collection of learning algorithms, but it's a collection of mechanisms that can learn how to learn.

Joscha Bach 4:01
And the present revolution is very young, the paper that in some sense, triggered it's not the algorithms, the algorithms have been, to a very large extent, been discovered in the 1960s and 70s. And then almost everything of importance has been discovered, as we know, in the early 90s, by young Schmidhuber. But the actual revolution, I think was triggered in 2012 Is this paper here. It was a collaboration between Android slack and Google. And what they did was unsupervised learning on randomly chosen YouTube frames. And they used 16,000 fours and 10 million images, many tweets more than a baby gets to see during the first month of its life. And this using sparse auto encoder and without any kind of labeling or supervision missing was able to discover object categories and And, for instance, categories for faces and for cats. And for other things, the cat thing was what cat had internet fame. And this was a big revolution because it was able to discover semantic classes in data without any kind of embodiment or human input just by doing statistics over the data is something that many researchers that work at the same time in linguistics, and so on thought would be impossible. The humans that you're using are nothing like real biological neurons, they are weighted sums of real numbers that we chain, you combine them into layers that approximate arbitrary functions. But the mostly, we are not using only charities, but we are using, basically radio chains of real numbers that we adjust with the chain rule and stochastic gradient descent. But there are alternatives to the artificial neuron set, we are currently using the function networks and a decision tree support vector machines. And I think that the, in some sense, barely scratched the surface of the things that would be possible to facilitate deep learning like things. And let's not forget that biological neurons are very different from the humans that we are currently using an AI, they design the systems from the inside out, not from the outside in they every biological neuron is in some sense, an autonomous reinforcement learning agent, a little animal that is being rewarded for firing at the right moment, and it has to learn its activation function, depending on the state of its environment. So the neuron learns which environmental states signaled anticipated reward. And this is when they have to fire on average. And the rest of the nervous system has to reorganize, to give the individual neurons proxy rewards, to tell it when it's doing going to do the right thing. And so like people in a society, the neurons in your skull, are forced to form an organization that converges towards a meaningful structure that can control an organism that is embodied in a physical world in order to feed all these neurons. Right. So this is the perspective that we could have on biological neurons, and is it self organizing system that is dealing with lots of entities that are locally self organizing, and locally self motivated. And artificial intelligence, we are currently witnessing a revolution which I would call the transformer revolution. And this revolution is basically what got us out of the convolutional neural network bubble. Perception could be dealt with relatively well with convolutional networks, which basically replicate a set of links that you discover a small kernels all over the input field. And the works relatively well, because in the visual and auditory domain, adjacent features that basically spatially or temporally adjacent features, they also tend to be semantically related. So basically, if you see pixels in an image, then the neighboring pixels in the image have a tendency to be semantically related to the image sets that you're looking at. So by telling the neural network that it should pay a great, great attention to the immediate neighborhood of pixels and very little attention to pixels that are far away, you basically give it a meaningful prior that allows us to deal with the complexity of the data that it tries to learn over, then it looks at images. But this doesn't work very well in language. Because in language, it's possible that you have sentences that are related across an entire book, or even across books, right, or you have words that are even a noun. And an article can be many words apart and doing statistics and grim statistics of all the adjacent words, is not allowing you to deal with successions of something like five symbols if you have an alphabet of a few 100,000 words. So

Joscha Bach 8:58
basically, you cannot use cough nets very well or natural language if you want to compress it. And so the big breakthrough of the transformer came from the Natural Language Processing community and 2017 is the paper attention is all you need. And the attention system and the transformer is basically making statistics overbought, you should make statistics over so every layer of the neural network is learning which parts of the neuron layer below it, it should pay attention to in the given context. And this turns out to be an extremely powerful paradigm. And it led to the development of the language models GPT two, and Bert that drives the majority of interactions that you're currently doing when you do a Google search, or when you do natural language translation at Google, that was developed in 2018 based on this paper, in GPT, three that famous thing that is able to write quite possible newspaper articles and translate programming code between languages and to simpler arithmetic and many other things, has been developed in 2020 by open AI. And the next thing that what happened was that people worked on making this attention, which for complexity, reasons about relativity based on text, but didn't work on images, basically found a way to feed images. And one of the first papers in this regard was open AI Stalley, which came out in January. And now we have very large multimodal transformers, like the Chinese system, who Dow that is a magnitude larger than GPT. Three, and uses combination of images and textual data to find shared representations. So what's so exciting about Delhi, instead of prompting to add this bit of text and then the continuous this text to PT three is basically in auto complete algorithm that tells you based on the text that you have seen, and the statistics that it is derived by reading the entirety of the internet, pretty much, how is the text going to continue, it's able to continue images, so you can feed it, a few lines of pixels is going to continue the image. So if you, for instance, did a few lines of pixels of a cat, it's going to give you the rest of a cat. And it's going to continue this image in this way. And it's also able to take a textual prompt and then converge on the pictorial description of that. So instead of a text is going to generate you an image based on the statistics of images that it has seen. And here's an example, this is the prompt that it got was an illustration of a baby Lemoore in pajamas using a calculator. So what you're seeing here is 25, baby boomers in pajamas, using a calculator, and what you can see and all these images, the statistical model has figured out that the, in order to use it, the baby Lemmer needs to, in some sense, touch the calculator, visits pause, and perhaps hold it in its paws. Right? It works in every of these images. And in some cases, it doesn't quite figure out the anatomy of the baby lamb or so if you look in the bottom row, you see, for instance, the tail of the limb or is attached in the wrong way, which is probably because it has not seen a lot of Anatomy of baby animals to get the generalization, right. But the generalization ability of the system, I think, is stunning. And here's another example. This is an illustration of the baby Pandavas headphones staring at his reflection in a mirror. And as you can see, the reflection usually doesn't extend below the mirror. So and the baby panda is looking into the direction of the mirror in basically all of the images. So here's an exception to this. But there's a second bottom second row. But did it again, the generalization is quite fascinating that the model is able to do this. And you can also ask you to reproduce photographic images. So here is the national animal of the Vatican state a few examples. And this is what it takes to be the national image animal of the Vatican state. And you can see that the top right looks like a pigeon, but it's not a pigeon. It doesn't know that difference. But it's quite fascinating. All of these images are not photographs that it has seen. These are all generated images. This is the national animal of China. This is examples of the national animal of South Africa. Pay attention to the interesting hybrid animal at the top left.

Joscha Bach 13:54
So there is something that is in captures in terms of semantics, it's it's not quite coherent, it is not in the same universe as us. It's not even in a coherent universe. But it's able to discover statistics and data via end to end learning that previous generations of models could not create. And the core here is that you're using self attention to make statistics on what you need to make statistics over. And this system is able to discover causal structure in the data. And this is something that researchers like Judea Pearl has been asking for, don't just look for associations in the data itself, or patterns that you will recognize in the data, but look for structure behind the data. And this allows GPT three, for instance, to perform arithmetic, simple arithmetic, but still, so it's able to discover regularities in the data that go beyond associations. And this what you also need to do is you probably need to measure the value of data. So in case of diabetes V. We know that all the data is there. You're gonna have to have be typed in by humans because this was all text data. For video, this is not the case there is a lot of noise in video, not every pixel in videos in YouTube is hand chosen and hand selected, most of them are irrelevant. And if you try to make statistics over them, you're not going to get to an any end with any kind of compute that is available to us. So you need to be very selective about what you make the statistics over and you need to measure the expected value of what you should be modeling. This is goes beyond what GBP three is doing. And you the other lesson from GPS for years, you need to optimize for local and global coherence. But these models are not doing so at the moment is just optimizing for producing something that is stylistically coherent. In some sense. GPSB detects the style of text, and the semantics of the text or the long tail of the style. And when we learn the structure of a text, we do the opposite, we first try to get the semantics, right friends, when we read a text about mathematics, we will first try to get the structure of the model that has been described there, right. And then we see the style of this text is the long tail that you might be interested in. And what we are looking for is basically a causal closure of a domain that you are able to make sense of the domain based on the symmetries that we discover. And what we also do we create a unified coherent model of the entire universe. And when we talk about meaning, you're not talking about a reference to the real world, you're talking about a reference to a unified coherent model of the entire universe, right? So when we discover the meaning of something, which means we discover relationships to the unified model of the universe that each of us has in their own mind. So how can I create a unified, coherent model of the universe?

Joscha Bach 16:48
The general form of perception model is that it encodes patterns to predict other predicate present and future patterns. It's a network of relationships between patterns that capture the observed in variances in the free parameters capture the variance. So we basically have sensory patterns that are being written by the universe. And we have free parameters in our mind hidden states, which are basically the states of the functions that make these patterns predictable and explainable. And the relationship between these variables are computable functions that constrain these hidden values respect to each other. So for instance, when you see a nose in your model of the world, it's not something that you directly see on your retina. It's a stable pattern that you use to interpret the blips on your retina in a certain way over time. And so the nose is basically a function, a set of hidden states inside of the model that you're using to interpret what happens on your retina. And then you interpret something on your retina is being a nose with a certain pose, and alignment and space. This constrains the rest of the perception in such a way that there must be a face nearby with the same alignment and pose, right. And if it's not, then you have a constraint violation. And you need to reinterpret what you are seeing. And these constraint violations are not probabilistic, they're possibilistic. So when you try to convergence, it's very difficult to get to convergence. If this is all discrete possibilities, instead of probabilities. The probabilities come in when you are trying to find a way to get the model to converge. And this probability means for instance, you will add priors to your interpretation of the image, for instance, that you're probably looking at objects in this video management space. And these objects have smooth surfaces, and so on. These are all priors that constrain your perception in a way that makes it possible for you to make sense of them and the vast majority of cases, but lead to defects in a minority of cases. So these probabilities that you learn that tell you how to convert from the given state of your model and the existing constraint violations to a valid state of the model, that all the constraints are satisfied, the model is coherent, these links are responsible for optical illusions, but they're also responsible for your ability to converge at all. And then in addition, we need to be able to measure the value of what you're modeling. So we add valence to the model. And we are also able to add priors that come from the outside for instance, we can add categories suggested by other people, by the systems that we are part of, and we can introduce them in our system as external knowledge or as norms is produced without priors and use this to structure the space of interpretations. So for instance, color categories are being suggested by other people when we grow up, and we use this to train color classifiers based on what we perceive. So there's an interaction between these external knowledge and the internal perceptual structure. So the goal of the model in every state is to predict the next state based on the previous state and the validation of the constraints leads to evaluation of whether the model is coherent. And at each step we will try to find a global configuration that minimum So the total constraint violation. And this now allows us to talk about perception in the sense of Piaget. And Piaget, the famous psychologist describes perception as assimilation, accommodation. assimilation is the process by which you are taking in the sensory data. And you modify the state of the model, not the structure of the model the state of the model to make it consistent with what you are seeing. So basically use all your perceptual knowledge to arrange it into a binding state that allows you to coherently interpret the sensory data, this is a simulation. And sometimes this is not going to be possible. And that's okay as you will need to modify the model structure to allow the simulation which means you learn and this is what Piaget calls accommodation. And accommodation changes our understanding of the world whereas, whereas assimilation does not change the models, it only changes the state of the models. So from a modeling perspective, assimilation is a convergence process. And accommodation is a learning process that has to balance error and uncertainty.

Joscha Bach 21:09
And the uncertainty on our model, there is local uncertainty, which is a set of possible states that a value can have next simulation step. And there is a global uncertainty. When you look at the world, it's the set of trajectories that the entire model can take over a given horizon. So basically, how does our universe branch and the uncertainty that we have no models results from the freedom and our constraints, right, so the uncertainty is a necessary aspect that we will have to have in the model, as long as there are unexpected objects in the world. So our constraints cannot be too narrow, we will always end up with we have an open world with a degree of uncertainty. And the uncertainty increases the cost of convergence. So basically, our model gets worse, the more uncertainty you have. And that's why we need to use our fixed resources to reduce as much uncertainty as possible. And we should reduce uncertainty based on the expected value of reducing this particular kind of uncertainty. And now this allows us to get to a possible definition of what it means to make a coherent model of the world that coherence can be understood as the minimization of global constraint violations in a model that minimizes rate uncertainty. And achieving this requires a specific system and attention agents something like a cortical conductor that lives inside of the perception agent and control agents in our mind, and helps us to pay attention what we should pay attention to. So what is the thing that we should be paying attention to, in order to learn efficiently and to disambiguate efficiently. And this thing is at the core of what we perceive as our own self, right? Our own self is a model of our own agency. And the agent in the sense is a controller, the cybernetic controller, that has a model of the future. And a set point generator, you know, of controller and cybernetics is basically a system that is able to collect data of a domain by our sensors, and changes the state of the domain via effectors, and makes inferences about the disturbances on the domain to minimize future deviations. So it can effectively regulate and this means that an agent is a controller that is able to make a model of the future. And this model of the future is going to be a model of the environment. This is the system that is producing disturbances in what you're trying to regulate. Right. So when we are dealing with the world, there are agents that have certain preferences of how things should be like how much we should have eaten, how relaxed we are, how much pain we should have, like non and all these parameters together give a number of functions if you try to optimize, and the system that you're optimizing is the movement of our skeletal muscles in the world and internal physiological parameters inside of our body and mind. And the environment is disturbing this in order to minimize these disturbances, we have to make models of the environment and Oss. And at some point of the modeling system is general enough that it can discover that there is an agent in the world that uses their own our own model, the model that we're currently creating to derive its own action. And this agent is us right? This is how we get to a self it was a first person perspective. And this agent is going to have a representation of its own freewill which means to have decision making under uncertainty we discovered that this system is making informed bets with in system of mechanisms that you cannot predict yourself because you don't know which decision making strategy you're going to use. And this is what you will represent as your own agency in the sense of having fever.

Joscha Bach 24:52
And this attention agent, it needs to perform a memorize operations on the perception model. So it needs to have pointers in The perception models and performs discrete operations on the perceptual opera models is backtracking. So these are grammatical operations. And this gives rise to this distinction between system two and system one is described, for instance by Daniel Kahneman is an example of a dual process theory. So we have two systems in our own mind, one that generates perceptual content, and one that reflects on it. And there's this big difference between percepts and conceptual structure. That is, for instance, percepts are being generated with some kind of distributed gradient descent, and the concepts conceptual structures can be constructed. And to construct things in your own mind, you need to have pointers and something like a compositional grammar, and you need to have a memory to undo changes that you're making, right. So when you are basically constructing something in your mind, for instance, a sentence or line of programming code or an imagination, you will have to have the ability to backtrack and this means you have an index memory of the steps that you are taking, and your perceptual system does not need to have certain integrated memory. And so there are very specific functions that our attention has to perform in order to allow us to synthesize programs over the perceptual data that looks at the perceptual data. And this gives rise to certain phenomenology of the attention agent. So we have attention to content, we have pointers into procedural objects, the relationship between them, and we have a representation of the mode in which we attend. So for instance, we are aware of whether we are selecting constraints or counterfactuals, or whether we are currently disambiguating, or memorizing things. We have reflexive attention, the attention process makes a perceptual assessment of whether it's actually the attentional process. So the process is aware of its own role within the entire cognitive architecture and model steps. And it's also interesting that our attention agent is not generating the motive force, your consciousness is not generating the motives of your action interests. Retribution, redistributes the motive force of your motivational system, by attracting attention, you're there. So in some sense, it's like a conductor in an orchestra that is not imposing its will on the orchestra. But imposing what is has to be played this evening onto the orchestra in the best possible way. But it's submitting to an aesthetics that is generated outside of the conductor. Right? So consciousness is not running the show. It's just directing part of it. And so what I mean is here that this attention agent, is actually the thing that facilitates the phenomenology of consciousness in our own mind. That's it. This is what I wanted to tell you today.

Unknown 27:49
Very, very, very exciting. I got a couple of questions and or does anyone else like to go first, I'm happy to have somebody else go first. They want,

Unknown 27:59
I just have a comment. And actually, it's a very, very exciting talk. And I really appreciate it. And I actually appreciate what you're trying to do. Because the structural machines, we were also exactly coming up with this hierarchical conductor and you know, agent architecture. So essentially, you have this body, and then you have the brain. And then you have the mind, and the mind is the conductor, etc, model. And so I think you're really right on target. And you seem to have caught essentially the same super symbolic computing that integrates the symbolic and sub symbolic computing, basically, you're doing algorithmic processing of agents, and then you have conductor who actually manages both, and gives you the freewill at the conductor level, and so on and so forth. So I think so exciting. I'm going to read your papers.

Joscha Bach 29:03
I think we have slide terminology differences, for instance, I would say that the mind is, in some sense, sometimes the software substrate that is facilitated by the activity of the brain. And it's, it's the entirety of the processes within itself, and agency and consciousness and perception run. Right? So

Unknown 29:22
it's a knowledge network, essentially. And all of the life's processes are events that are filed by wild connections, basically. So it looks like you know, very elegant model.

Unknown 29:39
You have a couple of questions. What about using something like capture data, where you've got hundreds of 1000s of images and texts that are articulating relationality and situations? I'm just thinking back to the animals but what with Capture, you've already got multiple love will sort of, there's already kind of a knowledge relation in a certain way, if if I could say that,

Joscha Bach 30:08
I suspect that the majority of captures can be sort of resolved without any kind of analytical thought, or the most captures, or just obscured perceptual problems, I see don't need to reason to resolve most of the captures. So we put basically need to look at those captures that require you to make a deeper inference about the structure of the universe that you are in to resolve them.

Unknown 30:35
And also, I was thinking of Kurt Lewin's topological psychology, are there any models based on on his thought?

Joscha Bach 30:48
I don't know him. So

Unknown 30:51
I'll send I'll send a paper to another area is abductive logic from Charles Sanders purse? Are you doing anything with abductive logic as a methodology?

Joscha Bach 31:05
I'm not working with this. But I think that if you look at what natural language models are doing, they implicitly have objective reasoning in them. So you can have zero shot learning where you try to make inferences about domains that you don't know anything about so far, but no relatively little about it. So you're basically trying to fill in gaps. And this is what these models are able to do. To some extent, what these models fail at doing at the moment is, I think, mostly, they're not very good at interacting with the world in the real in real time. So they cannot conduct experiments on the universe to see whether the conjectures that they can come up with are any good, right? So GBTC is able to confabulate. But it's not able to verify its configurations. Neither theoretically. So it's not able to develop new mathematics, because it cannot make deep reasoning and inference. And it cannot also also not make correct inferences about the world that it would be able to verify in any way.

Unknown 32:04
And one last, do you have a hunch about generating a meta attention agent?

Joscha Bach 32:11
So I think that our own attention is in some sense, a meta attention, but only if you are seeing attention in the transformer as we are attention I'm not sure it is, I think that transformer attention is basically just selected statistics. And I think that there is more to attention in our own mind, then select the statistics because our own attention is integrated, right there is a Nexus or locus of our own attention. And this locus of attention allows us to retrieve previous instances of where the attentional system has been attending. So we perceive our own attention as something that is continuous. But we know that this continuity is a fiction to some degree. We also know that the stream of consciousness is what defines us as attending agents, right? So this is what we would need to understand. Now I don't know what if you define attention as this integrated model of the world that has direct awareness of content and reflexive attention and the attention of the access mode, what would matter attention be beyond reflexive attention, that you can make attention its own content, so you basically can pay attention to the fact that you are paying attention, and you can remove everything else. So in the meditative state, you can basically close the attention loop to the point where you have no other content, but the reflexive attention itself, it's a relatively veered state, you could say this is core attention. And this is, in some sense, necessary and sufficient for the type of attention that we have, even though that attention that only directs on itself has no functional purpose. I mean, it's not interacting with the world.

Unknown 34:00
For me, you know, we've been talking a lot about ganz today, and so on. And one of my arguments is that these AIs don't really have a sense of the context that they're dealing with it. And so I don't know if that's a meta level, or if that what I'm interested in is how through multimodal learning through what I call pattern, pattern flows of I wrote a paper pattern flows, hybrid identity construction. So if you think of a little kid, and you hold a hand at a red ball, they see it and then they put it in their mouth, so they taste it and they smell it and they feel the texture of it. And that's their first instance of red baldness. And then that happens again and again and then and then that category of red baldness builds out and for a whole lifetime, it keeps building out and building out. And so I'm sort of that's why I'm very interested in multimodal Learning. But multimodal learning could could be done in more abstract ways as well, I think using statistics or using,

Joscha Bach 35:11
it has to be in some sense in our own mind. Everything that the brain is doing is doing statistics of electrical impulses. And from the perspective of the brain, I don't think that the modalities are different by the socket by which the brain connects to the outside world, but like a camera socket, and an auditory socket, but everything, in some sense is routed to the thalamus. And the brain is able to sort out these impulses based on the statistics that it gets. So it doesn't have to rely on all the nerves in your body circuits coming into the brain into the right order. But it just looks at the statistics of the nerves that come in, and it groups them in the first step by adjacency, which means if there is a co occurrence and firing, for instance, by nerves on your body surface, it means that there's a high probability that you're looking at adjacent nerves, right, because when two nerves very often fire at the same time, it means they're being excited at the same time, which means that there's probably a common cause that excites them. Right. And in this way, you can just by measuring the CO occurrences of signals, or doing statistics over them, discover that your body surface is a two dimensional map. And a subset of the nerves that are connecting the brain to the environment, are originating in this particular kind of map, right. And once you have that map of your body surface, which has high density of nerves for your tongue, and a very low density of nerves in your back, so the area in your somatosensory cortex that represents the back is very small, verses the relative area that represents the tongue is very large, you will discover that there is a deeper regularity in this map. And this is the objects that are moving over this map, right now that you have the map, you can classify the blobs of excitation that move along this map, like when you take a finger and trace your body surface. And this allows you to discover the structure of objects in space, and the alignment of your skin in 3d space, right. So you now have a second map that, in some sense, corrects the first map and aligns it with the space that surrounds it, like missed the floor, then the baby rolls around on the floor. And basically, this is when the baby learns that its back is actually large, and the tongue is small. And then it's also going to learn the shape of the objects that touch it by touching them, right. And the modality is fused, when the babies realizes that there's different statistics of the different modalities, at some level of abstraction, will have a common origin but have a common model, right when you notice that the objects that you are seeing are the same objects as the ones that you are touching. And that's requires that you have like a fifth or second order, sixth order model something in the in this area. So you have multiple layers of abstraction, until the modalities meet, but they will meet if you are able to make these statistics, right. And so at some point, all of the modalities meet. And this is when we have a cohesive model of the universe and can make inferences across domains, which means that everything that we are seeing in one domain is going to constrain what we see in the other domains. So the convergence is going to be much, much faster, right, because we now get more constraints on the data that you observe. So multimodal learning is actually easier than single modality learning, because the convergence is so much faster, it's harder to reason about it. But if you're able to come up with a general mechanism to build the architecture, and to give them a reason to the machine, right, then it's much much easier to converge.

Unknown 38:47
That actually requires trial and error, right. So you actually are trying to derive some behaviors from your observations, and then you are creating your model, and then you are testing that model, if that model is correct, it works, then you reinforce your model, otherwise, you will, you know, discard that model or change that model. So in order to do that, you really need the trial and error, you know, action, reaction, action and reaction. So by actually one question I have is that the behavioral extraction from the data instead, we also have a lot of knowledge already about the domain. So how can we use that domain knowledge and augment the deep learning, you know, data, the models the insights from the deep learning with a common representation that already gives you the context because the brain is really not a blank slate. You know, baby is not a blank slate that starts learning all of these models. It comes with already pre programmed physics pre programmed, you know, a lot of things that it knows genetically how to do it, right.

Joscha Bach 40:03
Yeah, I suspect that the baby doesn't have a complete physics, but it has this, lots of pointers that make the right vergence efficient, right. So in some sense, the model is biased towards converging to the correct physics. But maybe it says more degrees of freedom. So for instance, it turns out that if we draw people into a virtual world, they can learn to navigate a hyperbolic universe. So it is non Euclidean geometry. And we can also make sense of simple four dimensional objects if we get enough sensory data about them. So our perception is not constrained to the world that our ancestors grew up in. And we can make sense of additional worlds and not additional modalities. Thanks. Now, to your question, how can we combine this, there is, of course, a lot of work that is trying to mine the categories that are being developed by deep learning of use things together. Ultimately, I think it makes sense to focus on knowledge. And knowledge is a representation that works across models, right? Your perceptual model depends on your particular perceptual interface. And everybody's perceptual and action entanglement was developed, right, your effectors and senses are slightly different for every individual. So your brain is going to come up with a specific solution that works well for your organism, but cannot easily translate it be translated to another organism, so had to be in trained on every other organism. But using language, we are able to translate contents of models between individuals. And this requires that we basically converge to a language that is designed from first principles. So it can be anchored in every other model, right, because we don't actually have a shared universe, the only other universe with somewhat isomorphic properties and similarity between the processing of these properties between individuals, but it's not the same. So you cannot just take software from one brain and run it on another brain, like you can do with this better standardized computer architecture. But what we can do is you can learn a grammatical language that takes into thin air between speakers. And that's only possible because our natural languages are approximating mathematical abstractions of language, that the grammars that we are learning are mathematical abstractions. And every natural language is built around such abstractions. And that while these abstractions are not static, they are fluid, every generation language changes, every pair of speakers will have the impression that there is one right way to speak. And this is what we try to approximate this is a constraint that is necessary for making languages converge and language learning converge. And this is the same thing for our logic, we have the impression that there is one right way to to logic and defy it until we find a solution. And then we are able to talk about the world that we perceive by translating the perceptual knowledge into second order knowledge into propositional abstractions that we can express as a discrete string of symbols and exchange between speakers. And I suspect that the need to be doing a similar thing, in some sense, we need to teach our machine learning models, how to express their knowledge as symbolic abstractions. So it can be exchanged. And these symbolic extractions need to be grounded in mathematics.

Unknown 43:37
Yeah, I think that is what the structural machines, you know, the knowledge structures by gender theory of really gender they have information from Mark Burgin actually proposes, right? It's essentially all entities, relationships, and behaviors and behaviors are basically communication between entities, right. And so even a communication can change the local behavior. And that local behavior can trigger many, many other behaviors by communicating to other connections. So it's the Knowledge Network, where a single parameter change can create a workflow of exchange of information and local processing, which is really the local behavior change. So you can actually impact inflict local behavioral change just by mere communication. And then the Global change is a workflow of all of these local changes. And you need a higher level of management of all of this stuff, because now we don't have shared knowledge. And so you have to sort of broadcast the right information to the right people so that they can take the right actions and so on and so forth.

Unknown 44:51
I wonder if you've written a paper listing your first principles that you described earlier. Which ones I mean, you mentioned the idea that there are first principles need to be used in order to, now I can paraphrase it that you would use first principles to help you define the law or the mathematics of the law. So

Joscha Bach 45:17
this is a something that still needs to be done. And I'm currently thinking about the right way to do this, and are the right venue to do this. And, yeah, if I find enough time, I'll do it. So I suspect that the entirety probably needs to be put into a book. And there is this issue that in our culture, we don't have a way to talk across disciplines about the ontology of the mind. And that is a big gap in our own culture. And I suspect it's because our own culture is confused about the way in which the mind is organized for historical reasons. So it's very difficult for us to converge on a shared understanding as a result, even many neuroscientists are actually dualists. And, and because they know that dualism is nothing that you can properly discuss in public anymore, if you are an purpose entity company. Basically, a lot of people will feel that it's unscientific to discuss consciousness. Right. And I think this is the main reason why there is so much reluctance historically in Western psychology, and neuroscience to discuss consciousness, and why the existing discussion of consciousness is often so bad.

Unknown 46:35
I think that is changing, though. I have seen a lot of talks in just this conference on consciousness, right? Yes.

Joscha Bach 46:41
So I think yes, I think it's really a renaissance of consciousness. But exactly, we have a lot of people that belief in say, unscientific ways or philosophically unsound ways to deal with this consciousness that basically a lot of superstitions that are that are incompatible, is the very basic epistemological paradigm that you should assign confidence always in, based on the evidence that you have, right? So every single, everything that is not impossible is possible. But it's likely only to the degree that you have evidence to support it, or your confidence

Unknown 47:20
has changed quite a bit with Damasio and Stan is last, you know, there are pictures and neuroscience and so on, right maps, and so on and so forth. And it picture is emerging now, at least that, you know, there are certain ingredients you require for.

Joscha Bach 47:37
For instance, I don't think that Stanislas Dehaene, at least to the reader aware has advanced a cohesive theory about what consciousness is. So the passive audience is a bunch of research topics that are related to consciousness. And as the hope if you pursue these topics further, a coherent model of consciousness is going to emerge the least efficient again, I don't think there's anything wrong with this approach. But there is he does not have a systemic theory about what consciousness is apparently, and there are no bars global workspace theory is not detailed enough to go there. Right? There are more detailed theories like it, which I think is philosophically unsound and downward, right, but it's better,

Unknown 48:28
right. But there is a new theory is required, which essentially is like, going from candidate theory to statistical mechanics and things like that, you know, when are quantum theory or special theory of relativity from Newton, Newtonian physics and so on. So you need a new theory that essentially brings all of these observations together. And then finally, that would be, you know, sort of a higher level theory, that this all these lower level efforts, like workspace, and ID, and so on, are looking at some views of this bigger theoretical framework of the consciousness

Joscha Bach 49:07
and suspected, since this is mostly a philosophical problem, which means you have to connect the dots over things that people have been observing already, for 1000s of years, or 10s of 1000s of years. We are not the first culture that has successful attempts at solving the question what consciousness is, but the problem is that all the other attempts of solving good consciousnesses have been written on top of a different ontology. So it's hard to translate them into our culture, which means if you take a text from Sanskrit, that deals with consciousness and try to translate it into some thing that William James believes or something that yoga teacher in our cultural beliefs, it's not going to make a lot of sense because the ontology is not there, right. There is no conceptual framework, in which you can properly translate the text of Sanskrit so the translator is going to translate this into the otology that the translator already has. Right? So it's similar to somebody told me that Japanese animism believes that everything in the universe is alive and conscious. And I said, this makes no sense. I believe that Japanese enemies will have different concepts for a person that is conscious, and for a person that is unconscious, and for a person that is alive and for a person that is dead, right, so the enemies is not going to say everything in the universe is unconscious, except for an unconscious person. And everything of universities in life, except for a person that is dead. mistranslated the terms,

Unknown 50:41
we've been through the same history in physics, we went through the same history in all sciences, right. And so the same thing,

Joscha Bach 50:50
Sanskrit in culture in India, applicants, the Sanskrit texts written by previous civilization, and then there was a civilizational break, and at some point, for instance, Sanskrit, you will have texts about periodic processes. And after there was a civilizational breakdown, nobody knew what a periodic process is anymore. And suddenly, this was translated as real. And you had lots of monks sitting in monasteries meditating about wheels. And you have similar things in our old culture, when the Roman civilization crashed, it lost most of its literacy. And for instance, it took more than 1000 years before we had medicine at the same level, right. Paracelsus, the famous Western doctor was somebody who will live 2000 years after the Roman Doctor Celsus, after which he named himself because he had hoped to have achieved the same level of medical understanding. And so this civilizational breaks that happened, then a different culture takes over that has a more brutish epistemology and destroys dimensionality of the previous civilization is not a

Unknown 51:57
question. I think that any has a question.

Unknown 52:01
Yeah, I, you know, I've been tracking the neuroscience of consciousness for about 25 years now. And you know, I've gone to conferences with the Association for the scientific study of consciousness, and, you know, or Coke, coke, and lots of other people. And I've written papers on on the relationship between neural activity and, and auditory perception, it's not as bad as you make it out to be, we can look at the differences between the anesthetized state general anesthesia, and what's going on normally, and what's going on in the waking state, or what's going on the sleeping state versus the waking state. And we can look at what what happens when you have a masking paradigm where half the trials, you see an image and consciously and half the drought, you don't. And there are, there has been built up a theory of neural activity patterns that go along with the, with the waking state, and then they and, and the contents of, of registering patterns phenomenally so and that involves global recurrent activity that's self sustaining. So in the way that I think, when you complete a global loop, and you have a self sustaining pattern of activity, that that can support a, a, an ongoing, sustained, state resident state, that corresponds to the contents of your awareness at that point. Now, obviously, these theories need to be developed. But it's not like there isn't a lot of there is a lot of data. If you look at what's going on the brain, you know, across these different states of conscious awareness, the biggest problem with consciousness is that there are 1000 different definitions of it. And people are not clear about what they mean by by conscious consciousness. You know, and I mean, in a very narrow sense of conscious awareness of phenomenal consciousness and blocks, terms. But, you know, Searle says the difference between when you're asleep or anesthetized and when you're awake. So I think we eventually will understand how brains work, we will understand what are the differences between the anesthetized state and the in the waking state, and we will understand the differences in neural activity patterns what what are the requisites for you to be aware of anything? I don't know whether it will come sooner rather than later. But you know, I think in the next few decades, it's likely that that will occur. And then we can look at other systems to see how similar they are to the organization of our own neural information processes. And we can make some judgments on that basis. But, and I fully believe that we'll find that all animals have some sort of basic conscious awareness. I don't know about sponges, but

Unknown 55:37
I'm remind reminded, in physics, right, when we were arguing about whether light was a particle, or whether the light was a wave, and then eventually we decided on to the bar, it was both particle and wave, right. And so the same thing that is going to happen in with consciousness, because I think there is, you know, all aspects at different views of some bigger model. And we are only,

Unknown 56:02
well, the neural, the neural, the neural global workspace models are supported by the neural evidence, you know, these hot theories are not connected to the what's going on in the brain and lots of other sort of ontological and philosophical theories really aren't connected to what's going on in the brain. And I don't think that many neuroscientists are Cartesians, you know, the last one was, was popper and the Australian. They were looking for, for some neural, they were looking for a neural nexus in, you know, somewhere in the forebrain. And they and no one found it. And so no one really believes in a sort of strong Cartesian to substances view of things. I think that that's a projection onto onto neuroscientists,

Joscha Bach 56:57
I suspect that the Cartesian Dualism, as it's commonly understood is a mispricing of what's happened. It's driven by the intuition that what we perceive is the real world. But the person what we perceive is not the real world, we perceive is a set of functions that our brains generates to explain the real world, right? Yes. And so this type of these models that our brain are generating can be categorized into two classes. One is stuff in space. So our world is a game engine that the brain is generating that has a model of space that is encompassing the entire universe. And we have stuff moving around in that space. And this is rest extender. And then we have everything else ideas about that stuff. And this is resque Cocteau, twins. And these two interact somehow in our own mind, but they are two categories of knowledge of representations visit the same mind that it's not that one is out in the world, and one is parallel to the world. They're both being generated in the brain. And in this way, we have a dualism, but it's a two dual process to Buddhism, it's a very different reading of what Descartes has been tried to agree,

Unknown 58:11
I agree with your general assessment that the brain is a correlation engine, the more the more different senses that are correlated.

Joscha Bach 58:19
I don't think that we fundamentally disagree. The thing that might be slightly disagree is the status of understanding currently in neuroscience. When you give this example, this circle, this is indexical definition, right? consciousness as being the difference between somebody who is an SSIs versus somebody who is alert at its get this point, and it doesn't explain the structure.

Unknown 58:44
I agree. I agree. But it does the

Joscha Bach 58:47
same is true for most of the neuroscience stuff. So when you're saying consciousness is when this area of the brain lights up? Right, this is doesn't explain what's going on now.

Unknown 58:57
But certainly,

Joscha Bach 58:59
in your fMRI in your

Unknown 59:02
pattern of activity, yeah, no.

Joscha Bach 59:04
So now the thing is, you can often have this coherent pattern. In for instance, in the sleep Walker, and the sleep Walker is performing coherent actions, like opening the fridge and making dinner but there's still nobody home. Yes, and the difference here is that it's probably difficult to pinpoint it, please, the current tools of the neuroscientist

Unknown 59:25
questions reminds me of the blind man, the blind man and the five blind men and the elephant, right. And so everybody is looking at single part and they think that is the brain that is the elephant, right? And so it's the similar kind of thing here. I think workspace theory is looking at a very small piece and is trying to explain that small piece on how to you know, how to be shared knowledge and how to how does it broadcast to various items and so on. So

Joscha Bach 59:56
if you haven't, or if you haven't gotten to the solution that A light is both a particle and a wave, the problem is that light cannot be both. That is the big issue that was possible and is a solution is another one, you discover that the substrate of light is not space, right, the solution must be that light is not in space. And it space only exists in a certain approximation, but that the universe is fundamentally not spatial, or that the photons are not moving through space. And if photons would be moving through space, they could be a wave or they could be a particle, but they cannot be both. It's impossible. This is the reason why somebody is still talking about if you haven't just said, Okay, it's post, it's not post it cannot be post it can only be one experiment, it should

Unknown 1:00:42
be context.

Joscha Bach 1:00:46
If light is both a particle interface, and cannot be both the particle interface, no solution there is, you know, cannot be right, that was wrong. Right? It's something else, like particles.

Unknown 1:01:04
It depends on the observations, taking the shape, depending on the context. Similarly, you know, the

Joscha Bach 1:01:12
blue to optimistic, this is not a solution, you don't understand it, if you actually try to read it. The point is, that's why we were discussing the double slit experiment, even today is light cannot be both a particle or a wave. It has to be one or the other. Experiments show that yes, yes, the Experiments show that depending on the context, it's false. And even retro actively, you can change it by changing retro actively the way in which you interpret the data, you can change it. So that's the confusing thing. And the solution to this is that the thing that we thought was the invariance here, the space in which the lead interactions play out, is the mistaken assumption. So whenever you have a paradox, it's not because the universe itself is paradoxical universe cannot be paradoxical. It's because your representations contain a contradiction. And you have to resolve this by changing the structure of your representations. And finally talk about consciousness. So this feeling of what it's like, but what you probably agree with is that when you say this is what you mean, Mr. chorionic right. That consciousness comes down to this phenomenal experience this particular thing of what it's like to be experiencing something, we have to explain this in a particular way. And we have to explain this in a way that captures necessary and sufficient conditions, not just correlates or pointing at it. I agree. The question of whether that's possible is still something that is debated and not generally accepted in the scientific community, at least in our culture?

Unknown 1:02:48
Well, there's no, there's no reason why you can't find causal relations between a public observable such as neuros neurophysiological, patterns of activity, and private observable. I

Joscha Bach 1:03:03
personally agree with you. But let's be aware of the fact that a large part of our communities does a degree. Well, Christof Koch is probably wouldn't agree.

Unknown 1:03:13
I think people actually

Joscha Bach 1:03:15
Christof Koch thinks that I've heard him say it multiple times in public, maybe it's changed this opinion that a computer can never be conscious.

Unknown 1:03:24
Well, there's a question of whether the computer has the appropriate organization to be conscious computer

Joscha Bach 1:03:30
whenever I have an arbitrary organization, that's the point of doing other morality, it can implement any kind of causal structure.

Unknown 1:03:38
Right? I mean, people who are still thinking that church Turing thesis explains all computing, right? And so to me, I think there are always people who are, you know, they have their own views, and they don't want the change. And there are other people who will take it forward and change like yourself, I'm sure, you know, come up with the new theories, and people will accept it eventually. So just wait,

Joscha Bach 1:04:07
I don't think Jesus explains anything. The church Turing thesis is a claim about automata. About it's a mathematical theory. And basically, it's a claim about the universality of certain classes of automata, which means that once you have an automaton that is able to describe state transitions by using a table, this and it has no resource constraints, then all these automata that can be described in this paradigm are equivalent, and they can describe all possible causal structure. It doesn't say how to do this. So the church utilities is doesn't tell you how to build an efficient, efficient learner. It doesn't tell you how to do function approximation on the Turing machine. It just is a claim that power the existence of a class of systems of computers and computers are Now the substrate independent of what you're doing on it, which means that everything that can be described by transitions between finite sets of states can be described by a computer. This is all there is to the Turing machine. It's mostly a terminology, terminological thing that happens there.

Unknown 1:05:18
We had this debate in the early days of artificial life, people were claiming that they could have life in a simulation. And it's a question of description versus realization. You know, I, I, my, my working on ontology is a homomorphism, that you have to have a, the appropriate organization embedded in the material substrate to actually generate real functions and, and conscious awareness. And I think Koch actually has come to this view as well. That and the problem, the problem, I do believe that we could have consciousness in machines, but they would need to be structured in a way that the that will give us the same or functional organization as we have when we're in the waking state. So there's an information organization, it's got to be embedded in some material substrate, but it's got to be the right organization. Because when we're under general anesthesia, that organization is disrupted. So the question is not whether computers or devices or robots can be conscious, it's it's, what kind of organization do they need to have in order to be conscious?

Joscha Bach 1:06:49
Yep. I think to add insult to injury, I think that Christof Koch has backwards, I don't think that physical systems can be conscious, I don't think that humans can be conscious, I don't think that brains can be conscious. I think that consciousness exists only virtually, it's a virtual property, it's only a simulation can be conscious.

Unknown 1:07:09
It supervenes on the organization of neural activity,

Joscha Bach 1:07:14
yes, then your activity is producing a simulation. This in that simulation, there exists a simulacrum of an agent, Agent experiences things because it's being written into the model by the organism,

Unknown 1:07:26
the language of simulate sim simulation is not great. But But I think I think we actually agree on this.

Joscha Bach 1:07:38
I think I do because it's correct,

Unknown 1:07:41
possibly might mean greater probability of incorrect if we both agree on

Unknown 1:07:51
the computer?

Unknown 1:07:54
I think of the physical system and the consciousness co arising basically, do you think do you agree with that idea? In a certain way, it's a way to take a dualism and make it into a modernism.

Unknown 1:08:09
pantheism versus pan psychism versus emergent ism. You know, whether you think that consciousness, the the properties of consciousness, go all the way down to all infuse all parts, all aspects of matter, material organization, or whether you think that, you know, consciousness was an emergent property of the universe, when the first nervous systems, let's say, started, started organizing themselves in a way that they had reverberating activity and essentially recurrent neural workspaces. So now, you know, that puts, that puts the origins of consciousness back maybe, what 600 million years or something, you know,

Joscha Bach 1:09:04
or I don't find this argument very plausible. So I understand that pan psychism is mostly two things. It's oneness, it's an attempt to deal with the fact that somebody cannot see how consciousness would emerge from patterns in neural organization. And therefore, it needs to be intrinsic to matter itself. This is one reason to be a pen psychist The other one is the well you

Unknown 1:09:28
could say that there are different levels as well. The other one is, awareness arises when you have these neurons that are that are organized to signal to each other. You could say that

Joscha Bach 1:09:43
there is no practical evidence for Pan psychism. And I suspect that people will still

Unknown 1:09:51
or disprovable so just show I had

Unknown 1:09:53
a question for you. Actually a question for you basically, do you think that consciousness that is at one level or there is also a hierarchy of consciousness, that subconscious and various levels of various levels of consciousness, different portions, and they all go through some kind of a hierarchical reasoning.

Joscha Bach 1:10:15
I think that it's possible. So you could have Conscious organization on multiple levels, it's conceivable that our is conscious, multiple conscious entities in your own brain that are serving each other in some sense and don't know about each other because they don't share protocols. So they're not able to know what the others pay attention to, and so on. But there's very little evidence for that actually happening. But it might be the case.

Unknown 1:10:43
Well, there are Thomas isms. So you know, when you begin to learn a skill, you're very conscious of all the all the things that go into it. And when you become completely fluent in it, all those details are, you know, delegated to neural assemblies that have been tuned up, and you're no longer aware of the things that you're doing. So that's sort of an example of that kind of thing going on where things are taken out of your consciousness when they become automatic.

Unknown 1:11:18
So I have another question for Dr. Shah. Basically, if you think that this hierarchical consciousness couldn't be a model that is possible, then we could actually implement that kind of a model in computing structures, basically, I can essentially now have some kind of cognitive agent that different levels in different entities, and they can communicate with a higher level, organize the agents, and those communications can now you know, process across multiple domains, and so on and so forth. So therefore, I can actually infuse cognition into my artificial intelligence system, rather than worry about, you know, generating consciousness from bottom up.

Unknown 1:12:12
Okay, so maybe this is a natural place to end the talk. Joshua was really fantastic. And thank you. Thank

Joscha Bach 1:12:21
you. interesting discussion?

Unknown 1:12:25
Yeah. So let me know if that is possible, right, infusing the car vision into a machine with a model that whatever model you think of what consciousness is,

Joscha Bach 1:12:36
I think that we need to have a pretty concrete model that explains the phenomenology first. And I suspect that Michael Christiane was on the right track when he says that consciousness is the control model of attention with very specific properties. And I think that what it comes down to when we ask ourselves whether a cat is conscious or not, is, does the kit act on a functional model of its own awareness? So basically, does the kid know that it is aware and act on that model, and then the cat is aware of that I would say that the cat qualifies is conscious if the cat is not aware of that the cat doesn't. And I suspect not every nervous system is going to be conscious in the sense that it's going to maintain a model of its own awareness that it actually acts on, in part then uses this for its own control. Many nervous systems, from a certain degree of urbanization onward will do that. And maybe it's also possible that after a nervous system, or an information processing system goes beyond a certain scale and is able to automate its all its activity without attentional privileging and integrated protocol making, then this these systems don't need to be conscious in order to perform. And it could be that consciousness in the way in which we have it is a particular solution that exists in a certain stratum of organism, but not in others. And I don't know that, but I think that's plausible. So basically, I would say, we would need a functional definition of what we mean by consciousness, this functional definition needs to explain the necessary and sufficient conditions for the phenomenology of consciousness. So this feeling of what it's like, we need to explain what we mean by our own consciousness by the theory that describes necessary and sufficient conditions. And then we can verify whether these necessary and sufficient conditions are being fulfilled in particular nervous systems.

Unknown 1:14:38
Yes. How do you spell the author's name I use the attention author gratiana.

Joscha Bach 1:14:47
I spread it into the TED. Oh, thank you.

Joscha Bach 1:14:55
There's also an interesting paper by Joshua Bengio for you which I think is complimentary. And the call says the consciousness prior. And what he suggests is that our conscious attention is a low dimensional function that is acting as a regularizer on our mental states to create the largest possible dip in the NFC function, what this means is that when we look at the world, we have to disambiguate it, that there are many possible ways to interpret the features of the world that we are seeing. And the our attempt to disambiguate, it will require us to assume additional state that we cannot directly infer. And we need to identify the state that we need to impose on the world. So it makes sense, right, so the selection of states and it's basically a low dimensional function that is going to manipulate part of the representation in such a way that our mental representation snaps into a global optimum of prediction. And, and that is what he sees as the role of consciousness. So I write this down.

Unknown 1:16:05
Excellent. And also in terms of the ontology of mind, are there other? Are there authors that are chipping away at this now that you recommend as well? You know, articulate this ontology.

Joscha Bach 1:16:19
That is always done it. The issue, then it is that why he is correct. He doesn't seem to be addressing the parts that the doubters have issue with right, doesn't write much about the terminology of experience. And so I have the impression that Dennett and his critics are talking past each other. It's almost as if Dennett doesn't have a lot of fundamental experience and things that is nothing to be explained.

Unknown 1:16:45
Interesting.

Joscha Bach 1:16:50
And, by the way, I think that your fundamental experience can be transcended, right, you can take the scene that you're looking at, and you can conceptualize it until it appears to be perceptual, so stops being perceptual. So for instance, you can sit in the car and drive and you can make your presentations transparent until you no longer see cars, but only geometric shapes. And you notice that these geometric shapes can be interpreted as cars, but don't have to. And you're only while driving interacting with, say, the difference between the oval lights of the car in front of you. And when they go further apart, you know, you're getting closer, and you have to push the brake. And that's sufficient to interact, right? So you abstract away from your immersion into a real world into simple geometric abstractions. And then you can basically notice that everything that we're seeing is just simple geometric patterns and textures and dynamics between them. And once you realize this, you decompose your own qualia. And that's, I think, possible for any kind of a normal experience. And in the sense, I think there are probably people which don't have a lot of phenomenal experience.

Unknown 1:18:02
This is a very different topic. But what do you think of Luke Steele's work with robots and language? Do you know this?

Joscha Bach 1:18:09
I absolutely love Luke. So I like it. Yes. Great deal. Yeah, very sad that his experiments did not really converge. And don't think it's because it's impossible, but because he's just one man. And like this idea of joint language acquisition, I saw that the robots were actually a distraction. I don't think that he got much out of the embodiment that he put into this, the talking heads, but not interesting robots to me. So there was basically there was more exploration than actual thing, you could have gotten much, much better results in simulations, I think, in virtual worlds. But this was countered to the embodiment ideology that he was trying to advance as part of Sony, and the lab and Brussels. And I saw this construction grammar program program by which he hoped to extend the discovery of language beyond nouns into the joint discovery of grammar. I don't think that he's gotten this to work. But I think it's a very interesting paradigm.

Unknown 1:19:12
And

Unknown 1:19:14
Jeff Hawkins, book 1000 brains,

Joscha Bach 1:19:19
I think it's not really correct, but I fail to see the new contribution in it. So first of all, I don't think it's 1000 brains, it's instead it's a reframing of the holographic mind. So it's not that you have 1000 Complete interacting brains, but rather you have 1000 vantage points within the same organization. Right and this this is similar to a holographic representation that you have local representations that are not complete, but that are somewhat complete, just seen from a local point. And the holographic memory idea is pretty old. So it's I think the original introduction was by Connor and so on. And so I don't know how much it adds to this. And the other thing is the hierarchy hierarchy that he describes, in this hierarchical temporal memory, I don't see how this is very different from the capsules that Hinton is describing. And the temporal patterns that the neurons are in implementing, when they look for context, and the environment seems to me to be nothing but temporal convolution. So it basically I don't see what the actual novel contribution is. This doesn't mean to diminish anything that Jeff Hawkins is doing, right? I think it's all valid. It's all super interesting. It's fascinating the way he puts it together. I think it's a productive paradigm. But I don't think that he advances a fundamentally new theory on how the mind works.

Unknown 1:20:54
And you studied with a German scientist who have very few things translated. I can't remember his name right now. And what do you think about his work now? Or has things have things been translated now that are valuable? Would you say?

Joscha Bach 1:21:13
So? I think I translated the core of your theory in the books of principles of synthetic intelligence. And that was an attempt that I made to capture his idea. So if you pick up this book, you can find PDF. Oh, but yeah, so I think then you're, you're probably familiar with the core office theories. Okay. And I have not actually worked very much Miss Dietrich groaners, he's just, he was one of the first thinkers that I came across, who seem to have a convincing cognitive architecture to me that went far beyond what for instance, John Anderson was doing, because John Anderson was neglecting motivation, autonomy and emotion in his models, and perception. Instead, John Anderson was only focusing on abstract symbolic cognition. And he made an attempt to extend this with XR n into neural networks, but give up on this because it didn't really work out for him. And so all the work that existed in cognitive architecture today came across AR T was interesting by Rosberg, for instance, at at the residency, we had the idea of how you will oscillators can perform the entirety of mental processing. But it didn't scale up into a model that would actually do these things, right. And Turner was a guy who was trying to put everything together. And I found this fascinating. And so I studied everything that he did, and then visited him for a few years at his university in Bamberg, and stayed with him for a couple of weeks to discuss this him. And I thought, it's a shame that this guy was only working in German psychology and not publishing in cognitive science and AI. So I thought, let's translate this work that he's done. And systematize this theory a little bit. And I think that most of his ideas over the years has held up for me, but they are still in the construction paradigm of the mind. So in the symbolic AI paradigm, similar to what Minsky has been doing in the society of mind, what Minsky did was he basically identifies all the functionality that the mind has to perform, and then spells it out. And now we have to implement it all. And instead, I think, what happens in a biological system is a meta design problem, how do you build a system that converges with self organization to the desired functionality? Right, and this is very different question in many ways. And all social and biological systems are not designed by implementing the functionality directly. But by implementing meta functionality, if you implement the country directly, the system is going to deteriorate, right? You build an FTA. And after a few generations, it only serves itself. So what you need to build a system that wants to be the perfect FDA, not a system that is the perfect FDA at its inception and then deteriorates. And that is the same thing, but for mind it you need a system that is stable under disturbance in the grow into a mind. And if you scar it, it will try to heal itself.

Unknown 1:24:13
Right. Yeah. an emergent system basically.

Joscha Bach 1:24:16
Yes. Yeah. So you need to identify the meta level dynamics that give rise to all the functionality that you want to have. It's hard for us to reason about this

Unknown 1:24:27
are two of your book of your next

Unknown 1:24:31
I you saying that, if I know the fluctuations and essentially can predict a state that is going to be more stable or reduces my stability, I mean, increases my stability, then I would be able to move myself into that state before it occurs. Right. And so if if that is the case, then really what I need is in order to build some kind For a system that can monitor fluctuations and be able to reason about the impact of that on my biology, either my structure in the case of digital systems, and then I can reason and say, Okay, what is a possible solution, and I can auto scale or auto failover, or whatever I can do, and I'm changing my state. So at that system, I have to have at the highest level, the whole, you know, the analogy of the whole structure. So I have a function, I know how the structure uses the functions, and then I know how to, you know, destroy, monitor disturbances, and change my state so that I can be still stable.

Joscha Bach 1:25:51
Yes. Do only familiar with the idea of Neural Darwinism? No. Yes, the

Unknown 1:25:59
neuroscientist I'm an IT guy. So excuse me,

Joscha Bach 1:26:04
the idea is that there is a competition between configurations of your brain that is basically an evolutionary condition where the most resilient and competitive brain configuration or mental configuration is going to prevail. Right, so you could have multiple photo personalities that are compete over real estate in your own mind. And at some point, one takes over. And it's interesting to think, for instance, about the fact that children, human children have a gap in their memory formation, usually. So around the age of two or three, they have a break in their memories, and they have difficulty remember anything that came before. And it's not that it didn't have memories, before, they had just fine memories before. It's just that at some point, they are not able to bridge over in this previous set of memories. And I suspect what happens there is that when we start primarily thinking through language and concepts, we organize the way in which the index of our mind in which we access memories, and possibly also the way in which we conceptualize our own selves. So basically, at this point, the mind of the child gets taken over by a different organization that is replacing the previous organization as the dominant one. And it's, that's a very interesting example, I think, for this, we will Darwinism at play. And so in some sense, you will have always an evolution between different proto solutions. But this evolution is going to be heavily rigged by prayer. So you have pretty much the same outcome in every individual over the years. But in this perspective, or maybe the brain is best understood or the mind is best understood as something like a colonial structure, like a colonial government, that is subduing the barbarians and forcing it into some kind of globally organized structure. In a hierarchy of thinking and mental resource allocation and reward distribution, that is the most efficient one for controlling the organism.

Unknown 1:28:14
That's the model I'm looking at implementing so that we can build self managing automata, right. So autopoietic machine are a cognitive machine, which essentially knows its functionality, and it knows how the functionality is distributed, and where the resources are available. And therefore I can optimize my resources and maintain my stability, as you know, application level performance or application level security or whatever it is, right. And so that is a very limited application of consciousness or whatever you call, right. It's basically auto prices in some cognition.

Joscha Bach 1:29:00
Yeah, I think you're on the hour. Up. Yeah. Thank you, everyone, for having me for the session. Thank you. I hope I didn't dominate the discussion too much.

Unknown 1:29:14
No, no, it was very enlightening. Thank you very much.

Joscha Bach 1:29:17
Thank you too. I wish you a wonderful afternoon, evening or morning wherever you are in the world right now.

Unknown 1:29:26
Right. Can't hear you?

Unknown 1:29:42
I'm sorry. Um, thank everybody for participating and all of the talks are going to be put up so it was quite a couple of days, wasn't it? Anyway, thank you, Peter. For for your A wonderful talk today. I mean I, yeah, anyway, it's always interesting to follow your career and where you're going with things to mark Bergen for for letting letting me do this special session. It all happened very quickly. Anyway, okay, so great. Take care and I will. Once the chat is off, I'll send it to everybody also
