Unknown 0:00
Welcome everyone to our panel on motivation and autonomy. Today we have some very interesting speaker speakers. We have Christiana Castillo Frenchie. Christian volcania is Dietrich Dorner and your Shabbat. We will be talking about the some of the very pertinent questions such as How do we conceptualize and construct artificial agents with rich autonomy? Can we use additional models to understand agency in humans and in AI agents potentially. So our first speaker today is Christianna crystal Frenchy. Christiana is the director of the Institute for sciences and technologies and National Research Council and the full professor of cognitive science at the University of Siena. His research focuses on the study of autonomous goal directed behavior as the root of all social phenomena as the work as well as how social life shapes individual cognition Christianna The floor is now yours, you may share your slides okay? My side okay. Okay. You don't see them yet. You can't see not yet

Unknown 1:36
strange Why sorry. Are they able to share the screen community stream? So Okay. Okay. Now, yes, now, you just need to enter the presentation mode Excellent. Okay, the focus of my talk or my intervention is the necessity on a hybrid the modeling of the nature of mind or hybrid architecture, both for modeling your mind and for building artificial effects in mind. And these the hybrid nature, I focus on two aspects of this hybrid architecture. One is that we need two kinds of reasons, the reason of the health scholar and the reason of the reason the reason, the reason and the other is that we need to God driven action and behavior and go meta representation versus God oriented the device and be reactive. So, let's start saying that for example, we should go beyond the old BDI model, brief design intention model in which there are different mistake in my view, we need to provide a much richer model have goals and motivation. For example, it's wrong in VDI the use of desire as the generic negative or goals for models, no desire just a separate kind of goals. And they are part of a family of goals that the goal that we feel like needs emotional inputs cannot feed on all our goals and assign going yes, what does it mean to feel that we are receiving a signal from the bug or sensation we perceive our body and these make it real body our body not just a hardware artificial creature just seems to lead to have emotions to have a feeling if they do not be in any the door they do not have a real body sensation can either come here and now from my body, given the information about the state of the body or B and location activation, the memory of that experience right. So, the matter of previous experience of desires are based on fed imaginations of sensation, needs and based also on current body sensation coming now from my book. And I got asked if we should abandon a BDI model for example, is it wrong to consider intention as an independent bicycle student of mine believe desire, intention now, again, intention just a separate kind of goals a kind of goal. More precisely, intention is what the goal becomes in its processing. After we choose and we decide to do something for guiding our goal directed action we become intentional action. So is is double intention that the original part of the original objective reach and desire and intention to do which is the action plan that we were able to find In order to achieve that objective that goal, so, we have two modal goals, the processing and the cans, there is a rich ontology of goals that we need desired need intention objective Project HOPE is dictation and so on and so on and so on, including duties a special kind of goals very different from desires and that they perform on goals duties, and goal originated by norms and can be even disagreeable different from desire for example, and we need oxygen artificial agent normal sense the agent beyond the mind, the auntie representation demand of the agent. So we need the agent able to mentally recognize and or mass or normal, which is different from a personal request a fair an exchange or from a US habit. Enormous anticipation is a goal originated by some prescription all day from our authority that we don't authority, but a norm a real norm for really be representative of in my mind can we can decide to violate it, otherwise not really a norm in our mind is a bind, we can we can decide to obey or to disobey. And this creates some problem with artificial agents able to violate the norm. So, this is an open question.

Unknown 6:31
Other point report is my claim to need to distinguish between teleology through theory of goals the theory through goals, versatility, autonomy, to deal with other form affinities of annalistic behaviors, we are not confused through goal as a mental anticipatory representation, guiding our behavior which becomes an action with other form of interviews, goal oriented functional behavior, but not really God driven mean cybernetic and psychology that sense two goals at the center of mine, the center as per the function of his anticipatory nature, for guiding our view towards we're not going to use do goals in particular with a functions of behavior, which are not mental goals, the unintended outcome that feedbacks and we produce and displaying that feature would that be like in biological evolution, evolution in biology, in which the animal behavior has a lot of functions that are dysfunctional, but doesn't understand and they don't just represent the disease in his mind sank on to this thing which to go from B, the expected result of instrumental behavior is to make a lens Skinner model that is the reward reinforcing that reactive being the model necessary for beaver modeling be the expected result of instrumental behavior and classify or better visibility versified the perspective So, these are reservoir condition actions they want to response matching the condition execution of the action in this very versatile aware not only condition action, the condition and this condition of the result of this rule is a rule based behavior is based on learning previous experience. And it's very important that the reinforcement learning is the basis but this this mechanism is based on anticipatory representation is their expert. When expert mentioned when there is there is an expected result, there is a positive feedback both reinforcing the rule in successively that deviation or enforcing the expectation itself and your prediction believes. However, the X in the classifier is not a goal and the action is not really bad for the goal directed God dreamin although befinner eyes is goal oriented behavior into goal I start the top down from the motivating anticipatory anticipation desire, the desire for example, stems from that searching for how in bottom quote, I have to do can I reach the goal, if I can achieve it is to formulate a plan and then action this is this typical civil netiquette top down strategy. The expert in chrysoberyl is just a prediction. I believe in the future that if you realize feedbacks on do reinforce it. But then behavior and the agent remain just conditional actions given response. They I mean, just the respondent agent array Activity A behavior to the stimuli to the con is about to map the process not super natively top down like in middle ground to the premium model and despite to see Kinetica from the design representation

Unknown 10:16
now, the claim of hybrid mind architecture that we need both we need through cognitive mind a cognitive agent driven by true decision through goal deliberation system to reasoning about choice about karma about goals plan and decide and choose on the liberation agent. But we also need agent in though the system one mechanism meaning the reactive reflexes or the way the response heavies. So, also responding to reactive system not a reasonable one, we need both kind of system in their nature but this system one and system two, contrary to the dominating Richard or are not just parallel, they're competing with each other, no they interact with each other, they interfere, they are not integrated. Consider for example, the model our model of goal directed behavior guided by goal and formulate some goals and provide some background in the plan this is the plan goals and plans, but the lower level the level of the elementary action to perform and not the same structure, they are they are more similar to being networks, they use anticipatory classifiers. So, the real Gordon plan and then execute the implementing anticipatory classifier. So, they build a unique compress many levels structure others feature colonies actually nature, common in mind the cognitive mind is based on representations is pleasing the representation beliefs and goals the two basic important representation and the processing and the interaction in fact that these two basic representation on mind interact with each other for example, Ctrl Z is placed in is grounded on beliefs depends on really consider for example, our model of word processing, in which you did a different step step step step stepping broad processing on the activation of a human bizarro goals, then to the conflict, option selection, choosing goals and then formulation of the plan and then promotion intention then execution intention, the intention or just the final form or format of the original goals that become an intention and the action become intentional, but step by step in this goal processing the belief Filter, Filter, Filter, Filter Filter, and determine the life of the goal and support the goal processing is is belief based. Topic is important is autonomy in aging, we need autonomy, but autonomy is not just exec with the autonomous performance ability, it is gone autonomy, and that is called the autonomous choice through decision. And this gives us the room to initiative, which means doing something without request. Autonomy is v as your initiative. This gives the possibility for over l do it more than I required for a clinic I'll do is something different from my request but better for distribution of deployment so big.

Unknown 13:41
We need the full autonomy

Unknown 13:45
This is God autonomy, in which the agent may have some preferences and decision. The family really got autonomous I have more male dominance, my own autonomous goal and not just an obedient performer on the task. I'm not just able to automate execute the main task, for example, I can decide whether adopt your goal every you or not, where the two are there to your request or not to reject it to refuse it. And we need that Scioto AIA agent for real social interaction for efficient cooperation. Why? For example, because we need initiative in our partner initiative in our robot, which should be based on my reading and reading our mind. You know Donald's the rule Arusha there was a learning autonomous planning, autonomous knowledge acquisition of Solomon and so on and so on. But this autonomous really autonomous agent we have to interact with as its own goal, they have their own goals. And this creates serious drama. And they are danger in the fact that the agent that artificial agent is driven by have some goals, these open ethical issues is not the dream and justify the goal that I put in is my request. And this goal is rooted in the mind of the agent in federal womb, such a goal, they favor the interests of whom and based on which values share value cultural value, which ones say very serious problems. Other point is that the for a human life really social interaction, we need agent a

Unknown 15:33
robot, able of my

Unknown 15:36
reading and mind description for reincorporation of real conflict with our agent, they have to ascribe to us to recognize the goal in our mind and the underlying supporting belief on the basis of which we have that goal. In order to adopt such a goal, you have to understand the present we have Theory of Mind needed to go on to help us embed that also to string that our goal will be to change our goal by persuasion by thread inference mechanism, but in order to do that, in real social interaction should read our mind in terms of goals and beliefs and vice versa. So we need to be the intentional stance and my linear scripture based interaction communication and cooperation. We need this common in my architectural base or representation as president on belief and go and they read in these roles is really important from the mentor for the firm's explanatory the transparency now very trend, but also for truest vulnerability transparency we need among Venus Eclipse based communication and interaction may yell or be understandable or explainable is in terms of cognitive motivation or reason why you're doing what you're doing, for which reason we should believe the system as is not transparent, explainable, if it provides me it's user, not easy engineer, its algorithm, completely obscure rule or a statistic in probabilistic data the patient is learning now, it has to provide me its reasons, the reason why I'm doing that is it's Maldives, the goals and this assumption, they believe, the mounting why do we did so pursue which go provision potential for which decision and preferences and then which is the plan and the light its behavior and there is a norm impinging on that, in all these violation of normal why the reason of his action and the same for assumption, or which base is a reason which believe which data data is not there or which knowledge expectation is decide to do what is doing is doing what he's doing, and then why we cannot really understand all this Do we have to explain in that way, not only way to understand the system be very mental term the cognitive release on probably for a good interaction, they should be Bharata I have to read the mind of my robot, the my agent in mental terms, please design tension decisions, mutations, and eaters to read the mind mind and mental spiral. But these require something more is necessary, then that the agent represent it. So mind the mental issue there ACEF mind being ascribed to in between itself, believe desire and displaying itself, the its behavior in terms of a belief and desire and decision is a mental representation. So, to have a good interaction, we are to this pillar that ascription of God desire and need and we had so we had to mentally share a plan and the way we were not really cooperating collaborating. So, in my mind and in his mind, there is a plan goals believe and I know these are the eternal leads, okay, that jump at this point, cognition and emotion sorry, for around to the second aspect of the hybrid nature of motion with the different kinds of things in our mind, we need to kind of reason a two kinds of ways are in our decision making goal ever double value two kinds of ways. They reasonably value the utility based on reasoning and decision deliberation and the fail to pay attraction repulsion. The first is based on argument, belief can be explained discuss argument taking change, persuade you will change what you believe that that your intention your goal your pregnancies, but the second value of our decision and goals is just intuitive felt just due to some effective location to somatic medicine, I just feel some tears, attractive responses and so on that not justify explainable reasonable preventable, they actually perceive a body, the body and this feeling. And then within cognitive map mind the key cognitive process and value or the goal is no longer just due to belief reasoning, this is better for this concert is better, not just a body sensation, and the more intense the sensation, the higher the value of the goal, the more intense the value the more so, the body can

Unknown 20:37
put a goal in our mind and even assign a variable that goal is not just a goal based on reasoning. Analogous derivation between belief and emotion should be also reversing the classical approach from cognitive point of view is cognate appraiser believing something Have you noticed that the activation of affect your reaction visually their affective response, this is the classical model or cognitive tracer. I know we are also the other way around, we feel something and this feeling induces us to believe something and the more intense divvying they will show what we believe so it's vilandra classical model from belief to affect the response but also the other way around from effect response to believe in some what in psychology we call effect as information what I view as the basis for believing in that dimension we are to correct the magically in the measure view the somatic markets prune the tree of choice in other 10 minutes and they decide what to do on the basis of what we feel is us the idea is wrong.

Unknown 21:51
We are able to

Unknown 21:52
combine to calculate together both the felt value and the reason of the reason

Unknown 21:59
both

Unknown 22:01
marital genius dimensions not clear the mechanism model we can decide for example to do something which is very important from the rational point of view is very usable unnecessary I add to that disgusting oh my god I don't I don't like the don't do my F to do because it's too necessary what vice versa, I can decide to discard some diversity attract the I would very much to do that though I'm not sure you like that this image but I cannot I have to do this is more important business as reusable. So we can combine the two value and one reason that value can prevail on the first one or vice versa or vice versa we combine both is not true that there's somebody market proof that you have the choice and eliminate the choice and we did not attractive we went this is so true that we even ever meet the strategic layer really speak the middle colony level realistic about ourselves in order to manage these two dimension interfering one with the other and we can we have a rules and strategy for stop one or stop for the other one for example I can see to myself stop or go all the way between swimming pools it is really important to stop keep Baku lad nice and about you know the season or vice versa don't stay continue to listen is Gary with this convenience that content is irrational follow your inputs value your input. So, we have a major received the medical billing even for managing is two dimension estoppel one anybody to one or the other one. And as I say, we have go that involved feeling that I can feel like desire or need more or less intensive and go that quitter cannot feed will not elicit any sensation and fee because in price and some pleasure, like or live is our main goal without any pressure, just pressure in base. So that's my main claim on the hybrid nature or stature that we need for modeling human mind and understanding human behavior and also for building new artificial minds and that efficient behavior. We need to come to have the reason of their combined with the reason the reason to system interacting competing converging. And we also need to have a clear distinction between true goal behavior goal planning goal, the process your goals, the goal value and so on can in the action intentional action versus merely gone we entered device reacted device classifier system one and system two and the convergence and competence then cool if you're interested. This is my area says sorry for me to confuse.

Joscha Bach 25:06
Thank you. It was wonderful. It was a little bit too long. We went a little bit over time. I think it's time. But thank you so much.

Unknown 25:14
I didn't listen to.

Joscha Bach 25:17
Yes, it's fine. So

Unknown 25:22
thank you so much, Christiana. So the Christiana has also graciously provided an extended version of the slides, which I will be adding to the link, a link for in the video description once it's on YouTube. So next up, we have Christian bulkiness, who is a professor of cognitive science at Lund University. And he also is the director of the Research School of the Wallenberg AI autonomous systems and software program for Humanities and society, the goal of which is to study the consequences of AI for society. His research focus is on computational modeling of cognitive and emotional processes. And they're using the control of robots. And his interdisciplinary research also looks at the areas of neural engineering, AI, machine learning and robotics, psychology and biology. Welcome, Christian, the floor is now yours. Thank you.

Unknown 26:25
As you realized, my research focus is not very focused. Instead, I tried to understand how many different things connect together. And today, I want to say something about I've tried to approach motivation, emotion and attention, and trying to build models of this phenomenon. There are many, many ways to approach this, these areas, and many people look at the basic emotions or basic motivations, which is very often Moreover, linguistic analysis when actual analysis of processes. And as an alternative, you could have the more phenomenological approach. But that's hard to do something that is useful for for machine implementations or a robot. So that approach I think, is perhaps the most useful is to look at mechanisms and try to reproduce mechanisms and computer simulations in the in robots, I will mention them briefly later on. And when you do that, it always turns out that these labels, things like motivation, and emotion, they don't fit very well. That's not a one to one correspondence between words and processes. But I have tried to ignore that and use these words. Anyway. So very generally, I view the business something like this. So motivational systems, they tell us what to do. It's related to behavior selection. Of course, also two things like needs, specifically to future needs. And maybe like we do sometimes have immediate needs, but most of our needs are future ones. And just as a simple example, something very simple like being hungry, is not caused by starving right now. Instead, okay, instead, instead or body expect us to eat and because of that, it lowers blood sugar, we experienced that as being hungry. And that's not at all the current need. It is a future one, but we will we experience it's right right now, and may select the behavior that that would fit that. So anticipatory nature of motivation is very important that we have emotion or emotional processing. And that's perhaps the area where there are most different views about what it would be the way I say this, that the system would tell us what we should have done, it has to do with evaluating behaviors, evaluating outcomes, situations, and so on. This relates to learning in various forms and things like reinforcement learning in the simplest cases, obviously, emotion also have a master exponential aspect of it that I don't really study. And of course, the social aspect, we communicate emotional reactions to others and so on, but we'll ignore that for now. So one way of defining emotions that I like very much this one was put forward by Edwin rose but before him by a different race. That was that's the idea that emotions are states elicited by reinforcing stimuli, which seems a bit simplistic but but it connects emotion to reinforcement learning and to evaluation of situations. So to the right you can see one simple a There's actually four dimensional space, it looks like two dimensions, but there's actually four dimensions. So the basic emotional signals, it's not emotions in itself, it's emotional value. So one pair of dimensions goes from positive to negative from a pleasure to paint, say, hope to fear if we're looking at more anticipatory emotions. So let's wait one or two basic dimensions, they can actually exist at the same time. Suffering can be very pleasurable and painful at the same time.

Unknown 30:36
Actually, most interesting things in life are not uni dimensional, they have they have both aspects. And then there are two dimensions that have to be dealing with expectations. So if we expect to be rewarded, but we're not, we might be frustrated. And of course, being frustrated could mean receiving no stimulus at all, could be a situation with absolutely nothing. So the frustrations doesn't come from the actual situation, not from our comparing our expectations to what what actually happened. Similarly, for relief, we expect something unpleasant did have happened. So by combining expectation with sort of primary value, in this way, we will get four dimensions, but we can go on forever, which Jeff regretted many years ago and his theory. So we can expect to be relieved, but we will not and that will be a more complex combination of these dimensions. But basically, emotions here here, as I say, it is used for for learning, and for changing the evaluation of situations and behaviors and so on. But emotions can also be a motivating, which complicates things. And motivational states influence emotion. So what's good or bad? To take something simple depends on my motivational state. Again, a simple example if I'm hungry, and maybe food will be a positive thing give rise to positive emotions, but I've already eaten it's completely changed. And of course, that goes for more complex things as well. And I think that binds everything together. When is attentional processing. So that's the attention is to interface between the environment and the motivation and emotional processes and between those processes and the memory. So the way that I view my memory is like a backup for the environment. If we can do something in the moment, we do that, if not, we'll do it in memory. But as I see it, it's basically the same processes got going on. I'll get back to that in a minute. So What emotions do for the attentional system is to help decide what's interesting. It will evaluate evaluate possible stimulus, possible choices as possible objects, and so on. So given that we have a simple situation with some objects in front of us, we could in principle, attend to any one of them. But the emotional system would tell us that, given this motivational state, some of them are more interesting. And we will then be more likely to do something with that object, of course, will also be an influence, emotion and motivation. So if we see something interesting, we'll be more motivated to do something that's related to that, again, taking food example, because it's so simple. Today, we're not hungry, but we see something tasty, then we'll be perhaps feel more hungry, because we haven't done much about action selection here. Because in many cases, stimulus selection through attention is sufficient. In most normal situations, once we have selected an object, there are not too many things to do with it. I mean, we can do a million things. But usually the action we perform is given by the stimulus object itself. That doesn't mean that behavior selection is not important. It just but it's not made in the environment, many simple situations, which may be surprising. Now, so trying to model this kind of processing

Unknown 34:24
the interrupt, but the audience is saying that the slides are looking static. I'm not sure if that's intentional.

Unknown 34:33
Yeah, that's true. I was just about to switch to my next one. So thank you. Yeah. So someone with 20 years ago, we started to model this and here's a new one. So the first picture, what it shows is one, which is a model of of learning in the amygdala, in the brain. And as I said, it goes a long time ago before color was invented. So it's in black and white. It was a simple Little network model for the describe processes in within the amygdala, which is supposed to be involved in emotional evaluation, among other things, and its interaction with the orbital frontal cortex, which is the part of the brain that inhibits, especially emotion and other signals. So one of the things that this model was used to was to describe how emotions become context dependent after a while. So initially, we will just react to something, but when our expectations are not fulfilled, they will be increasingly context dependent, that seems to have been rebuilt with the frontal cortex. Now, this model will continue to work on that for for quite some time. And most people have used it, I actually use it as a control system. So it's used in the industry now in various processes, which is quite surprising. That is not something we expected. But if we look at a more current version, so here's here's one of them, I don't have time to go into the detail. But my point here is more of a today, it's possible to go through the neural anatomy, of emotional processing, and build computational models that actually work and do the things they're supposed to do. So they are so one extension of the model involving things like the hypothalamus, and also parts of the sympathetic and parasympathetic nervous system. So we tried to model for instance, how emotional reactions influence pupil dilation, and things like that. The nice thing about this kind of modeling is that we can reproduce very detailed experimental data. So in the case of pupil dilation, we have been able to reproduce nearly every experiment we found with this model. And even though each of the components here, one on each circle is one small part of the brain, but when we combine all of it, we get a lot of very interesting properties. And we can do a learning experiment, we can we can scale the model, see what happens when, for instance, we shine light and die afterwards, which was surprisingly influenced, and so on. And in this case, we look also on the effect of touch on the trust, and how it motivates approach or, or avoidance behaviors. So this is just an example of a kind of modeling we are doing in my group. And this is not a complete model. But just as an example of what it looks like, I would like to show some some more global views on on the model. So here's one study we did recently where we looked at how social emotional or value processing could influence decision making. So very important part of this model is a picture to the left here and how everything is controlled by a loop basically, starting with perception going through memory, where whatever the model is looking at, triggers both memory processes and associations with value. And this is basically the old amygdala model and in the new version. So when we look at something or even think about something that will be accompanied by an evaluation of where each of those states, if we're looking around us, it will be a variation of similar Rounders, if we are imagining things, it will be an evaluation of that. And when we do that, over time, looking back and forth, or thinking about different things, we propose as virus and accumulate the mechanisms that accumulate value for for those different states or objects, which can then lead to to a choice or one of them is actually an attentional choice, which will lead to spatial attention, which will influence our perception again, and this goes in in both ways. So if we look something at something, we will accumulate value for that if we think about something that could make us attend to it, and so on.

Unknown 39:14
So this is a very dynamic system that continuously works back and forth between the memory and the environment. And what you see to the right here is just a simple neural network for Dewback accumulation for for have a special case of selecting between two different topics. So when we simulated this, we look at various simple choice procedures, but turns out to be very complicated things simply use similar mechanisms. So this is a sketch of an experiment we did a few years ago, where we looked at much more complicated moral decisions. So people were asked things like, do you agree that while there is some cases times justifiable, and we had two choices like yes sometimes or never, and we looked As far as I movements wherever I go deciding, and we can see that, as people are thinking about the different alternatives, they look back and forth. And the hypothesis was was that they would look more at the choice I would actually do, which actually happened here, as well as in simpler situations. But one very interesting thing was that we tested whether we could interrupt people, while we're looking at one alternative, and asked them to make the choice immediately. And it turns out that people are very likely to choose whatever they are looking at if we're forced to choose at that point. So let's support the idea. That's some sort of document processes that are not on here. So we could actually force people to think that the motor is justifiable just by interrupting them at the right time, which is quite interesting. And this fits very well with with the model we're working on. So finally, I want to show this this overview of of an alternative version, where we looked at how value processing could influence memory processes. So basically, there are system for identifying or localizing objects of storing anything working memory, and so on. Also controlling attention through a mechanism similar to watch show previously here. And in the center, that's a value system that modulates everything, so it will enhance processing of, of location or stimuli or fixing working memory that are valuable. And to that we had again, processing, which basically controls random randomness of associations in memory. So that accounts for focus, or even things like creativity, if we increase the randomness, we'll get new memory traces, a combination of things. To the right, this just shows how this can be used, for instance, in a choice situation, so say we have simulated agent in teammates and can go left or right, so it left looks left or right, sees a stimulus. And that starts an associative process within the same within the brain within its memory. And that's what you see to the right, that transitions between the between different memory states. So essentially, it fantasizes or imagines going through the maze, and at some point, it will reach a location like the gold state that has value. And we will then fix that and lock lock onto that. And that could be used to make the immediate shores. So it's links sort of the immediate situation with memory, imagination, and then choice based on emotional evaluation. Because I know running out of time, I will just very briefly share this last picture. As an example. What we're trying to do right now is to take this model based on neurophysiology and gradually building it bottom up, and also putting it into some of our humanoid robots in my lab, so with that, I thank you for your attention.

Unknown 43:08
Thank you so much, Christian. That was an excellent presentation. So next up, we have Dietrich Dorner, who is Professor Emeritus for general in theoretical psychology at the Institute of theoretical psychology at the Otto Friedrich University in Bamberg, Germany. Professor Dorner has also received the Gottfried Wilhelm Leibniz prize, which is the highest honor awarded in German Research. Famously, he is the main author of the cognitive architecture called psi theory. And I believe today's presentation will touch on that. Dietrich, the floor is now yours, you may share your slides.

Unknown 43:57
Well, I'm going to talk about motivation, motivation play ecology is not very much estimated because it is a kind of counter program to cognition and cognition is a modern directional psychology. This is completely wrong, I think, because I believe that I believe it together with Aristotle. The soul is a controlling device for a body which by being controlled at Life is a wonderful sentence because it brings two very enigmatic concepts together. That is live and controlling. And to identify, well, you have bodies yet still ones and whatever think that exists on mode and these are not alive. But if a stone becomes alive and has fear about the car, which Gretchen's don't know Then the stone would have life because he's controlled and he tried to rate the car okay, if you for instance your car then the series first purpose the car and you know either it goes the whole by steering the car, you'll strive for a goal and a simple system which strives for a goal in control. So, good feedback circuit or legal climate in German This figure shows control circuit the tank and this tank should be filled this level of this red arrow and that could be done by open a tab but that must be opened and the tab must be connected to some sort of order to be that works and so, the converter works in the following way the setpoint and the actual value that what is called nominal value to and that is calculated the difference between the setpoint and the actual value and if this difference is greater than zero, then you have a need for something then you need all model data to Phillips and then attend on that it's an action which accounts for the filling of a tank is activated and if everything goes well, then the water tab is opened and the tank will would be filled and this is a control circuit in the normal form in little nor complicated here we invented the ABA tender and these other tender serves to hinder disturbances to empty the tank and this with the other tandem with an action to fight enemies of a tank. This court also good is like living be the tiniest review you can't attend for and if you look for the origin of life, you must find the point where the first natural control circle came in life I don't know whether there are speculations about how the happened but this is a very simple and such control circuits they exist in the normal environment is a not not living environment to there are control circuits in which are not a rich unknown which have no life but access okay. I

Unknown 48:10
have here a kind of example of such a living system. So,

Unknown 48:18
what I would say it's a living system and it has about three control circuits. The first one is for fuel system muscle fuel to heat this tank and then we have water this water is heated and so, steam begins to exist and the steam for instance to have a system which drives the wheels or the steering system here and this system would be like this system could be able to identify the environment. And therefore, we have a third control circuit here that is you must look for places where you could fill up the tank and this is normal a normal environment is gas station, you should be able to identify the specific situation of a gas station. And if you're able to do that you can drive to the gas station and you can fill up the tank again this idea and I just sketched such a such environment before this steam engine car or living circuit or system consisting of three living control circuits. And here we have it. This is again the car here and we have a kind of Thailand with some houses and so on, and some gas stations here from the shell URI. Maybe unknown in the in the States, but it's in Germany and this system would be able to explore this town and find the gas stations. And then when it gets 30 in the water is not it's not a no photo, the system could find a place for water, you know, turn up the water tank again. And therefore, this I would say it's not at all and it's not a non complicated system which just is controlled by three control circuits water, fuel and information about the environment Okay, that's it very simple system, but it works and you could have such as to be the receiver subsystems and they work okay, but it is too complex, not enough complicated to really plant or an animal or human. And we will now go on with how such Mako complicated systems could be could be realized on the basis on a system of control circuits you must now have first an idea what kind of control circuits you need the first you need some control circuits for basic needs, food and water for instance, that is energy and some stuff which is needed to change food really to energy to sugar in our system. And then you have a processing which is able to regulate itself, well, another control target should control the absence of pain, that is that sort of damages to the to the body, and they should be minimized or they should be repaired. Then next one is sexuality for a living system, it is not necessarily good to have a loose immune system, it's all security, but to have development, you need sexuality, because you need that the system can die and new systems could be which have another room and better environment, security, then comradeship, friendship, love and power

Unknown 53:04
relations between the different animals could be there. And this is even very early in, in the development, comrade ships friendship and so on. But, my Koi fishes in my garden pond has a very complicated system of comradeship and friendships, I will call them friendships, because I have them also normally, this is not this is not well, one of the attributes one would one would find in fishes, but they are there and rather rather complicated form then very important to knowledge about reality very simple living systems could just look at random and environment to find some some sugar and to eat it up and then they would live much better is to have systems which are able to acquire knowledge about the reality and to orient themselves in their reality. And therefore, learning is even very simple organisms. Reality and the system builds up the kind of memory the kind of model of the environment and then is able to act much more success for them without that knowledge of theory. This in my eyes are the most important systems and I think this system is rather complete. And but you couldn't speak much more All about what in what kind of the knowledge of reality is is created and how for instance religions or ideologies that come into the game and form and give you the form the knowledge of renting should take but there's another question or it is not another question it's a very important question, but we could not we could not speak about it here okay. These are the systems and now we have here the image of the organization of the whole system, this is a complete soul which we have here. And it is not uncomplicated first us here the basic control circuits I've left out most of them, but anger pain affiliation, that is relations to the other members of the group, it's essentially the knowledge about reality and then competence an estimation of the ability to cope with the environment to cope with reality this is competence very important okay. How does the system work? Well, quite simple, you have a number of these basic control circuits and they all have a set point deviation that is the deviation between the nominal value and the actual value and this depends on the strength of the motive, well, there might be four different systems there might be weights for the strength of motors, there is for instance pain at a higher weight than hunger this strength of the motors then are activate a system or a neuron s okay an S is a strength of the motivation, but to the strength belongs to the competence that is not only the Saturn setpoint deviation is essential, but the competence to get them something which fills the tank, the hunger tank or the P tank again must be there and this competence is correct with competence tank and therefore, when the competence is low then the motivation strength is no and therefore, you must define the motor strengths as a modification of competence times setpoint deviation this one okay.

Unknown 58:06
What then happens is a selection process this is done by a suppressing mechanism which is more and more activated therefore, exerts a suppression to the MS neurons you here which has the same value until only one m s survives and this one is then transferred to the next layer to this layer here and this sun fires cognitive processes

Unknown 58:37
okay

Unknown 58:39
under Goal here we have a direct connection between Abba tandem and the goal you have for instance cranberry bushes and this could be used to bring to diminish your hunger Well, this is a simplest form of a cognitive activity just remembering something, but I will not know go into detail about the different states forms of cognitive processes, this is just a cognitive system and the cognitive system is under control of the motivational system. This is the system on moderation very easily to understand it. And it is very well it's not very complicated. And it is the same in all animals or plants. And but control circuits are different. The and the kind of learning is different and so on. Okay, that's the core of Plato. Ready and within 10 minutes you can explain it, but you could explain it only in that way because you have you must have the concept of motivation otherwise without motivation, there is no life and now living system because the psyche, the soul is nothing else but the controlling system. And we have seen that controlling system does and it is not at all complicated.

Unknown 1:00:30
Okay, so let me introduce your server. In this case, your Shabak is a cognitive scientist and AI researcher who focuses on computational models of cognition and your symbolic AI. He has, oops, I have screen all the sudden. He's currently a principal AI research researcher with us at Intel Labs in California. Previously, Yoshida has taught and worked at in AI research at Humboldt University of Berlin, the Institute of cognitive science and Osnabruck, at the MIT Media Lab, and the Harvard program for evolutionary dynamics Yoshua, the floor is now yours.

Joscha Bach 1:01:18
Thank you, Tony. When we are looking at motivation, here, it's in the context of the larger perspective on what we call the vectors of intelligence. The idea is that the systems of the future will be not evaluated in a single dimension of performance, but rather, they will have many capabilities across fields, we will no longer build systems that are task specific. And currently building systems that are more broad and flexible. And this is in this area of deep learning. They increasingly have models that are able to perform many, many tasks, some of them, which they are not even built for. But the question is, how can we get to systems that go beyond this flexibility towards generality. And this means that we are optimizing many dimensions, for instance, we need to guide our presentations away from operator specific representations, towards universal representation. So we can represent everything that happens in the world. And the suitable structures for that are we want to go from offline learning as we currently have in neural networks, towards online learning, and eventually to self improving systems that have an idea what they learn and why we want to have systems that go from the open loop control of that you currently have F mostly when you act on the world, two closed loop systems that have feedback to the environment and eventually construct new feedback loops throughout South extending. And with respect to autonomous systems, we want to go from task directed systems towards goal directedness, and then eventually, self directedness. So basically, systems figure out what they want to achieve and why and what the context is. And then you have collaborative systems, you need to go from systems that show what the Interstate is to the user towards explainability. So they can tell the user what their state is, explained it in terms that the user can understand that also means that they have to increasingly track the state that the user is in. So they have to understand what the user can understand in a given context. And eventually, the systems have to be able to understand what the intentions are. So they can truly collaborate and adopt the goals of the ones that are interacting this. So when we are talking about autonomous systems, we go from task directed to goal directed and then to self directed systems for collaborative systems, you go from transparency to explainability, and then goal adoption. And, of course, the question, Does an AI system need motivation at all? It's conceivable that you are only dealing with Oracle's right and Oracle is a system that is responding in a similar way as Google search to a question that you posed to it. And after that particular task is over the system is done with its task and goes back into quiet sense. And it's it's different from a system that is continuously interacting with the environment and serves its own goals and this purpose, but the there is not necessarily a very strict gap between such systems because for instance, neroca gptc, that is a system that is not goal, interested by itself, it has no motivation. It's just a system that is basically to an autocomplete and is creating some story or response based on the auto completion, but GBTC is very well capable of creating a story about a motivated agent. And so if you put an underlying motivation or combine that was the motivational system, then even such a system is able to switch from the Oracle state to something that is more agentic what is an agent think that the notion of agent can be best understood. And you come from cybernetics. So when you take the concept of a feedback loop, but you have a controller and regulated system, the controllers affecting the regulated system is actuators or effectors. And it's gets information about the regulated system via sensors. And the sensors tell the controller in which way the regulated system deviates from the ideal state from the setpoint of that system. And that's an environment which disturbs the regulated system. And the easiest example of such a control system is the thermostat. But the thermostat is not an agent, because the thermostat is only modeling the deviation of the next frame. So it actually doesn't have a model of reality. It's just reacting. And you want to have a system that is able to integrate the expected deviation over a longer timeframe and optimize over a longer timeframe. And as soon as you have a controller that is able to model the future, then you will have a branching expectation horizon, and the control becomes decision making. And this is when you have an agent. So an agent basically is a controller that is able to model the future and is combined with a set point generator.

Joscha Bach 1:06:18
So we basically have stages in which we can understand intelligent agency, the simplest system that is acting on environment is the regulator with a feedback loop. And when you are at a modeling system, you have a predictive controller. And when you combine this with a set point generator, you have an agent. And a sentient system is one that is so general, its modeling capabilities that it can discover itself, and to discover and model the relationship that has to its environment. So it understands what it is doing and why it's doing that. And you can also go a step further and think about agents that are capable of transcending themselves, which means they link up with other agents in combined collaborative agency that is coherent, and is basically instantiating agents on the next level. And humans are of that type. For instance, when we have close relationships with others, we are going to link up into a shared agent. And we also state building that is our civilization itself can become a coherent agent that is able to act on its environment, due to the way in which people are collaborating. And in our own mind, we have a variety of agents that interact. And I think we can see the way in which we deal with reality as a perception agent, that is a system that is trying to track reality as it progresses and generates functions that predict the sensory patterns. And we have an attention system that is interacting with this perception system and is creating actively a coherent model of the world. And you have a motivation agent that is giving a motive force to the perception agent and the attention agent. And what you're looking at today is mostly the actions on the structure of this motivation agent. The core of the motivation is valence. Valence is what determines the value of the operations that we can perform on the environment. It's grounded in the multi dimensional motivation system, and at the root of Valence or needs that we have. And the big question, what is the minimal set of needs that you would need to become an interesting agent, that excellence environment. And the most radical proposal comes from Karl Friston with the free energy principle, and what he says is, basically, you only need to minimize the prediction error. And this will lead you to structure discovery. And if you are trying to minimize the prediction error in a system that has set point deviations, you get all the other behaviors. And I would say that this prediction error is equivalent to an aesthetic need. It's a need to discover structure in the world at large. But I don't think that it's sufficient to understand human behavior. The

Joscha Bach 1:09:14
GBD three is an example of a system that is minimizing prediction error by nothing else. And it seems that the order in which GBTC learns things is very different from the order and people learn things, basically, meaning is if GBTC is able to calculate at all, the long tail off style. And so GPT three is only looking for an aesthetic structure in a way, and not even for symmetries in it. It's also in this regard incomplete, because it's not necessarily coherent. It's not directly optimizing for coherence. So I suspect that why, if you zoom out long enough, there might be a convergence between systems that only minimize prediction error, and properly motivated systems in order to get a system that does in its lifetime, in the finite lifetime is going to get to Do an interesting behavior on the boat you need to put in additional needs. And also for the purpose of learnability, you might even have to have something that transitional needs, motivation reflexes. For instance, when a baby is born, the baby does not have an understanding of hunger and thirst, these are too complicated desires, and they cannot get met be matched on actions that are available to affordances that are available to the infant. So instead, the infant has hardwired reflexes. When the baby is born, and gets hungry, it has a sticking reflex. So it will turn its head around and open and close its mouth. And it has a second reflex, if you put something into its mouth will reflexively sack. And it is a swallowing reflex, if you put liquid into its mouth, it will swallow that liquid. So if you have a newborn infant, you want to give it medicine, you just squirt it into its mouth and swallow it regardless of whether it's tasty or not. But as a result of these three reflexes, the baby gets into a situation where it learns to connect hunger, the swallowing, and the satisfaction that comes afterwards. And once that learning is online, the reflexes are turned off, because they would interfere with the properly learned behavior. And at this point, you can no longer give the medicine in this convenient way. And it's interesting to think about how many of these transitional reflexes we have, I suspect that many of our moral emotions are actually reflexes that are built into us before we understand the true symmetries, the economy and the game theory of the social reality. And the older we get, the more these moral reflexes are replaced by true understanding. And the more reflexes are just no longer active because they're replaced by an understanding of what state is of the world that we want to achieve, what are the behaviors that get us there, the individual needs can be understood as because the logical needs social needs, and cognitive needs and physiological needs of hundreds of them, but their sustenance, pain avoidance risks, libido and so on. And they have a bunch of social needs. For instance, they need to affiliate with others in need to care, need for affection and need for legitimacy, which means that we try to serve internalized normative goals and need for dominance to rise up and hierarchies and so on. And obviously, not everybody has the same needs and the same strengths. There are individual differences between those needs. And they have cognitive needs and need for competence of being basically more skillful, and need for reducing uncertainty environment, which lets us explore and our need for aesthetics, which lets us discover structure. And each need can be understood as the target value and the current value and the deviation.

Unknown 1:12:52
To show your video from

Joscha Bach 1:12:54
the left, so our need to tank rent straight is basically defines the urgency. Can

Unknown 1:13:00
you froze for a second? Okay.

Joscha Bach 1:13:04
Thanks for letting me know. So we can the urgent indicator changes over time. And the stronger it changes over a unit of time, the stronger of a pleasure signal you get it goes in the wrong direction you get this pleasure signal and both pleasure I think you're

Unknown 1:13:29
having connection issues again, your shot agnostic

Joscha Bach 1:13:31
is diverse, and in this way structure the world based on hope. Yes, I am. So as far as I can see, everything is good here. Can you see? Understand you still? i Yes.

Unknown 1:13:43
But I broke off for about five seconds.

Joscha Bach 1:13:49
Oh, okay. So both pleasure signals and displeasure and signals, reinforcement signals. And when we act do not act directly on these reinforcements when pleasure and displeasure happened, it's too late. They need to act on models of them. So basically on the on future pleasure and displeasure on anticipated pleasure and displeasure. And the urge signals are part of a larger architecture, they give rise to priming and modulation and to learning and decision making in our cognitive systems. And the modulation gives rise to emotional states. So we get modulated based on our changes in our urges and the expected changes into different configurations that adapt us to the environment and that when we perceive them ourselves, you perceive them as emotional states that we are in. And the same is true when we perceive them and others. And this gives rise to what I call the engine of motivation. So you see a bunch of tanks. To the upper left you see the physiological needs. At the bottom you see the On social needs, and to the top right, you'll see the cognitive needs, each of these times can run dry. And when they do this, they create pain signals. And when they the level increases, they create pleasure signals. And they interact to produce an integrated sense of valence and urgency and urge strings, and they affect the number of modulators that configure the cognitive system at any given moment. And we can use our models of needs to map them on the Big Five and this way, explain personality. Basically, the hypothesis here is that personality can be understood as parameterization of the motivational system.

Joscha Bach 1:15:44
But our needs not a hierarchy. Don't we all know Maslow's hierarchy of needs. I am not a big believer in this hierarchy of needs. Because, first of all, I don't think that these are needs, these are purposes, right? The needs are more basic things than self actualization and self esteem, which are conceptually quite difficult to understand. Or safety. This is something that you cannot measure directly, you have to construct these structures that you eat see here. So this is actually a hierarchy of a lot of needs, but of purposes of models of needs. And what we see is that, basically, the lower level, you have organismic purposes, and the next level, you have relationship purposes. And at the top of Maslow's hierarchy, you have purposes that can be integrated with the lifespan of the organism, which is basically the ego. And in our own organism or our own own mind, they are not organized into a strict hierarchy. Our needs are in direct competition, they're next to each other, they might have different strengths and might be replenished in different frequency, but they are directly competing, you're not just waiting, when you want to have stage of evolution until you have eaten, sometimes you will not eat for a couple of days because you have more important social or high level goals. And of course, we have goals that are above the EBO that basically what we put called sacredness purposes that are so important that you're willing to sacrifice the goals of the organism or if the ego for it. And once we have sacredness, all our other purposes become instrumental to satisfying our sacredness our higher meaning. There's nothing esoteric about this. It's just the way in which most human beings are implemented. And this sense of having purposes above the level of the organism is put allows us to link up into higher level agents. And I think that our ethics depends on having this notion of sacredness, it depends on every shared purposes above the eagle. Basically, ethics is best understood as think as the negotiation of conflicts of interest, under conditions of shared purpose. If you do not share purposes with anybody else, there's no reason why you should have ethics, right? You are a singleton that is interacting with other Singleton's you're maximizing your own goals and there is only game theory that you have to obey, all your interactions are going to be transactional. And if you want to have non transactional interaction, it means that you have to act on a shared purpose, it means that you are giving something to somebody else not because you expect it to get something in return from this particular agent. But because you and this other agent are part of something larger, and you want that other agent to achieve their goals because they're ultimately also your goals as part of that something larger. So you don't need this direct return from this other agent in this context. And the trade purposes need to be given by innate needs, you cannot infer them something they're sociopaths which are completely functional as cognitive agents, they don't have any cognitive deficit, but all their interactions are going to be transactional. And the interesting thing about our own species is that we can link up and the shared purpose that we are able to build societies at scale that you can build coherent agents at scale. And when we think about AGI or artificial intelligence systems, that are ethically interacting with people it means that you have to build them in such a way that they are going to share purposes with us. And these shared purposes need to play out in a coherent world which means before we can have ethics we need to develop a shared aesthetics of what the world should be looked.

Unknown 1:19:34
We all share frozen again.

Joscha Bach 1:19:37
So do not converge to a coherent reality that is sustainable. Okay, so it makes no sense if we have shared impulses that will not converge to a coherent reality that is workable and sustainable. This is this for me.

Unknown 1:19:54
Thank you, Yoshua Dietrich. Are you able to Hear us. Oh, Dietrich is a guest again, that didn't make him a co host. Dietrich Alright, can you hear us? You should be the co host Matt again now

Joscha Bach 1:20:19
hello I cannot see him. I see him.

Unknown 1:20:30
Yes, I can see him he's unmuted. You Dietrich, can you hear us? I think we have a connection issue.

Joscha Bach 1:21:28
Yes, that's very unfortunate. Okay, let's hope that he is going to be able to join us in the moment it did work in the preparation. Yes. And maybe what we should be doing is for our to jump into the discussion. And hope that we can draw him into the discussion later. Okay,

Unknown 1:22:04
so I'm at this point. In this case, I would like to invite questions from the audience, you can post them in the chat. To begin with, we have several questions to the speaker to jumpstart the discussion. So what kind of algorithms? Do we need to compute and distribute rewards in artificial AI systems? So that their goal goal aligned with the humans?

Joscha Bach 1:22:55
Yes, I think you did that directly give this question to a speaker. And firstly

Joscha Bach 1:23:07
I think that there can be various algorithms that do it. And eventually what is important is the functional result of the application of these algorithms. And the easiest perspective that we can have on that is probably a cybernetic one where you have a regulation system with feedback loops, but it doesn't matter how you actually implement it, what matters is that the system is acting on something that amounts to a shared purpose. And I am not aware of a very good model for this in the context of AI that does this in a general form. But I think that Christian and because the other might have thought about this as well.

Unknown 1:23:53
Christian, do you have any thoughts

Unknown 1:23:57
to some extent, some form of reinforcement learning system but not like the ones we're using today? Because they usually only have one general value function for the whole system. They shouldn't be something something more complex with multi dimensionally to start with, but also not necessarily reinforcing very simple associations and something like that, but probably influencing a lot of different processes of perhaps if we develop something like that, this will be more more like a control system. Suggesting

Unknown 1:24:43
I agreed not to my domain, but let's say this point, which we need the model at different layers emerging one on the other by self organization. So when In the modeling theory, many layers and then we need our theories for each layer one is the algorithm our brain implemented new neural activity inhibition activation Association, but on this basis low level then we have the I really don't mind the program, mind the cognitive programming, which is implemented at the neural layer is not reducible, we do not really understand that we were just with the basic description or neural activation, now, we need the functional explanation are low level and then higher level of functional explanation at the company level and so on. So, in a sense, we have to model a different layer of logarithmic machines cerebral

Unknown 1:25:50
Christianna actually a question for you, in your presentation, you spoke about intention as consisting of the of what to do and choosing like of these two aspects of the goal and the actual choice of executing how does this How does this architecture lie within the system? One system two distinction

Unknown 1:26:24
Okay, thank you. For me, it's basically is part of system two, because it's based on believing the signing ongoing reason and then doing but but as also back Yorkshire say and also reason we have to integrate the system one mechanism devices as the basic structure implement the even intentional action. So at the end, it is a performance by automatic mechanism. And vice versa. It's possible to throw throw transform an intention device and read a decision in automatic habits by use for example, when I start to drive to learn the driving car, I listen about why I have to stop or have to then become an Adobe designer. And by the way, it is possible that the adult automatism is make explicit then become a decision making process. So it's got to be automatic and decide to control and decide. So it's possible to the implementation the two systems and they implement the conversion to system because competition and collaboration interference. Are those two possible they one is just in the other. Currently, people deal with the deal your sister one sisters two are not so clear on that. They usually present them as two competing mechanism is typically one or the other. This is too simplistic. They interact interfere. And there's little one in there. So it's a bit more complex in the thank you for your question.

Unknown 1:28:15
We have a question for your Shan perhaps also for the others. Do we have a theory for high level agency? It seems from what you said you're sure that game theory isn't enough at least namely, this is this is from Mohammed Baraka.

Joscha Bach 1:28:34
Yeah. So first of all, to the system, one system two distinction. I think that cristiana was exactly right, that they're not separate, but they have to be understood as something that is working together. But there are different operations taking place in system one and system two, that seems to me that system. One is primarily perceptual, and it's getting to its understanding of reality, this some gradient descent, which means it is following a deviation to a local optimum and perception. And if you are following a gradient, you don't need to have a memory of where you came from, you just need to follow that gradient until you end in the local optimum. But if the search space is very large, for instance, the space of things that we could be looking at at any given moment, based on the models of reality that we know we could have often don't converge. And so we need to construct a solution and this construction of a solution when you cannot follow a gradient does require memory because you need to remember what what you tried and why and which which things you try it worked in which one didn't and

Joscha Bach 1:29:53
this discover its own index memory, its own stream of operations that it did. And so we have a reflect A system that has memory and is able to perform analytic operations, grammatical operations and get to a low dimensional representation of reality that is very stable. And you have the perceptual system that is much more dynamic and more continuous. And that interacts with this reflective system. I suspect that the reflective system is primarily a top down process, whereas the perceptual system is primarily working bottom up, but they have to work together to work at all. Right, I don't think that one can work without the other. Now to the question, if we have models for emergent higher level agency, I think that it's interesting to look at the work of Thomas Aquinas in this regard, Thomas Aquinas has discovered, concepts are policies for rational agents. And he says that basically, every rational agent can infer that when it's in some kind of social context, that it needs to optimize for four different things. One is, the first principle of rational agency is goal rationality, you need to pick the right goals and actions that you can hope to achieve that these goals. In his old language, he calls this prudence, right wisdom, and then you have to have optimization of your internal regulation, you need to regulate your own needs in such a way that you don't fall apart, that you don't overeat that you don't under indeed eat that you maintain your homeostasis of your organism. And this is what he calls temperance, you also need to optimize the interaction between agents and keep them in balance and homeostasis. And this is what he calls justice. And then you need to maintain the correct balance between exploration and exploitation, which means you need to both learn. And then you need to be able to act on your models. And you need to understand when you need to act on your models, and this is what he calls COVID. And these are the principles of rational agency, they are not giving rise to higher level agency, if you want to go to higher level agency agent, then you will first of all need to submit and serve a higher level agent, right, you need to recognize that there is a higher level agent that you want to be part of, and you need to be willing to serve it. And this is what he calls face. And you need to be willing to link up as the agents around you to do this, right, not just an abstract sense that you abstractly serve some higher level agent, but you need to do it with the people around you that also shared this need to serve the higher level agent. And this discovery of shared purpose above the individual, this is what he calls the mouth. And then you need to be willing to do this in the absence of expected word. Because before you act on this, this higher level agent doesn't exist, right before the higher level agent exists, it cannot give you the better environment that the higher level agent can construct for you. So you need to be willing to invest into this thing before it exists. And this is what he calls hope. Right? And in the language of Thomas Aquinas, these rational policies are called the practical virtues. And the next level agent forming policies are what he calls divine virtues. And we are used to take these things as superstitious or religious, but the concepts that are connected to them as such, but they're not I think that they're entirely rational, and the way they were conceived, and it's just that there are our world is so steeped in them because we used to be a religious world and 150 years ago, that they seem to be ubiquitous and part of the previous civilization. But I think that he made a genuine effort to discover principles for emergent next level

Unknown 1:33:59
on there was an interesting remark in the chat. During Dietrich present Dietrich's presentation dreams no longer a component. So it brings it inspires me to a question of, well, what role does dreaming and imagination play in autonomy and golf formation? And I would like to ask Christian volcania is for his thoughts. Okay, so

Unknown 1:34:31
the way I view it, the sort of basic mode of the brain is just imagining things, more or less a complete mess that tries to follow what's happening outside the head to wet when possible. So it's like we have the world outside including the body condition continuously evolving over time and what the brain is trying to do is to follow that we with sort of an eternal process that follows along as well as it can. But that's during learning, but it can also be decoupled from the environment. That means let go state transitions that it has long goes on, on its own inside of the hat or less. When we're not controlled, it will be a daydreaming or imagination. And depending on this factor, I talked about gaining control, it can be completely random. Or it can be focused on a particular problem, or aimed at solving a problem, for instance. So in my view, the system used for daydreaming of imagination and for for planning and problem solving, it's the same, the difference is to what extent it is controlled or limited to, to just states that are relevant for whatever we're trying to think about. But that's the short, short version, the long version is very long so far.

Joscha Bach 1:36:03
Thank you, Christian. Yoshida remarked to this and the audience is dreaming the default state for which you can create the awake. And I think that's basically correct, in the sense that everything that we perceive is by its nature a dream, right, it's, if we can perceive it, the must dream it physical system doesn't have any events, it can only be conscious in a simulation. And what we perceive is the game engine that is generated in our brain that is tracking sensory data. And at night, this game engine is not tracking sensory data, because it doesn't receive any and it's dissociated from the sensory processing. And instead, it augments data by varying the states of the rule ever systems so you can can experience things that are not grounded in direct sensory experience. But this system is initially trained by having access to this outside world. And by tracking these patterns. So our dreams tend to be heavily inspired by the patterns that be perceived in the real world interaction. But I think it would be very helpful if what people in general for epistemological reasons are aware of the fact that everything that they can possibly experience, think every revelation that they have, and so on, ultimately, is a brain state. Everything that we perceive is a configuration of our own brain, some kind of dynamic pattern that unfolds. And that is attended to by a reflective system that has memories of what it attended to. So we can only have access to the things that we have, that we remember having attended to. And what we attend to, are always states that take place in our own brain. At least that's the most sound theory that we can observe. That makes sense of what of what you can observe. And because this system is during wakefulness, coupled to the external world, we can make a model of what representations are the result of that coupling. And the functions that best predict the next sensory state are what we perceive as perceptual reality? I think this is how it works. But to this original question, it I think that also implies that without the ability to dream, the system is not able to have the ability to make sense of the world, because it's difficult to build a cohesive function of the entire universe that you can relate things to. And when we talk about understanding something, when we understand the meaning of something, it means we establish relationships into a universal, connected, coherent model of everything. And this universal connect connected coherent model is what we individually call The Universe. Right? Everybody creates a universe in their mind as a dream. It relates everything to it. Thank you, you're

Unknown 1:39:03
sure. We have another question for you, which I think would also be great if we could get Christianity's input on that. And the question is, um, so I'll just read it. This is more of a non technical question. But are there any immediate strategies you're sympathetic to, of applying these notions of coordination and higher agency agencies to improve or increase the agency of the social structures we currently live within? What do you think? Are the factors driving the seeming lack of agency within our current civilization model? And do you see a steady state dynamic diverging from the current situation? Your show I'm not sure if you would like to go first or whether you would like Cristiano to go first on this question. Christianna with I'd like to go

Unknown 1:40:01
I'm not sure if that is fully understood the question sorry because I wasn't able to read the the interesting was about the coordination agents coordination that the emergent collective result in sociality and so on, if I understood

Unknown 1:40:29
there are two phases one is we need grounding found in sociality, social relations, which are not just COVID that are also conflict on uncompetitive conflict is part of society is fundamental but no conflict no democracy to give you an example. So, it's cooperation and conflict is and is not only adjusting, adjusting and adjusting might be able to the others a total change in the view or the other influencing them and changing the mind. Usually, there is a mystery misinterpretation our Tom or Tyrion mind is for predicting understanding adjusting now, Tom is basically for changing the behavior, the other influencing them to change their mind by changing their mind. So, we have to one other important theories to grounded sociality in different aspects in the mind of the agent and not totally exchange a game interaction, a dose of collective something, the emergence of a distributed collective will have this thing with the collective knowledge something which is very, very, an end even more difficult institutionally, the counters effect of certain actions and objects are socially established. So, these different levels can be implemented in the mind and they work to our mind for example, not norms and so on. This is one aspect the other aspect is we need the to be your best theory. And those revolution is theory of sociality, the kind of interaction and sociality and on that point of view, let me say that I have enjoyed very much dermal clinics due to the simplicity statistical use whole correlation and so on. However, another criticism, which also referred referred to some question about the game theory and so on, careful, many theories are in fact, the idea ideal typical, are in fact, normative theories very strange one in economics, that's that way, game theory states way there the ideal way the rational theory, but no, it's not the idea completely scientific theories activity should be should be explainable the fact in video, not just the normative idea. So, we need a compass also originally based on different kinds of very short for me the basic of sociality of all social phenomena external is dependence, the fact that the agent are not able to achieve all their needs, we have a limited ability, limited competence, limited knowledge, limited resources. So we are dependent on the other we need the other and this advantage will be different, different different complimentary. So service based on dependency networks, relationship, who needs what not only exchange value different kinds exchange, I need this, you need that. But insufficient interaction, like as the theory or also for true collaboration, we share a goal, we have a common goal, we have to associate our power in order to achieve the common goal, the different, but even more even altruism, I don't expect anything at all. I just do that for you, and so on and so on and so on. So we need a more systematic theory of different forms of social relations interaction that for me are based on different kinds of dependency. And we need to ground that to found that in the mind of the user, because he is the man that control the social media. Sorry if I'm wrong.

Unknown 1:44:39
Thank you so much, Christiana. We Dietrich is back. Dietrich. Can you hear us?

Unknown 1:44:46
Yes, I can hear you. But I can only see you in a very blurred image. I'm so sorry. But that will take a Any quotes don't look at my problems just continue the reading and I try as good as possible to follow

Joscha Bach 1:45:19
Dietrich, what do you think are the minimal needs that you built into a system to give it sociality and social agency?

Unknown 1:45:31
Basically, I got only the half of your

Joscha Bach 1:45:33
okay what do you think are the minimal thinks that needs to be built into a system to give it something like social agency so that you have collaboration and competition and the emergence of normative structure? Well, the first thoughts about this

Unknown 1:45:52
on my social competition, and they have social they behave socially for instance, our mother mice, they look for the children and feed them and teach them things, how to come to a certain place or not, and this is very necessary to do that. And this partially developed by genetical genetics are great in simulation, because if you have a gene pool, a lot of things, you have not to program, but the development for themselves. And this is quite interesting that I think this is a good medium to start with, to simulate just human development. And quite interesting, and when you get a different form of behavior, for instance, they will, they there must be a basic need for contact. This is basically where you have this contact, and my fishes in my pond, they have that they look at me, and then OB and they like to have contact with me. They don't ask me but wait, for instance, that I give them food, and they don't not right then from from other people, but for me, that is their connection with me fishes, and they have contacts, even they have social structures, such as koi cups, and everything in a rather primitive form. And to have is the norm and sexuality heads not in development, I would like to, to go on with our main problem the moment to develop language, not just the artificial language, easy to make. But to develop that kind of that image i i talked about, about cordovan. Oh, and

Joscha Bach 1:48:32
okay, we just did, again, the gods of the internet, not visit us with respect to our connections to Bavaria. I'd like to briefly also emphasize this point, that Christiana just made that collaboration and competition have a relationship between them, I suspect that our need to collaborate and our ability to collaborate is the result of an evolutionary pressure. That was the competitive pressure. Basically, the agents bind together and form coherent behavior, because they are forced to, because they are competing with systems that have discovered the power of cooperation. And once you have that power of cooperation, you are able to organize yourself in governance structures. And these governance structures mean that you are doing things that are not optimal for yourself, but that are optimal for the group, basically, for the next level agent for the next unit of organization. And as a result, you're able to collectively out compete individuals that are only acting on their local own self interest. And if you take away this pressure, if a system becomes too big to fail and the competitive pressure falls away, then the system loses the need. The evolutionary pressure towards collaboration and cooperation, which I think is what we might be observing in our own societies. And as a result, what we see are increasing failures of coordination. Right. So our inability to coordinate, for instance, in the face of a pandemic is probably the result of a change of government structures in the last 50 years as a result of our society becoming very safe and too big to fail, in a way is absence of competitive pressure from the outside the fact that the had have happened systems that protect us effectively from any outside attack, and so on. And the fact that we have built an agriculture that is feeding us very well and a political system that is relatively stable, have removed the need for the cooperation. And so in some sense, that's the way in which agents are organized to next level agency is the result of pressures that exist in the environment of the agents. And this is related to what Dietrich was describing in his simulation of simulated mice, he builds environments where there is a benefit from competition at first, because there are limited food resources. And the environment is also set up in such a way that there is a benefit from cooperation. So they can share information, they can define the food sources against other groups of mice. And as a result, there is an emergent social structure. I think we are on the top of the hour. And I would like to take this opportunity to thank all speakers for joining us today. And I'm very grateful for getting us together despite the connection issues. I'm very happy about the possibility of having such a conversation over the internet, and via zoom. I'm also very, very grateful of having met Christian in person and the Christianna. Again after many years and also Dietrich. We will cut this video into a form that we can release if everybody's okay with this. And I would also like to thank the audience for following along asking questions and paying attention and see you next time. Thank you very much.

Unknown 1:52:26
Thank you for your beautiful initiative. Thank you for watching.

This transcript was generated by https://otter.ai