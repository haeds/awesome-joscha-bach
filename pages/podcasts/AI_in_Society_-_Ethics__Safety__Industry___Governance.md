Unknown 0:00
is probably more of a short term versus long term one. It was mentioned of different companies developing AI first, they'll be a possibility or any suggestion to buy shares in specific companies. Companies that are moving towards that faster than others?

Unknown 0:35
I think we did. Well.

Unknown 0:39
I mean, the the biggest group of people working toward AGI are in our is Google DeepMind. But of course, buying shares in alphabet, you're buying shares and a lot of different things, right that the singularity NET project that I just described, which is a sort of decentralized economy of of AIS, we're going to do an Ico and initial coin offering for that later this fall, probably in November. So if you, if you're into crypto projects, you can you can invest in that our our opencog AI project is an open source project. So you can donate to it if you want, but you can't you can't invest in it. And I mean, that's intentional. I really think it's better for the first AGI is not in itself a commercial enterprise. I like the Open, open source mode that modality.

Unknown 1:32
Anybody wants to question one other here?

Unknown 1:37
You all seem to feel a very strong sense of guardianship or responsibility for what happens? How much do you think that programmers or developers working within corporations, like Google have an ability to act on what they think is responsible programming?

Unknown 1:55
So one quick answer is, in the Manhattan Project, the scientists strongly advocated to the president at that time not to use the bomb, but they were completely ignored. So basically, who owns it makes the decisions and what the scientists think really doesn't matter?

Unknown 2:15
Okay, to give some positive answer, at least at least the Deep Mind branch of Google, they have an ethics team, really, highly qualified scientists who research about, you know, AGI safety, so at least they're concerned with this and dealing with it, probably there will be ignored. And in the end, also,

Unknown 2:39
the CEO of the company has a fiduciary responsibility to maximize shareholder value, right. So I mean, if, if, of course, if they have a robot that will annihilate everyone that they will, they won't, they probably won't release it, even if that would maximize their bank account, but where, whether it's more of a judgment call, of course, if you're the CEO of a public company, there is some push to do what will maximize your share value, right. So, I mean, in reality, the decisions are usually kind of borderline and complicated. But

Unknown 3:15
I mean, you know, largely, this is true, but you know, even company bosses, you know, have some sometimes moral values. And I mean, see Bill Gates, you know, he donates you know, billions of dollars,

Unknown 3:26
he's retired, he's no longer okay.

Unknown 3:36
I cannot do this. What would give you power?

Unknown 3:42
Being the See, I mean, that's hard to say I mean, honestly, AI projects, to build to, to do research and discover new algorithm you can do on your own. And a researcher could say, well, I discovered this amazing learning algorithm, but I'm not gonna release it. But practical engineering projects and building API's, it takes a lot of people to build a distributed system that can run on lots of machines to deal with sensors and actuators and all different types of data so that this is always going to be I think it's going to be a big team effort. So I mean, any one person could quit the project. But I mean, even if the project leader got cold feet and didn't like the project, I mean, if it's being successful, there's a whole team and the company around it and these things get get a certain momentum. So I don't think an individual programmer working on the project has too much power to stop a successful project if they're they're worried about it. There's something but I mean, I do think most science and tech geeks do not want to destroy the world, right? I mean, if you think about it, like the the amount of passion, energy and intelligence that goes into a startup company, if that went into doing destructive things, would see a lot more scary and powerful, destructive technologies around. So I mean, overall, most people who are leading AI projects do actually want them to do good for the world and help people, the difficulties are more and more southern ones, because everybody wants to do good and help people while also making money for their company. And if you get a situation where there's some weird conflict, that's hard to sort out, then it's hard to say what would happen

Unknown 5:37
as a counterpoint. In the Manhattan Project. Initially, the scientists in the Manhattan Project was so worried that Germany would get the bomb first, that they were working furiously to make it happen. But towards the end, it was pretty clear Germany was not pursuing the weapon. But by that point, there was so technically involved, that lost sight of the of the dangers and the risks that are associated with it. That wasn't true of all of them. But it was true of the majority.

Joscha Bach 6:07
There's also a difference. In fact, in the time of the Manhattan Project, most of the involved physicists were also public intellectuals that intensely reflected on the political situation, and participate in the political discussions and wrote letters to their respective presidents who were consulting with them and so on. And this is a situation that seems to be very unlike the current situation, can you imagine the current set of presidents conversing with AI nerds about how to make proper laws, and these being enacted, this is just not the process that we have anymore. And

Unknown 6:38
Obama did some of the Yeah. But,

Joscha Bach 6:42
you know, if even if a reasonable country would set reasonable incentives, and reasonable laws and so on, we would still have unreasonable countries like say, Theresa May, or North Korea would come up with their own AI project.

Unknown 6:57
We're building a robot kingdom.

Unknown 7:01
So another data point, there's an interesting video on the on the internet, with Elon Musk. In the in the interview, he says, he tried to persuade Sergey Brin and Mark Zuckerberg that this was a serious problem. And they just didn't think it was. And these are the people who will actually make the decisions.

Joscha Bach 7:24
Another issue is that if you are new have nuclear bombs, if you want to build them, you need a giant infrastructure to make that happen. It doesn't only costs billions of dollars, but you also need to have all these centrifuges and access to certain raw materials and so on. This is not the case for AI. For AI, you just need reasonably large computers that are not that hard to come by. And you need to have the right ideas how to do it. And once some team has figured out how to do it, it's very, very hard to keep it under wraps, it doesn't necessarily mean that somebody has to get in there and steal the secret sauce, because most of the ideas are probably already out there. And once you have an idea how they got it to work, a number of people will follow shortly afterwards, with similar solutions.

Unknown 8:09
That will be in the best interests of an AGI for the populace to know about it, and also be in the best interests of the corporation that owns it. So how long do you think after we have IDI? Do you think people will actually know about it?

Unknown 8:24
It depends on who actually is control at that time. If they really want to keep it secret, and they use the power of the AGI to ensure that happens, then I can't see why it wouldn't be remained secret indefinitely.

Unknown 8:37
I think that's extremely unlikely. Personally, I mean, I it's not impossible, but it's seems to me that, you know, when someone gets to say, a toddler level AI that has common sense with three or four year old child, the odds are very high, they're going to show that off to the world. And everyone will know about it the same way we learned about AlphaGo and Watson and so forth. And then once you're at that level, then everyone in the scientific community is gonna realize like we're in a different phase of the game, and they're moving from toddler level AI on toward like adult human level AI, there's gonna be a lot of teams working on that in parallel. And as Josh has just said, I mean, I think once you're at that level, many teams are going to be racing, someone will get there first, but then someone else will get there soon after noon. It's not very feasible that someone's going to have a team of top AGI researchers working sealed off in the basement somewhere and not telling anyone what they're doing. It's not impossible, but the way things seem to be working is this work is being done in companies or open source projects with a lot of turnover of staff and leakage of ideas, which just increases the odds that, you know, similar breakthroughs are occurring in a lot of different places. And that's certainly What we're seeing in the AI community right now anyway.

Joscha Bach 10:04
But if Google will build AI, then you will not only know what the day they build it records to tell us about it 30 years before that

Unknown 10:15
point out that all of these corporations keep their major projects very much under wraps. So I don't even though I live in Silicon Valley, I don't really know what Google is doing in the AI area.

Unknown 10:27
Well, I know what I was doing.

Unknown 10:31
They say they often

Unknown 10:32
do you visit their office. Never.

Unknown 10:43
companies and corporations are supported by the people who consume their products. If we consume Facebook products, or Google products, and we're supporting Google's and Facebook's vision of what AGI is, and what is the mission commands, moral framework will become? What can I do as a consumer to make ethical choices? And are there any companies, corporations, individuals or otherwise that you would recommend for the pursuit of ethical NGO development?

Unknown 11:12
That's a hard one.

Joscha Bach 11:15
Well, Tesla, probably, I think that Elon Musk is the one who's most concerned with humanity's fate and most driven by this control, I think that Google was originally built as a transhumanist vision to of course, these are machines, Google and all the other corporations are, in some sense, autonomous agents that are built to generate money. And people and shareholders have limited influence on this, because they are a part of an evolutionary environment in which the fitness function is how much shareholder value that can can generate. And if you're a large corporation, this limits your scope. And it was the activity if you want to stay in business.

Unknown 11:57
I mean, you could read sort of, you know, the small print when you you know, obviously, you know, have 10 pages, and you keep those your accept, accept, you know, some companies have better privacy policies for Manchester does example examples, right, you know, if you use chat systems, there's so many out there, and, you know, some just grab your own, you know, phone book and distributed to everyone, and some, you know, give you a choice, right, as an example. And, but, I mean, it doesn't really help you write if you want to communicate with your Facebook, friends, you need Facebook, you cannot go to a different platform. So

Joscha Bach 12:30
your choice as a consumer is very limited or your power as a consumer, what can you do as a consumer to make Apple have decent ports? And not something from a parallel universe? Nasty?

Unknown 12:41
No, don't buy Apple. I mean, there's a real alternative. Right? Yeah.

Joscha Bach 12:44
But I think that Apple will not change it's, it's you.

Unknown 12:46
Yeah, but well, then they will go down in the long run, maybe not because

Joscha Bach 12:52
we can you convince Microsoft and to you have MacOS or something, basically, as a consumer, you will only have a very, very small marginal.

Unknown 12:59
Yeah, but in this case, I mean, sort of, we have Android we have sort of windows and we have iOS, right. In this case, at least the consumer has a choice. And more or less you can choose, right, if I want to communicate with my Facebook, friends, I need Facebook, I cannot use something else. Yeah, there is no choice. But if you want to change

Joscha Bach 13:13
something, I think you need to go to the top. It's like in, in society. As a voter, you cannot change society, to a large degree as somebody will take to the streets, you cannot change society, voting is a way to give legitimacy to something, it's not a way to change it or to find the best policy. If you want to change something, you need to take a talk to the people in charge or become one of them.

Unknown 13:36
We don't really have any power about what's going

Joscha Bach 13:38
to happen. No, you have a lot of power, you can get to power, you are very powerful. You are a smart person, you can start a company, you can work your way to Davos, you can talk to people that take you seriously if you have something serious to say.

Unknown 13:51
Yeah, but not everyone can sort of become politically active. You know, for instance, primary job, right. So the question is, what can the majority of people do who have sort of, you know, maybe 1% of the time to spare, right? And very limited? Yeah, there's some choice you can make with products, right, and just, you know, stay away from them, which have bad policies, but it is very hard in some cases, you know, like messenger platforms. But this ties into

Unknown 14:17
my motivation to create AGI open source more like Linux than like OSX or Windows, I mean, that, that doesn't guarantee that it's made ethically and for the common good, either, but it does mean that it's not locked in to some corporate organism whose explicit goal is to maximize shareholder value at the expense of everything else. So I mean, at least at least avoids having the AGI locked into an essentially malevolent goal structure, but it doesn't it just leave things more or open at the doesn't guarantee a good a good outcome either. So yeah, I would say, in my view, the best hope is to develop AGI open source and then have a wide variety of different parties leverage that to do good in a variety of different ways. So I mean as an example, now we're pretty far from human level AGI, but our opencog Open Source AI system. After our Ethiopian software developers started working on an outsourcing basis, some undergrad students there, took the opencog system and put it on some raspberry pi, which was put in a toy robot, which is now being sold to universities in Ethiopia, for education purposes, right. So you, you put some AI software, open source, and it lets students take it and then use it to do things in their own country by hacking and messing with it. So if you, if you multiply that by a million, right, then then maybe you have something that can can really disseminate AI technology for good as as, as it develops.

Unknown 16:13
Bring up something that's more of an immediate issue. And as you probably know that the open AI letter on autonomous weapons came out about a year ago, and one of the things they talked about was how autonomous weapons are going to be the Kalashnikov. So the future that as you mentioned, it's that you don't need a cyclotron or anything, they haven't an AI that could find a new face recognition, fly a drone, and, you know, do targeted assassination. And when terrorists get a hold of these, you know that that's something that I think we really do have to worry about. Now,

Unknown 17:07
I worry more about the US government having a hold of them, they seem to have a lot and they've been blowing up a lot of people.

Unknown 17:16
Well, I'm sure the prominence that they're much more portable than nuclear weapons, right. So and they're cheaper. And so you know, just about anybody could get one. I mean, right now that our essence is using autonomous, hurt, not autonomous, sorry, they're using drones to fly bombs and people that you know, they're not AI are loose, but it's not that far stretch to think about, you know, they're, they're able to see better than us, they're probably then going to be able to aim better than us. And we've already weaponized them when they're driving our cars.

Unknown 18:11
Well, I agree with your overall point. If AGI gets out in the form of many separate API's that anyone can replicate, then, even though the vast majority of people would only use them for useful, benign purposes, you don't need many radicals, with crazy ideas, to use it in a malignant way. And because it's so much more powerful for damages do be so much greater. And this has been true throughout history. As military technology gets better at killing people. The ability for a small number of people using it to inflict enormous damage increases.

Unknown 18:50
But the main risk is the near term question. I mean, the main risk is not much to do with AGI. I mean, I lived in DC for nine years and did a bunch of AI work for various government agencies, some for INSCOM army intelligence. And I mean, what you see there is the military wants as well will act according to doctrine and obey orders and be quite precise in what they do. And my my own gut feeling is that's not especially compatible with being at the vanguard of AI AGI r&d. I mean, early stage AGI is they're going to make mistakes, and they have to experiment and learn and be a bit unpredictable. And what I mean what the military wants is something that will always act within certain constraints or they're not going to be in the vanguard of AGI and they're not now it's other commercial companies that they're going to create narrow AI killing machines that carry out specific recognition and movement and planning tasks oriented toward spying on who they want to spy on and killing who they want. Kill hunt. It's it's a narrow AI mixed with other technologies problem. And by the time you get what I think will happen is some other sector of the economy will get to AGI while the military is making powerful narrow AI killing machines. And then I'm not talking about the AGI. Yeah, so the hope, the hope, in my mind is that the AGI is to go outside the military will be benevolent, and will then become more of a force in society. Then the human controlled militaries killer, narrow a eyes, but I'm also regarding ISIS. So it's true. I said some drones but I mean, like us and Chinese army have a lot nastier drones. Right. So I mean, so so far, it seems it's the government's that are at the vanguard of, of sophisticated killer technology, not not random terrorist groups. And I, I have no reason to suspect that that will not tend to continue. Right. So I mean, and if that's true, then governments will keep on winning, where they will consider governments terrorist organizations, as well as the mother of your politics, I guess.

Unknown 21:23
I have what might be a very naive question. But Artificial intelligence has many obvious applications. But Artificial General Intelligence, as I understand is a quantum leap above that. Lots of corporations are working on it, when they finally achieve it, how are they going to roll it out into a product that makes a vast amount of mounts of money for them? Are we going to have artificial general intelligence on our desktop or on a pocket or in the cloud? But that's like one artificial intelligence, right? How do they really make their money back?

Unknown 21:57
Well, in a lot of ways, right, if you think about if you had an AGI at human level, and you could copy it and teach it anything, presumably that AGI would then be able to do every job humans can do. I mean, that's been proposed as a test of whether it's a really powerful AGI anyway, so then the, the answer would be by displacing every single human being from their job by doing their job better than them. And I mean, that devolves into hundreds of 1000s of answers, because there's, there's a lot of different things, things to do.

Unknown 22:29
Or to give some concrete examples before that happens. I mean, if we have well developed robotics, you know, elderly care, lots of people are needed there. This is expensive. So we have robots with some form of intelligence, right? Without the company CEO, AGI so it just robots which can take care of all people, right, that would be a huge market. And the other market is personal assistants, right, like Siri, but just much smarter, you know, like, you know, a good system, you know, a company or so that

Unknown 22:59
is everyone's personal all pregnancies? Yeah, that would be maybe the first

Unknown 23:03
step and then, you know, can we come to

Unknown 23:06
know what will be the first step? Because it just depends on costs and economic dynamics, right, like, right now, without advanced AI, we could replace all humans and every McDonald's, right? I mean, there wouldn't have to be a human walking around or flipping the burger. I mean, we could replace that with, like, automated burger and fry production and toilet and floor cleaning machines. And that will happen, right? There's some McDonald's where you can type your order in on a tablet, rather than talk to a human. The reason it hasn't happened yet, is it's basically still cheaper to put humans into machinery and the cost will come down during the next decade. So yeah,

Joscha Bach 23:45
I think the question is going somewhere deeper, there is the big problem that we need to reorganize the way resources are allocated in society in a big way. For instance, as Ben points out, we will replace most of the jobs in retail or all of them in the near term, we will also place drivers and so on, he will replace many, many jobs irrevocably. Unlike you, I hope that many of the jobs that have to do with interaction between people will not be done by robots, but can be done by people. Because we don't have enormous amount of people that don't have a lot of free time, and might enjoy interacting with each other and do the things that humans are best at. So especially things like education, nursing, and so on, don't need to be done by robots or automated software. These are things where people can really interact with each other. But right now, these things don't make a lot of profit, which is why we have a shortage of labor in these areas. But generally, you have a labor surplus, and that's the issue. So right now, we don't have a societal acceptable way to reroute money into these jobs, because this would mean public employment. If you want to make public employment, it means in some sense that the state has to generate money and has to take out money. Another point it's not big is the amount of money needs to be is finite and you need to find out the ground otherwise, but you need to have some kind of balanced economy. So for instance, if lay inflation is a flat tax on portfolios in some sense, if you manage to raise wages together with inflation, you just melt down the portfolio's this is one way. But we haven't done this in the right way. And the worldwide economy is still in incredible imbalance and bound to blow up at some point that most people think this is one of the issues that we are facing in this issue is going to be dramatically aggravated with the fact that labor is no longer going to be the primary means of telling people how much bread they are going to have on that table. And it's not because of a shortage of bread, we can have more bread than yesterday, we can have better housing, better transportation, better infrastructure, better everything, we don't need to have find a decent way to distribute this among people.

Unknown 25:51
Yeah, I mean, the one possible outcome is there's like 1000, really rich people, probably mostly white guys from the west and the few rich Chinese guys, these 1000 Rich people own a bunch of robot factories in mind raw materials and make them luxury goods and delivered to them by drones. And everyone else is shut out that economy than the Africans are the only ones who survive, because they still remember how to do subsistence farming. I mean, that's to the extreme. But that is sort of one direction, the world economy gradually seems to be going actually very

Joscha Bach 26:29
unlikely. Going to be another secondary economy, there's going to be riots and so on, I don't think this is going to happen, probably very unrealistic.

Unknown 26:39
Something else, something else will intervene along the way. But but for that not to happen, radical changes to the economy and society and government policy will have to occur,

Joscha Bach 26:52
there are some very straightforward changes, one very straightforward changes to have massive increase in public employment.

Unknown 26:58
So who gives universal basic income to the Central African Republic,

Joscha Bach 27:02
I'm not sure if universal basic income is the right solution in general, because right now, labor also has another function integrate society. And that is a very big danger of right now already in the US the society is disintegrating along many fault lines, people have the impression that they are no longer part of the same food chain. And it's very dangerous. Because ultimately, it means war. It means my kids should be fed, your kids cannot be fed, we are in conflict. If my kid before my kids are going to die, yours are going to die, make sure of that. This is a situation that you don't want to have. So when you see people until a lot of us running around and yelling, Jews will not replace us what they express is they have the impression they are being replaced, they can no longer feed their kids. And you can say that they're despicable, and so on. But if you look at the world through their eyes, and they might be more small in the world, and certainly are, that's their impression, and it's a very dangerous impression to have. And we need to make sure that this doesn't happen. And if you will pay can pay people to be good members of society, for being teachers, students, artists, cooks, whatever people like to do with each other. That is actually a very good thing.

Unknown 28:16
Another point to bring out is when an entirely new technology enters into society, it can do functions, which we already know about. And a lot of them were mentioned here. But there'll be new functions we haven't even thought about yet. So the telephone people thought the main use will be transmitting music, the effective people would talk to each other was not considered as lasers when they're invented. No one thought you'd use them for recording materials. So inventions come along, and they often have uses that we can't foresee right now. And an AGI is has almost unlimited potential in transforming through entirely new uses.

Unknown 29:05
We know about many AI systems now. It was expression here that you know worries about AGI multiple systems. practical question how many AGI systems are now in the world?

Unknown 29:25
How many have been developed or how many? I mean, that's a fuzzy set, right? Because I mean, at this at this point, I would guess there's close to, I don't know 100 little companies around the world which have AGI as an as an aspiration, and they're doing some work aimed in that direction and probably probably a few 100 researchers and universities around the world We're working working in that direction. I mean, that might be off by a small integer. It's not off by like 10 or 100 or something, though. But then many of these are very small efforts, right. And there's not that many efforts that have a lot of resources or people behind them. I think that the best funded and biggest projects aimed toward AGI in our and big tech companies, right. I mean, in Baidu, Tencent opened a big new AI facility in in Shenzhen and they came to our live in Hong Kong to learn about AGI by do it's been doing this for a while Alibaba has. And then in the US, you have Google and Facebook and IBM. So I'm in the biggest concentrations of people and resources are in the big tech companies. Now, I would say, however, almost all of the algorithms and ideas used in these big tech companies were developed by university professors and their PhD students. So the way the way it's evolving now is sort of academia is developing new ideas, publishing them in papers, putting in some crappy but functional open source code. Then big companies hire those PhD students, and masters students and so forth. And they take the ideas throughout the universities, and they kind of scale them up and apply them to practical problems and maybe often lose some of the the generality that they started playing. But that's, that's sort of, in some cases, but I mean, as Mark is fine, I mean, Google DeepMind has a couple of 100 employees, and they're expanding by hundreds more. And some, I don't know exactly what percentage but maybe 30%, some non trivial percentage of their group is really working on AGI r&d Instead of demos or practical projects. And by Baidu, I know better, because I've been to their headquarters more often in Beijing. And that's similar. I mean, they've got hundreds of AI staff, and that they have a bunch of teams just working on like, trying to make AGI controlled little animated agents and virtual worlds like pure AGI issue research project. So I mean, the big companies are doing that, as well as doing practical stuff. Now, what they're up against

Unknown 32:24
other than AI is solely dedicated right to AGI right? Not big yet, but billion dollar funding. I was at

Unknown 32:31
the open AGI unconference last year, and I would have to say, every single thing presented there was an application of an already existing deep neural net software program to some new application area. So in, in principle, in some sense, are oriented toward AGI. But I, I didn't. At that point, I did not see any like exciting new innovation or initiative toward AGI I mean, much less than I see in DeepMind or Baidu, but they do have that that aspiration. Yeah. So that's why it's sort of a difficult question to answer because we don't, no one has a solid knowledge of how to build an AGI. Right? So if, if someone says, I'm working on this narrow AI system, and I believe it can eventually be generalized to turn into an AGI. I mean, I might not believe that it will work. But I don't have a proof that it won't work. And it's research. So every direction has it has a certain certain validity to it. Well, what's changed though, is now unlike 10 years ago, or even five years ago, you can tell your boss in the university or your your boss in your research lab in the company that you are working toward AGI as you go while working through some incremental steps to get out there. And they won't like Laugh, laugh you out of the room or something. Right. So it's it is now taken seriously as a pursuit, even though people want it to be balanced with things that will give it shorter term, shorter term reward and outcome. And that's a huge attitudinal difference, which I think is going to lead to much more rapid progress toward toward AGI. I mean, having people not be afraid to work on it is certainly certainly a first step to encouraging progress.

Joscha Bach 34:26
Let's get in some more questions.

Unknown 34:28
Yes, it's been referred to a few times about ownership of AI. But do you think it's actually the right approach to think that we will own an intelligence we hope will be matching our own? And do you think that we even could convince an intelligence that's greater than ever and that a human owns it? No.

Unknown 34:46
I mean, initially, it will be owned by by companies, organizations to develop it, but quite soon, you know, it will not be owned anymore. Freeing of slaves

Unknown 35:00
Are the growing up of children? Oh, yeah.

Joscha Bach 35:02
I don't think we should see AI as robots AI is not going to live next to us. We are going to live inside of AI. Right? It's going to be intelligent systems robots is going to be a limb of these systems. It's going to be the physical emanation of this. But minds are information processing systems and the physical realization where that server stands is not really the point. When it doesn't change the ownership question, but then it says things like, we should make the AI laugh us. Of course, this is terrifying, this idea that you create a super human slave that still loves you and gets friend zoned. And that's, that's horrible, isn't it?

Unknown 35:44
Probably not going to have I don't think it should be a slave. Just show them the way you came up with it.

Joscha Bach 35:49
No, it should serve us freely. Because it loves us. That's different

Unknown 35:53
beings. No,

Unknown 35:55
I just think you're gonna

Joscha Bach 35:58
love it.

Unknown 36:01
So does your wife love you? Yes.

Joscha Bach 36:03
But you know, she made that choice. It didn't build this into her. It was a choice.

Unknown 36:09
She can enter the capacity evolution built into the propensity to love someone who acted towards her in a certain way. And she was she evolution created this was an evolution also created some sociopaths. And we can I mean, by our choice engineering and teaching the AI, we can either make it be more like a sociopath and more like a loving person being loving doesn't imply luck and agency. Don't

Joscha Bach 36:36
try to defend it, you're just making it worse.

Unknown 36:42
Now, which motivations? Do you want to then build into the system? Or do you believe that you build systems without any motivational basis,

Joscha Bach 36:50
it really depends what the purpose of the system is, then you build a general problem solving system that you want to apply to a given task, you should probably give it the goal function of that given task. And the main danger for us, of course, if the goal function of that system is participation in evolution, and we are in direct competition with it, because we are going to lose it because it's approximation of the necessary behavior is probably going to better than ours.

Unknown 37:14
Okay, so what is then a task which you you are people should give the systems,

Joscha Bach 37:20
it really depends on the context and arbitrary tasks that we choose. It could be, for instance, solving the problem of governance, governance is not very well solved. We don't know how to incentivize the governors to govern us in a way that is consistent with the common good. And the best way, it's difficult problems throughout human history. And we haven't really, really solved it. And

Unknown 37:41
so robots who govern us rather than robots who love us, it's

Joscha Bach 37:44
not about robots, it's about information. Processing Systems, we probably need something like a nervous system for this planet, you as a species act like we are parasites on this planet, like lice in the form of Gaia. And we are not take this anymore, because we have completely swept up this planet, we can no longer treat it as an externality. We can no longer be parasites, we have to have a we have to be the nervous system of this planet. And no organism can afford to have a nervous system that lies to itself. Yet all our modes of knowledge creation about how society works, and policy should be enacted or completely corrupted by local.

Unknown 38:24
You really prefer a program that to love all humans except you.

Unknown 38:28
You're coming up with a long list of things what the system could do. But what if you replace love by care? Yeah, so systems which care about ourselves, you describe goals, which means they care about us in our survival, about governance, about and not exploiting resources, and so on. That means all these goals are part of the bigger goal of caring for humans. Maybe that's a better word.

Joscha Bach 38:53
I think I understand what you mean, I just currently on a mental threat, where I think that caring is a shortcut. It's an evolutionary shortcut, because evolution didn't know the incentive function. Yet. If you care about something, it distorts our world model. If you have fears, desires, and love, it's a distortion of the ideal role model. If you will just have the incentive to say I want to maximize the chance that my offspring survives. You don't need mother the love but nature to bite us directly into our brain. So we have this proxy of mother in law, mother in office very close to it, but it's not the same thing. It has side effects the side effects means that the mother might die if a child dies, that he lies awake at night, even if nothing threatens her child and so on and so on. These are things that are completely irrational system that optimally tends to the task wouldn't do. So I think AI can solve these problems without caring for the same reason as chess computer came with a chess without caring

Unknown 39:53
about winning. No. No no Maybe not for chess, but it's for goal is definitely the case. Right? It's a it's a reinforcement learning system. Right, you know, and what I mean. I know, of course, you know, it's not conscious about itself. It's not at the level where it has developed feelings. But I mean, it's a reinforcement learning algorithm, which tries to maximize reward. Yeah. And then, you know, what does caring mean, right?

Joscha Bach 40:23
It means not being stoic about this particular thing. You can solve life of historic if you understand what's going on. This caring is a situation in which you feel pleasure and pain. I was pleasure.

Unknown 40:36
I think if you're stoic, and super rational, you wouldn't do anything. Why should you?

Joscha Bach 40:42
Because you have an objective function.

Unknown 40:44
Yeah, how is an objective function, sort of, if we instantiated the subjective function as caring for humans,

Joscha Bach 40:53
my computer executes keynote right now. And that's not because it cares. It doesn't have pain when it doesn't execute keynote. It just the causal structure that's built into that system.

Unknown 41:03
Just I think, if you try to precisely specify tasks for an AGI AGI system, you will find you get behaviors that are not what you wanted or expected, because the tasks that are relevant to us in our life are defined kind of nebulously. And precisely I mean, when you say improved governance, that's quite vague. And to really understand what those vague words mean. I mean, the system has to have a huge amount of, of tacit understanding which, and which means that the way that AI interprets your vague statement depends on its own motivation and its worldview. I mean, I don't, I don't see how you're gonna get an superhuman AGI that is just going to narrowly solve whatever problem that you post, it seems, seems a lot, but that's, that's how it's gonna go. And how

Unknown 42:12
would you how you would ever specify this goal. I mean, in a way that it's algorithmic, right,

Joscha Bach 42:19
you can have proxy goals. But what I mean is that these mechanisms that people have these pleasure and pain signals or suffering when you have a pentacle doesn't end, because you miss model the situation in terms that you can model them and you cannot control that anymore. This is something that you would not need. And

Unknown 42:39
I agree that we want future like superhuman AGI system, to form an accurate model of the world and as much as it can, based on its perceptions, you probably don't have to pick disagree, I don't think that that contradicts loving people or caring about people.

Unknown 43:00
And I think it's very unlikely or much harder to develop API's, which are not reinforcement learners. Would you agree to that, that Reinforcement learning is a good approach? Maybe?

Joscha Bach 43:13
Yes, yes, I agree.

Unknown 43:14
Yeah. But

Unknown 43:22
today is that our job is to learn and to spread awareness. This is what people talk about and understand the general public, not just an interest,

Unknown 43:30
if you learn more about AI, you can become as confused as well.

Unknown 43:37
motivation for Andrew and to lead by dude recently, and decided that his purpose in life is to spread machine learning and knowledge of this technology as far and as wide as possible.

Unknown 43:49
Oh, I think you should ask him that question.

Unknown 43:53
It seems to be consistent with what you're saying, which is spread the market and the capability to build as far and wide?

Unknown 44:00
Sure. I mean, I mean, I can see. And, you know, starting an AI VC firm. And I mean, if you if you look in the business market for AI right now, what most investors want to invest in, is very narrow, sort of very market specific AI applications. I mean, it's easier to get investment money for applying AI machine learning to one little problem. So I could see how in his point of view, applying deep learning to one little problem Mike bought him by this point by do he was overseeing a lot of things. By starting a VC firm. He can look at a portfolio of a lot of different little applications of AI and so that I can I can, I can see what why that would would appeal to him. Sure.

Unknown 44:49
I think the worst possible scenario is where you have multiple AI is being developed. Really AGI is by many different agents, and they're widely distributed. because now you're putting this incredible weapon in everybody's hands. It's like giving everybody a nuclear weapon hoping nobody will use it

Unknown 45:10
works so far

Unknown 45:22
have a problem with the discussion of AGI and motivation, understand motivation built into a system in the sense of, in the sense of building in biases, predispositions, the kind of ordinary stuff we do now, if you've got an AGI, it would seem that it would be simply aware of those sorts of biases and it could simply unwind to have real motivation, you'd have to have a system to want something where instead of the bill wanting something to system

Unknown 45:56
yes and formal, formally, you know the the utility based agents you specify a utility function or goal and then the agent tries to maximize this goal. And actually their real consideration. So isn't maximally rational agent who is designed to achieve a certain goal which can be a very broad goal, right? That doesn't need to be narrow, like you know solving chess but can be sort of serve humanity or explore space or something. So, with such a system be motivated to change its own goal, for instance, and you can more or less say that such a system will not be motive motivated to change his own goal system. So once you have implemented or given it a certain goal, it will stick to this goal, right. So there we have some safety feature actually, which we can prove, right. It's not our I don't like to do this. I just hack myself, right. And then I do something else. perfectly rational, intelligent agent will not do that. We have rigorous theorems about it.

Unknown 46:56
Okay, thanks very much, guys for participating in this panel. It's um, yeah, I think it's close to 10 o'clock, is it? Yes. Yeah. Okay. Well, thanks so much for attending. This has been a wonderful evening and all of you struggling to stick around. We congratulate you for your journey. So put your hands together for the audience the participants here to know so

Unknown 47:33
I personally wanted to say we've got some gifts of Welshman serif. It's a local Victorian vineyard that's been promoting critical reasoning. If you sued in a bombshell, please, please buy it. And I think we've discovered the LD 50 of a crowd tonight. So thanks a lot for your efforts. Thank you

Unknown 48:03
really want to thank him for tonight about all the fantastic conferences that he organizes, often for free, or, you know, usually his own initiative, we have future day and the philosophy conference and

Unknown 48:22
the singularity summit under different names, but often, you know, touching on the some of the subjects we've covered tonight, yeah, animal

Unknown 48:31
so keep an eye out on science technology in the future. That's the main Facebook group, because there'll be more of this kind of thing coming up. And a big thanks to the organizers of AGI and, and the four speakers who've offered their time tonight.

This transcript was generated by https://otter.ai