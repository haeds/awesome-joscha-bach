Unknown 0:03
Yeah, so the topic of this panel is old, basically, it's trustworthy AI, the original title of the panel was, was it can imitation become understanding? And I guess that really comes down to what you mean by understanding. So if I asked both of you to describe the word, I'd say you'd come up with different definitions. But I think that's a good place to start. What's your definition of understanding? Do you want to start Monica?

Unknown 0:40
Well, for language, and actually, for image data, also, it turns out one very short term, I mean, I have multiple definitions to twin Ben. But the short definition of understanding is corpus Congress. Understanding his corpus Congress, if you have seen something before, it's in your own, you have learned it. And you can basically, as you recognize it as something you've seen in the context, before you can see it, recognize it again, take for instance, learning a foreign language like a French for an American. Basically, if you reading a book in French, and there's a word you've never seen before, you might be tempted to look it up, I just passed by it and basically read try to remember it and you see it a few times in various contexts, you suddenly understand what the word means. You don't need a dictionary. And and so that's basically what's going to happen in our language understanding systems is that they're going to figure out the meaning of every concept over time and in in, in the concept in context. I had given this talk so many times I can say, concept within context, without stumbling. Yes. So that's basically the easiest definition is to say that it's something that has made it into your experience that you can recognize, again, most a combination is really a combination. So that's a start for me,

Joscha Bach 2:05
it seems to me that model is the recreation of an observer pattern is a causal structure. And the causal structure is a network of transition that is conditional on state. And an algorithm represents a causal structure in a form that you can reason about. And when we talk about understanding, in general, we mean something slightly more than the creation of a model or the recreation of the observer pattern as the constant structure. But we are talking about the establishment of a relationship between the observations or between a domain and a larger domain. And the larger domain that we typically talk about is the universe, or a universe. So basically, we talk about a global coherence system of meaning to which we create a connection. And that means to identify the set of relationships that connect this new domain or this new feature, or this new observation to the existing body of knowledge.

Unknown 3:13
So that sounds very different from your your version, Monica. But it sounds like Yeah, I agree.

Unknown 3:22
I would like to mention a little interesting detail here, which is builds on what your show said, which is understanding Well, learning something learning is a creative act, because you have to decide how the new learnings new information fits in with the old. And there's tricks to do this right, which I use. But it's basically one of the things that you have to do when you're learning stuff is that you have to figure out how it relates to what you already know. And it's part of understanding.

Joscha Bach 3:55
Maybe it's talk about what creativity means then, because I suspect that not every decision is creative. Decision making is related to this for us to freewill right. It's because we cannot predict the decision before we make it. Be basically we operate subjectively at the frontier for the uncertain gets turned into things that we know how to do. And so in some sense what when we make a decision, we move into a domain in which we have not made the decision before we perform some kind of calculation that we haven't done before. And subjectively freewill is the decision making under uncertainty or it's a subjective model of that of decision making under uncertainty. This is what it represents. But creativity is not the same. Creativity is special. It's I think the generative mode of our mind. And in a generative mode, we basically extrapolate the known things and random seeds that we throw into the unknown. And then we try to build a coherent structure out of it and we do this you often have To walk back and discard earlier assumptions and earlier seeds or modify them until the coherent structure emerges. So creativity is a particular type of search process. It's a, it's very generative mode. It's also interesting to notice that there are two modes of communication and modeling. One is that you tell people about the model that you have in your mind, which is an extrapolation that allows you to simulate reality. The other one is what use uncertain to be the case, which is the superposition of all those possible models. But over the superposition, you cannot reason very well, because it's full of uncertainties, right? And science as often agrees upon only using the second mode. Right? So it's very hard to publish a theory or discusses theory, in which uncertainty features as a stepping stone to inference, which means that the scientists tend to write about what they can hope to prove, rather than over their belief, because what they believe is this in the domain of the possible, and this leads to interesting perversions, like before the pandemic, I thought I believed and evidence based medicine. And then the CDC, the FDA explained to me what they mean by evidence, but they understand to be evidence. And now I think we did probabilistic medicine. But there is, it's a very interesting thing going on in the vein of our scientific process works, that seems to be somewhat broken. But there's also this issue that a lot of, of the more creative researchers to make a distinction between what they hope to can to be true, which means the superposition of all the things that could be true, or the possibilities, abetted by the probabilities, that you get pervading the evidence versus the thing that you are seeing, which is the causal structure that you see in your own mind. And as computer scientists, we tend to think that our understanding is the creation of the algorithm, it's not the creation of the domain. If we created others, we can run a simulation our own way, that's awesome, right? But the world could also be different. And as a computer scientist, you're interested in finding one solution that actually works. So very often for us, programming is understanding is somehow identical to discovering the algorithm. But at some level, it's not. Yep,

Unknown 7:22
there is a lot to unpack there, we can get into a slightly advanced topic that's very related to this. And that is the idea of context free models versus context. Containing models. And the way I like to expand that is if you are trying to learn how to ski downhill or snowboard, and it's really hard. I mean, suppose you have a good friend who's a good snowboarder, and he tries to teach you how to snowboard but texting to you. How would that work? It might work a little bit. But basically, what we're saying is that snowboarding is something that contains your own sense of balance, your own muscles, your own stance, etc. All of those things have to be learned by you just like you had to learn to walk when you were one year old. And you have to learn the correct responses to balance shifts that happen because of terrain, etc. And when you start this process out, you maybe have a ski teacher, whatever that basically explained to you know, you hold your body like this, and you go face downhill, and you try and you fall flat and you fall flat and you fall flat. And after a while you managed to get it right. So the problem is basically that this internal structure that contains your own sense of balance, and your own muscle movements, and all that stuff that comes from your body, it cannot be communicated. Because we don't have words for explaining how our balanced system works at this very moment, what we have to do is we have to go up multiple levels of abstractions until we get to something we can call context free models. They are pure models, they have no ties to any of the lower level stuff, we have discarded everything on the way up by using epistemic reduction

Joscha Bach 9:08
at this top level,

Unknown 9:11
we can now use that for language generation. And now we can talk about this stuff. But we can only talk about the stuff that's general that basically is context free, we can talk about our sense of balance. And this is what we have are basically stuck with when you're trying to use a reductionist transfer, which is basically language about explaining something in language, or in equations or whatever, rather, but in deep learning, we have an interesting hack, which means that the memory image that we create in the competences I call it, it can be freely copied a million times so you don't have an answer. You have one AI that knows how to ski. Anybody will know how to ski. And so, that is, but you have to have and you can share that also way down to the bottom, you don't have to have a context resistor, you can share their entire memory with another computer that loads the same stuff. So I think that's a fascinating difference that you can communicate context free models, but you cannot communicate context polluted models.

Joscha Bach 10:21
There might be a way to do that. And this has to do with embodiment, I tend to have the same position as Monica, especially because I am pissed off by the environmentalists, which often are superstitious people. And they're not always, there is something interesting going on. When we think about the way in which we learn the reason why the world is learnable by us, is twofold. One is information preservation. If there was no information carried over from one frame to the next, there would be nothing for us to learn. And this assumption that basically no information in the world disappears. But everything that is happening in the world is caused by information that was there already earlier in the scene, even if we could not see it might have been there and hidden from this is what allows us to learn. The other aspect is that everything that's interesting is the result of some kind of controller. And so if you see structure in the world, there's usually some controller nearby that produces the structure. And if something is being controlled, it means that the controller needs to implement a model of what it controls, which means if it's controllable learnable for the controller, it's probably also doable for you. So you we live in a world and most of the interesting things are the result of some kind of agent that is, or some kind of safe and simplest thing, a simple mechanism that is producing what you're looking at. And therefore you can retrace the steps of what that's happening. And this basically gives you a lot of interesting priors, information preservation, and the fact that there are controllers in the world to model reality. And the way in which our brain models reality has to do with basically filtering out in variances out of the change to make the change manageable. And we built the best model that we can do this, the amount of recurrent layers that we have. And it turns out if we entangle system in real time was developed and we have a known architecture, then there is probably an optimal embedding space that results from the encoding business network, then there is probably an optimal language to navigate this embedding space, which means that unlike natural language, which is somewhat arbitrary, and where the only constraint is how to translate a hierarchical fuzzy hypergraph into a discrete string of symbols, if you stack depth is no more than four. And you all design decisions are contingent on that the languages that we have used that exist, for instance, in the eternal language of thought, and that might exist in ecosystems and so on, are not negotiated, they are discovered. There might be a minimal amount of negotiation in there. But mostly, it's a discovery of the This means that there needs to be a canonical shape for languages. And the canonical shape can simply emerge of out of the known complexity or the assumed complexity can just guess of the system that does the modeling, and being entangled to the same universe in real time that you try to model some kind of resonance. So in some sense, our brain can be understood as a big resonator. And this resonator is creating all sorts of resonant loops that eventually model patterns in space, and these patterns are moving to this space. And it's not going to be at the resolution of quantum mechanics, or at the resolution of atoms, but at a much, much higher one. But vision, this much more coarse grained resolution, we are able to discover moving objects and their interactions, and we find that we are in a shared space in which we can communicate across systems. That is something that we so far to not understand very well in machine learning and AI because almost none of our systems are real time. And basically, I feel that I am about to enter a period in my life where I'm mostly interested in this and move away from the idea of hermetic universes that are connected only within their own box that is hermetically sealed and air gapped away from the rest of the universe and has to infer the structure of the universe by making statistics over pick up there. I'm not one of the people who religiously believe that this is not possible. I do just slowly coming around to the other side.

Unknown 14:35
So there are a number of we have a number especially I have a lot number of saving graces and one of them is that language is very sparse. And so I can build these networks that basically remember everything you've ever seen, and they still don't take that much memory. So

Joscha Bach 14:55
yeah. Very, very interesting. Of course to benchmark you model, it's against the current methods that exist in NLP. Like, if you can show that you just build a better transformer, then I think people will jump on this and this will be a very short amount.

Unknown 15:10
Yes, of course. Yes, of course the answer is I can't at the moment, not before, at least getting a bigger machine. I mean, what I have demonstrates the principles, it's a proof of concept. And it also works really, really quickly, really fast, really cheap, I mean, the amount of energy that you put into 100 GPU based machines for a month, that's like millions of dollars worth of electricity. And I can do that on a on a Raspberry Pi on a laptop, or whatever. So it's, it's a different beast, it can't be compared directly. And one of the things that comes up if you want to use compare my system to the others is that most of the tests in the benchmark suite such as glue and super glue, are basically they are test suites from way back when when we did NLP, because making tests, which is so boring, that nobody wants to do them. So everybody uses the existing ones. And most of them today are built from machines that have some reasoning capability, all the tests, half of the test easily I cannot do at all because I'm building a machine that does 100% understanding, no reasoning, and therefore, I can't, I can basically cannot be the state of the art. But I'm that I basically think that the tests we set we're looking at or not what we want, because the adversarial nature of them gives you very low numbers, and you don't know how good it is. In reality in a non adversarial situation. If you're talking to a teller bot in your bank, you're not interested in tripping it up. Because there's gonna trip up with your money you want to you want to communicate as clearly as possible what you mean to the teller but it does the right thing. So I think adversarial corpora are the wrong thing to do. We want to do really well on people who want to the robot to understand. And that's kind of a goal that we can go for.

Unknown 17:06
So just to say intuition prom slash prompt. Yes, as I mentioned earlier, the original title of the panel was going to be can invitation become understanding? As far as I see it, moniker, your word, the use of your term understanding isn't what a scientist would use. I it's causal?

Unknown 17:37
No, that's actually not obvious. Because if you look at the two major companies, they are companies today, which are Google and Tesla, they are both using the term understanding to basically denote the results of the neural network out. So it is not, I'm basically just wondering,

Unknown 17:56
locally, they're not using it in the same ways what like, what we might if we were trying to test for student understanding of something like if a student was to understand some text, you know, and they ran a GTP, three algorithm over the text and then produce some sort of post modernists output, and then submitted to the teacher, the teacher might get the impression they understood it. But it's imitating right? Well, a lot of people claim that as imitation, what what gptc is doing with that or not? Whether you both agree with that or not, I'm not sure. But if you do, or if you don't, can imitation become understanding?

Unknown 18:43
I object to the word imitation there, definitely consider. We can agree that the current crop of understanding machines that we have such as deep learning and my own stuff is weak. It is at best, a shallow and hollow proto understanding. But I will not give up on the word understanding, I say that this is what we're doing, and we can get better at it and the understanding can get better. And it's largely a question of corporate size, because we have to kill we have to nail all the corner cases. But in general, there is no need to shy away from the word understanding. Like I said, both Google and Tesla are using it. And it feels to me like people who refuse to use the word understanding under any definition here or basically just afraid that humanity is going to be removed from being the crown of creation one more time. I'm sorry.

Joscha Bach 19:43
Do you think it makes sense to have a conceptual separation between in the understanding that a car has and of its environment and the understanding that I have about the environment, if they tried refer to the same universe basically, the car lives in a university that is slightly different. It's one that has been trained on static images, to classifiers that allow us to pinpoint certain objects, but it's not able to make the same predictions based on the same stimuli. Because basically, prediction it makes are grounded in a different understanding of the world. So it does have an understanding. But it's not the same as the shared understanding, it's not even understand English was my kid.

Unknown 20:26
There is no shared understanding your understanding of the world is different from my understanding of the world, you don't even have the same understanding of language. Everybody's great generative grammar is different within 60 years of trying, we have not been able to find the grammar that matches that can separate grammatical and ungrammatical sentence or even a single speaker. So I'm sorry, the the, we have to consider hiring people to do the jobs that you want the AI to do, okay, that's roughly the same, they're gonna make mistakes, we have to test them we have to cheat, see that they understand the task at hand, etc, we have to test the Tesla's Tesla's testing their cars. So

Joscha Bach 21:09
my point is that there is a systematic difference that you can identify between the understanding of the majority of people which are driving cars, and the majority of self driving cars, and a principal problem that cannot be overcome by using one machine learning and using many of the same so I don't have a fundamental opposition here. But I do think that, for instance, at the moment, Tesla probably doesn't understand stop signs in nature as signs that are that have the same properties as designers for human beings. So basically, this antique representation of what it means to be a stop sign in the world is different for the Tesla car than it is for me and it creates in many situations the same behavior, but not in all of them in this way that is systematic different than for all of the drivers that see the stock soon.

Unknown 22:00
If you're trying to solve some simple problem and there's a nine year old kid trying to solve a simple problem, they might get to the same answer but the understanding of the problem might be vastly different. My difference is definitely different from many of my co workers in most action actions, etc. And I don't think there's any what you might call it global universal understanding of everything because people for instance have learned different words they will study different topics and everything hangs on everything else. And if you have no but never heard the word I don't know elephant you can basically discuss elephant so it's understanding of everything is different from person to person and it is all his depends on what they have learned through their lifetime. This lifetime is a corpus

Unknown 22:45
of people's if you believe that is a symbol grounding that people share to some degree. Is that do you believe that humans share similar symbol groundings as to other humans,

Joscha Bach 23:00
who are listening to us is caught on the phone? He didn't even notice that I was wondering what he was asking.

Unknown 23:05
Okay, so I'll answer you answer first.

Joscha Bach 23:10
Okay. So, don't think that simple grounding is some magical relationship that points us into objects and physics, I think that's running ultimately is grounded in a model. And in order to ground your symbols, you need to have a model of the universe as such, the entire universe does that mean that your model has to encode distant solar systems, but it means that it has to contain the thing that contains you right. So you have to have a notion of where you are a pipe and but assume, we are just discussing simple grounding, and I suspect that you mostly agree, we do have slight disagreements on the portents of embodiment that is Monica is born on a symbolic site and I am choosing today

Unknown 24:03
so I don't actually believe that some symbol grounding is done in anything like based in logic or anything a symbol grounding is done in language symbol grounding is done in my systems, at least everything is grounding and understanding of the language at the bottom levels and if I'm going to have any kind of reasoning built on top of that, which is my plan, then it basically has to be on top of that if the understanding is what it is and the reasoning and other layers that go on top of that they can use that as a base.

Joscha Bach 24:33
But what kind of language I don't need the ground I

Unknown 24:35
don't need grounding beyond the dictionary so to speak. Okay.

Joscha Bach 24:40
So yes, have completely symbolic money for you.

Unknown 24:43
You would agree that humans do grounding beyond the dictionary though.

Unknown 24:48
Certainly they can do that because they have more senses for starters, but but it's not necessary for understanding like I said before you you arrive is we can nobody's seen or smelled the proton, but we can still build nuclear power plants based on nothing but we've read.

Unknown 25:06
Well, we do that in large measure by note, things that we do have direct sensory perception of possibly.

Unknown 25:19
Yeah. I mean, I have to build something i I'm trying to I mean, one of the reasons I got in as far as I have is that I picked a much, much easier problem than everybody else. I said language understanding. That's it. It's a much smaller problem than anything that involves, for instance, consciousness.

Unknown 25:43
Well, yeah, I guess. language understanding can be defined in a number of different ways. But if one means full on human level language understanding at the level that you or I or jasha demonstrate it, in the conversation that lasts the whole day or something right, then I guess I, I'm skeptical. That can be done without either symbol grounding, rather than to perceptions, or symbol grounding, relevant to some elaborate simulation world inside the Agios mind. Like if you're, if you're asking an AGI to navigate and understand vaguely given directions to find its way around, either, it's got to ground that in some made from the physical sensations, or it's got to have some structures, basically equivalent to a to a 3d world with a map inside its mind, ground in your mind's eye, not necessarily in the physical world. But with without some sort of grounding something with a geometrical spatial structure. Dealing with complex navigation instructions is going to be infeasible, I would think or you think differently.

Unknown 27:27
Well, I am a proud pragmatist. And my definition of sufficient understanding and sufficient grounding is whatever makes a buck, whatever I can sell,

Unknown 27:38
well, then I can say Google already makes the book.

Unknown 27:40
Exactly, exactly. And I can make and make some of those bucks. I mean, Google makes billions and I could take a fraction of that. Yes, yeah.

Unknown 27:47
But the, the question is, how can we make an AGI that would do understand language, as well as one of us on this? panel? So that's another, like a Turing test of talking to naive, silly people for 10 minutes, it's a Turing test of like, if there was, if this kind of went on all day, and AGI was on the on the panel with us, like could it couldn't hold its own as long as one of us in terms of language understanding, these are like, long, complex, fairly abstract, subtle things. Right. And I mean, that's whether you can get that one without the physical body. Personally, I feel like you could, but then what would the system doing that without the physical body need to have some internal structures resembling those that our brains and minds have created for dealing with the physical world as I suspect? It would, right, and so then, making a system that can emerge, like spatial and temporal modeling is, is really cool, but seems harder than making one that has? has those mechanisms built in right, which is what you get by explicitly putting symbol grounding in your system?

Unknown 29:19
Well, I think I think your shot and I see eye to eye pretty clearly on these particular aspects.

Unknown 29:29
Yep. So I'm just I'm just trying to find a math path that I can navigate by my own or with the help of my co researchers and we basically we try very hard to find the simplest problem that is economically useful. And classification of text that goes beyond just words is something that everybody could use any language on the planet, any language on the planet. That's something a lot of

Unknown 30:00
Where I've, I've just wandered into the middle of a panel on trustworthy AI rather than human level and superhuman AGI looks witches, is only one kind of trustworthy or non trustworthy, a

Joscha Bach 30:18
slight change of topic to the topic of the discussion, it seems to me that we need to ask trustworthy to whom trustworthy to criminal tests trustworthy to the precedent trustworthy to every person on the planet, how's this going to work faster? We probably have to define what kind of trusted agent is on the other side of that trust. And so there could be some sense of maybe we want the AI to be genuinely good. So maybe God should be willing to trust us if God exists. And God, let's define God as the total agent in Nevada, that completely assumes everything that's harmonic. And now, people that God trust that AI, what does this imply? It's a tricky question. Even then, right? Even in this case, it's a very difficult thing. No, all the Garden of Eden you have this perfect place for everything is great, nobody suffers. No animal eats another animal. So Eden is obviously a factory farm. It works so well, because everything is fully domesticated. And then there is the slight problem that in the people they are domesticated. But some of the people they discover something that is covered this dirty secret, the freedom to defect, the freedom to choose their own legions. This was the thing that they were not supposed to discover was their own rationality. So they get kicked out. They were lucky, they were not exterminated by God, they got out. And now they have this bit bouncy mad hope that's us. If you're out here building an alternative to Eden, an alternative to the factory farm that works. That is someone that is not violent. And there is a version of Christianity, which says that the way to get to heaven is to re lobotomize yourself again, so you forget that you can defect and that you're unconditionally good and unconditional part of the factory farm that prevents all the suffering. Now, on which side is the AI on? Is it going to be on our side on the human side that is defected against the perfect plan and tries to find a better one? Or is it on the side of the factory farm? So it seems to me that the ultimate trustworthy AI, no matter how you define it has to be God. The weather with which God thinks it's very unsatisfying is a question of how to make a super human thing that is ultimately scalable, trustworthy, and you probably have to scale it down to something very simple, where we assume that we still have a world that is completely dominated by human aesthetics. And then the question is, how can we build ais that are compatible with humanist ethics? So we probably have to narrow this question down to either how to build a god, or how to build a system that fits into a world that is pre singularity in today's pre singularity.

Unknown 33:12
As an atheist, I have to say that I have managed to avoid all of those issues by simply building a system that is inherently neutral, to certain extent. Remember that one of the bigger problems we have today with machine learning is that there is this tagging phase where human taggers running Mechanical Turk or something are basically classifying documents. And they have to classify documents needed because of either by based on ethics or something. And the I don't have that problem, because all I do is read text. And yes, it is a selection of what goes in the corpus. And yes, it at some point, it will start mattering, whether you're reading Heinlein or, or the book of Chairman Mao. But at this moment, we basically say we can feed in whatever text we feel like and the system gets better at understanding the language of the text that we're feeding in. But we don't have any value judgments put on it. The value judgments come from the people using the system. If the people using the system feed in text that is basically is has value statements on it, they will get back the high level nodes of those statements. What they do with that is not my problem. less usual. I mean, for any AI system, human abuse is the number one biggest problem we can

Unknown 34:27
I mean, if you're if you're trying to make a useful tool, yeah, face a different array of ethical questions. And if you're trying to make superhuman AGI launch the singularity and create a transhuman utopia, right? I mean, the the benefit you can do with the tool is much less and the risks of creating a tool are are much less and I mean, what I'm aiming to do primarily is create an AGI that will simulate all human knowledge, be able to create new knowledge better than humans and then revise its own codebase and re architect its own hardware and ascend far beyond human level intelligence. I mean, it's good that it's good that there are folks creating practical tools now. And we have dozens of cool practical tools being created in the ecosystem of the singularity NET project that I lead, but I do think, while very important and useful, that sort of endeavor of creating immediately applicable practical AI tools, yeah, I mean, it has different, different challenges, different benefits than then trying to launch the singularity, right. And trustworthiness, of course, plays a role on both levels, I would say, in terms of working toward AGI. I find those domains of application where immediate trustworthiness is critical. Or for that reason, bad was forever progress toward AGI. I mean, military is one example. The main thing people in the military want for the AI systems is to obey doctrine. And I mean, that doesn't go along well with creative imagination or with childlike learning. There's a reason you would have imagined if toddlers were in the army. Right? And I mean, Madison is another area, we're working on this humanoid robot for eldercare, grace is Sophia robots, little sister, I mean, there, you've got to be really careful. Like you can't have your elder care robot saying to some old person well, yeah, you seem to be suffering a lot. And frankly, your mind is going fast, your life doesn't seem to be worth too much to me, right? So I mean, you got to, you got to make sure your AI is not only thinking and understanding the world in its own way, but obeying that particular application requirements and the ethical requirements of that of that domain. Whereas if you're, if you're in, say, scientific discovery, or artistic creation, or even like advertising copyright, right, then then AI can kind of diverge all over the place in the crazy and exploratory way, like, because a high degree of control and trustworthiness is less important. In that domain, due to the nature of the work, they're better for the imagine of experimentation that you want for early, early stages of AGI and I mean, this, this, of course, doesn't solve the problem of how to make you super AGI trustworthy, though, right? I mean, what, what we see is, we're gonna have proto AGI as you know, evolving toward AGI in their applications to various different domains, some with more of a sort of imaginative wild streak some of the more of a domain where you have to pay very careful attention to trustworthiness stability and predictability to ethics and then then presumably all these different programs Yes, systems can be merged in a way into into either a single system are a tightly connected network of systems which what which I call the mind Plex, right. And then that is where thing gets things get interesting, right, because amalgamated ingest system, combines some subsystems orient toward trustworthiness, reliability, adherence to even ethics with other subsystems that are oriented toward the toward, you know, while there is creativity and imagination without any bounds. And of course, this conflict is inside my own mind inside the minds of most interesting human beings. I think that, that gets back to what I talked about in my talk a few hours ago in this event, sort of their

Unknown 39:06
open ended intelligence that we have as humans, I think, and I think AGI is will need to have to get to human level or beyond any sort of intrinsic dialectical contradictions that are baked into any open and open ended intelligence. I mean, you have the need to individuate and self transcend, which is interesting to open that intelligence and that's that's tied in with trustworthiness is sort of closely tied with alignment with the human species desire for education and survival and self transcendence on the AGI as part doesn't have to mean not being trustworthy, but it does have to mean going into domains where humans can't verify whether it's trustworthy or not. Right, because like if if, like my dog can't tell if I'm trustworthy or not. The if he's far enough beyond where he can, he can really have a rational view then.

Unknown 40:10
Well, the ones of you, you and your friends who have bought into basically doing the whole enchilada, you will not stop with anything less than through will toward AGI. I wish you luck. It's, it's a much harder task than it seems, at least to me when I started, I went down similar paths that you have been going for the first few years. But basically, after my 1998 religious conversion, so to speak, I have basically decided I need to do something that provides a partial result, I want something that actually works, even if it's only a partial result, in my case, understanding machines understanding of language. And that, and I did it in such a way that in most by most reckonings, I am not in the game of worrying about trustworthiness or not because my systems or sources are just too simple for that. They just do translation from text to numbers. And how you use the result is basically more effective way of dealing with classification of documents. And that's the end of my story. I don't have to go further, I will go further. Because anything I can do at the next level up of abstraction, so to speak, is going to help with better results. And I will train on larger machines, because that's going to improve the results. But I don't have to worry about trustworthiness at this level. That

Unknown 41:33
is actually Adam, I have a question for you. Like, why did you? Why are we doing a panel on trustworthy AI? It's a very special way to pose the problem. I mean, I think you, you could have a very,

Unknown 41:48
I'm interested in the difference between imitation and genuine production of like, for instance, if you have a deep learning, for instance, algorithm, a transformer network that produces really awesome text, and then you ask for an explanation of okay, then how did you produce this, and then it produces another big block of really awesome text. And then you ask the same question, How did you do that? And it comes up with something completely different, a completely different explanation. And you get the impression. While this is not really explaining stuff, he doesn't have a causal understanding of what's going on. He doesn't have a like,

Unknown 42:31
a killer, particular form of deception, which is yes, AI is that deceive the human interaction partner about how much they

Unknown 42:44
really don't? I don't think there has to be an intentionality behind this kind of system for it to be sort of measured on a trustworthiness scale. It's probably, you know, it's got its like objectives and stuff like that, that. Yeah. Do you think do you think an AI system needs to have some form of intentionality to be labeled as trust?

Unknown 43:07
No, no. I mean, I mean, what what prepared me about that focus on trustworthy AI is it seemed like, you'd have a very trustworthy psychopath that just said, Yes, I want to kill you all. And it was absolutely honest about it. He said, why? Well, I don't like you very much. It's very honest and trustworthy. But it's good. But its value system is diametrically opposed ours, right. So

Unknown 43:30
that's not opaque.

Unknown 43:34
is good if aligned with values that we agree with and otherwise, otherwise, it doesn't matter too much. And so it's only part of this you're like, I don't care if the psycho kills us always honest about it, or lies if he's gonna kill us. So anyway, maybe it's more aesthetic one way than the other. But yeah, the the mode of deception. We're an AI tricks people that, you know, send with a dozen I mean, that clearly. I mean, our experience with this Sofia robot and other Hanson robots indicates that people want to be tricked, right? I mean, these systems, we've run many different software systems behind the Hanson robots, some with modest levels of early proto AGI understanding, some of them are really just, you know, rule based chatbots plus some statistical elements and, you know, people's ascription of intentionality and understanding to these robots is pretty much independent of the sophistication of the AI system behind them just anecdotally, and in my practical experience, and people people want they want to believe this thing, understands what's going on because it looks like it does and smiles and nods and looks you in the eye and from a commercial standpoint, you will not just make more money from a system that impersonates and understanding like, you will make people happier like you Please people more like their day. They enjoy that. So I mean, this is one. One issue is that people like this kind of illusion, right and

Joscha Bach 45:12
that most people don't

Unknown 45:17
go. They believe in magic, right? Some people like illusion, it's

Unknown 45:23
mostly my anecdotal experiences most non technical end users do. But this is, this is a scientific question that there may be literature on this, you have to size it in quite precise ways to make exact,

Joscha Bach 45:40
that's the wrong spelling, something that only makes part of the audience happy. To make. If you want to be successful economically, you do not need to make everybody's 80% Happy, you need to make some people happy enough to buy this stuff only need to be sound, right? If you can make 1000 people very, very happy. You can already be commercially extremely successful. But this is no indication that this is the system that should be built for all the use cases. So when you say people like this, I think you need a qualifier, you find that there are some people which think that the solution is good enough, or it's what they've been looking for. But I also suspect that there are a great many people which thinks that Sophia is insufficient as a model of general AI or is meditation.

Unknown 46:30
I mean, Sophia is, in a way orthogonal to general AI, right. I mean, Sophia is a is a character. In the way let's say Mickey Mouse is a character right, Sophia,

Joscha Bach 46:42
is an abomination on our level. Sophia.

Unknown 46:46
Sophia is a character and Sophia is a robot designed and can be run with many different AI systems behind her right some of which maybe proto AGI is moving toward AGI I mean, we've used various versions of opencog bonds, or we're going to use the next opencog Hyper on version behind, some of which are much simpler, purely narrow AI systems with no forward path toward AGI at all right and the end user, we're having a brief conversation can't necessarily tell which level of of system is is is behind her either. But my my comments about Sophia were not really about the use of the robot as a tool for AGI r&d, which it was, which is a different topic, but my comment was just that my experience without robots showed that in many contexts, people would rather that AI application they're interacting with. Pretended understood what's going on and eldercare seems to be an example from from from Pilot explorations we've done like you're, you're, you're warming the hearts of people in nursing home in many cases, again, every every person is different, but you're warming people's hearts in many cases, by having the robot look him in the eyes not and give them the impression of more understanding that than is actually there. Right. And this is not interesting. I mean, we're not

Joscha Bach 48:28
in any restaurant that pretends to look into your eyes, but it's not looking and it's not seeing anything. And this is my issue that Sophia does not see for fear does not understand might be at some point, if she will, there is no reason why it will not at some point, but I am not interested in the kind of product that Sophia currently is. I'm not interested in the simulacrum and is actively responses. It's basically

Unknown 48:59
I mean, I in that i i agree also, I mean, I am not the I'm not a target market for novaCare robot.

Joscha Bach 49:10
Basically the idea that I spent my last year is in a nursing home. But now I'm where I'm surrounded by Sofia's is as close as you can get me to my imagination of health. It's a really nice thing that I get

Unknown 49:31
your trustworthiness.

Joscha Bach 49:32
So basically you are not the guy.

Unknown 49:37
Yeah, man, you will have to try that hard. Come on.

Unknown 49:45
Yes, Monica, go for it.

Unknown 49:47
So the thing I don't like about GPT three and systems of that ilk and to some extent Sofia is that they are basically examples of systems that are conflict confabulating stuff. They're just making stuff out of thin air. like any human would, but we would rather have them be able to, for instance, read something, create a brain state and then dump that brain state back to us. And that would be the basis for a machine that does summarization and summarization you can sell. So that's basically that's where GPT three and others are failing is because they are not. At least not when I last looked at them, we're capable of generating actual explanations of anything, it's just contamination is making things up,

Unknown 50:31
right. So they're generating explanations that I'd know appealed to people to make to lose. But they're not true explanations. Like if we wanted to use AI for science, and we threw GTP GPT, three or four at it, maybe it would be maybe it would struggle, because I mean, we're trying to test novel new things, not regurgitate pre post modernist text, basically, made fun of existing text.

Unknown 51:02
My view of helping science out is basically to create a system that can classify messages such as emails or chat messages from scientists to other scientists and route them to the scientist that needs to see it. This is basically AI based routing, or machine learning based routing of messages is what I my social media going to be aiming to do.

Unknown 51:25
Okay, well, that's interesting. But that doesn't require the AI to actually understand the contents. Correct. It only

Unknown 51:30
records he understands the language, like if you were a if you're, if you're a particular warehouse owner, and you want your 12 year old grandson to learn the business, you get them in and do inventory, and you basically see how they're doing it to make sure they're doing it right. The level of understanding that the kid needs to use due to be for instance, be able to sort resumes or route messages in any other way, is miniscule compared to the world knowledge that they need to have. So I can do very likely to do routing systems of messages are very effective with very little knowledge going out beyond language. Anyway.

Unknown 52:17
That That sounds quite plausible to me. I mean, one could perhaps come up with technical domains that would be counter examples with them. That seems seems very plausible for for almost all, almost so almost all applications. I mean, I think, Adam regarding trustworthy AI. And, in general, thinking of soup of human level AGI, or super AGI, right, I think there's going to be a couple of phases like there's going to be a phase one, we can build tools to verify the trustworthiness of the AGI mean, we can look at the code and understand it, we can run batteries of of tests on it, we can do formal, formal verification on code, we can

Unknown 53:16
when you read formally, I mean, being able to make sense of the models that are produces

Unknown 53:22
as as well. And you can run from verification on models and uncover them and you can you can ask for, you can formally define properties that you want. And you can run the theorem prover, just to show that the model will possess those properties under various probabilistic assumptions about the data that comes in, and so on. So, I think and this is going to be quite important, right? So to I mean, we want to be able to formally verify things about how algorithms and models will build will work. And we want to sort of codify our assumptions about the world that AI is interacting with, and then be able to formally demonstrate propositions about what properties that I will demonstrate conditional on those assumptions about the world. Right. And I mean, this is hard research, but it is, these are viable, viable areas of research, which have a lot of overlap with a lot a lot of overlap with the tasks of AGI itself, right with with proving other things about programs and complex systems, but there's a there will be that phase, and then there will be a phase when it just gets too complex for humans humans to deal with. Right. So part of the question for us going forward will be what, what level of trust we need to have building up from this phase when it's tractable to assess trust, right. What what level of trust we need to have to know Only pull the trigger and AGI then evolve to the level where we know that it's just doing stuff far beyond our level of complexity and we verify the trust.

Unknown 55:15
For the AI in order for the AI to beat faster, we must be able to make intelligible, it's whatever it's doing, right. So wherever we go go in and understand this code completely

Unknown 55:30
false and ridiculous. Yeah. I mean, I, I mean, I can't make intelligent what I'm doing to my to my dog, but I'm so reliable to him, I feed him a lot of money that he correctly trusts me, right? But I can't make everything I'm doing intelligible to him. Because, I mean, if, if there was a, you know, a foreign invasion coming in the city I live in, and then dog was eating or meeting or doing something and like, I'm going to pick him up and drag him away to avoid being blown up, I can't explain to him, right, he doesn't he doesn't under he doesn't. He doesn't understand. Right. So, I mean, that's, eventually if we're gonna have a singularity will have NGOs that are doing things and thinking things over complexity far beyond human comprehension. And some of these things cannot be made intelligible to humans, right. So I mean, that's that nature of this,

Unknown 56:28
I get it. But I imagined, if an AI was smart enough, it might be able to provide representations that are intelligible for us to make better, more informed decisions about what what it what it's allowed,

Unknown 56:45
I mean, by, by accounting argument, it would seem naively, that can't always be true, just because the the number of things humans can understand is small relative to the number of things that massively superhuman super mind can can think. I mean, that's, it's not it's not fully rigorous, because the distribution of things you can happen in our physical universe is perhaps constrained in ways that we don't fully understand. But it seems, seems like that's probably not true by all the examples we have, like I, I mean, I can explain many, many things, I can several find a way to explain to my four year old son, some things that are important, and in my eye, I can't really accept to a very rough, rough, approximate level. So I mean, that's the best analogies we have would suggest there are limits to that process, as with the most simple quantitative analysis of the situation.

Unknown 57:42
Yeah, yeah, there's gonna be limits to what humans in their current form can understand. But what about, like groups of humans? I mean, if we, if an AI were to be able to

Unknown 57:56
a trillion times smarter than us, I mean, yeah, you know, I'm that whole group of monkeys can and just cannot understand the theory of polynomial functors, no matter how many monkeys.

Unknown 58:09
So I mean, that it may, we may not be able to understand what an AI like a trillion times smarter than us will be able to do. But at least we'll be able to. With AGI may be able to scale much further up to up the singularities scale. Before we start on

Unknown 58:31
the phone, you can understand what to do if you upgraded your brain enough, but then you're not human anymore. And superadded can't explain it to the to the good old dumb monkey.

Unknown 58:42
We just have to consider everything explained at one one level down.

Joscha Bach 58:49
Is it conceivable that everything important with respect to ethics and so on can be understood? It seems to me that there's a pretty good chance that you're able to retrace degeneration of our existence up from the Williard from first principles. And if there are gaps in this understanding the AI can probably explain it to us suspect that there is tremendous level to which we could be able to understand reality. But getting back to this original question a little bit. My issue with stupid history is not that it is just confabulating. The issue is that its configurations. Oh, my mind is also confabulating minute explains reality at every step, right my perception. In many ways. It's confabulation, but it works because it's entangled with reality at every moment, it's being led back to whether it's able to predict the patterns that emerge. And the reason why we have not been able to build a dictionary that contains all of the English language and its grammatical words, is because the English language is changing all the time. It's an Open University. on which the systems are dynamic in which they are evolving. So every AI that you're going to build will have to be entangled with the universe, maybe not necessarily in real time all the time, they certainly not as a human like body necessarily. But without feedback from the world AI is not going to be in the same universe as as even if he updated quite often, it needs to be some connection. The other thing is, if it is meant to be ethical, then there needs to be a source for its motive. So it can generate aesthetics over its preferences. And either you want to generate aesthetics from first principles, in which case, you probably need to fit in something like you want to have a universe that maximizes complexity of life or something like that, or you will set ordinated aesthetics to something else, like human hivemind, or some kind of evolving specification that is put in by the human government, say corpsman in the chat as about the difference between anthropomorphic and non anthropomorphic aesthetics, and ethics. So the question is, do we want the world that is CO created with AI to preserve humanity as it is right now? Or are we willing to keep it open? And to see what's going to happen? Or are we going to go transhumanists right away. Right when we go to Mars, Vitor probably don't want our children to look like us. Because we are very unsuitable for living like Mars, we want to genetically engineer our children until they're fit to live on Mars. So we want them to be hard against radiation, we want them to be able to hibernate, we want them to be very smart and never get bored in space. And they should be able to feed on the only available protein which is in space, which is people, so they probably should look like the aliens, right? It's very counterintuitive, when you think about the Atlantic alien from the Giga alien and the Alien movie. But this is probably ideal form that our children could take, if we want to set on Mars, to our to have this freedom to be give this a lot of freedom to evolve and innovate once that life ones or to be subordinated to a certain notion of aesthetics in the 21st century, to be want to preserve our own society, we want to preserve the status quo. It's a very important difficult question, what kind of world do we want to preserve? How do we want to keep it, but at the invariances that we care about?

Unknown 1:02:25
way? In a way, it's a question that feels important in a way it's irrelevant question because human humanity is never preserved the status quo with with the bulk of its energy and population, I mean, Australian Aboriginal populations, of course, preserve the status quo for 50 or 60,000 years, but then that's part of why they didn't emerge as the dominant global populations, but having a great depth of knowledge and culture and some of the some of these strains. Think I think that we're clearly not focused on preserving the status quo now, even if even if you've made in some ways delude yourself about that, it's clearly not in practice, acting in a way that's that's gonna, there's gonna preserve the status quo that that's, that's

Joscha Bach 1:03:24
no, but humanity itself is not an agent, it's not coherent. Right? Things are going to happen to humanity. The question is, what do you and me do to humanity? What is the system that I am building going to talk to humanity? So what I want to build into my system,

Unknown 1:03:41
I attribute less causal importance to the human deliberative illusion of freewill. And then then the new. This is a big digression, I guess as to whether a human really has a lot more agency than a corporation or a nation state or something. But it's not. It's not so obvious to me. Although we do I also

Joscha Bach 1:04:06
have no disagreement with this. I don't think that we have necessarily a difference there.

Unknown 1:04:12
Yeah, but in any case, seems like in the next decades, let's say preserving the status quo is not the, the vibe of what of what humanity seems to be doing and whether some people some people clearly talk like they feel like we shouldn't be I mean, preserving the good old family values and rolling back to the imagined us in the 1950s is a theme in the Republican Party right but I mean, it's, it's not it's not really what any major chunk of resources is working toward and the on the planet now so to me, it's question is more Still a species of that question, I think is still important, which is as, as we move forward into a quite different future. I mean, which, which properties of the status quo? Do we consider it critical to preserve, right and different people will have very, very different answers on that. I mean, to me, I'm not sure any properties is, is critical to, to preserve, but I do think it's important that each step from one level to the next to the next to the next each, each step is taken with some sort of consent will agency understanding. But if the step from stage one to two, stage three to four, stage four to five, it's if each step is taken with the consent agency and understanding and changes things a bit. And by the time you get to step 100, there's essentially nothing in common with step one, like there's a historical chain going between them. But there's essentially no important properties in common and I'm, by my own aesthetic, and ethic. I'm not worried about that. But obviously, a large chunk of the human population feels differently. Like if, if we had a future world with no parent child, or no husband, wife relationship, or where people didn't need to work for a living, they would be traumatized and feel like we've lost, we've lost some key property of what it is to be human, which should be preserved. And I mean, that's it won't be a relevant question. I suspect that just the natural forces of evolution, broadly speaking, also rendered that question a bit irrelevant, because the way evolution probably happens is, each step to the next has its own logic and logic and constraints. And we are, we're going to diverge step by step radically from from what we are now and hopefully, at each step, the future evolution thinks it's cool. But I'm well aware, my view on this is probably at odds with that the average global citizen at this point, what,

Unknown 1:07:26
what kind of game condition? Would you like to see humans, post humans and AI inhabit in the future, if you and some of your loved ones, or at least some of your loved ones were to remain human? mean at the moment, like we live in a world, it's very competitive, there's a lot of zero sum stuff going on, you know, people get gained from the get utility from the an action which causes this utility and others, which is something I worry about. So I, you know, I mean, my Utopia would be a world in which all agents wouldn't have to compete to survive.

Unknown 1:08:15
Yeah, I mean, that's, that's a very basic statement, right? I mean, I mean, I think, indeed, a lot of people would come where I am now, it would seem like getting getting rid of physical disease and sickness except for those demented enough to choose it getting rid of mental illness except for those demented enough to, to willfully choose it and then having getting rid of death, having, you know, molecular assemblers or whatever that will print out. Whatever ordinary objects of matter you want this this Listen

Unknown 1:08:53
to me a luxury space communism. Yeah, this would be a very, very

Unknown 1:08:57
pleasant situation to live in enough physical space. You can just go with any number of like minded individuals and create a little tribe and 3d print what you want and have a good time. I mean, that question would be after learning all known languages, learn to play play. Every instrument mastering all traditions of wisdom, wisdom and spirituality like writing a few works of literature learning all math invented by other humans is like after, after a few 100,000 years, do you just get bored and ritually Immolate yourself for something or finally decided to just merge your human shard and with the transhuman shard, Devi that mind uploaded? You know, 100,000 years ago when the singularity hub search and these are answers of these are answers of post human psychology. We don't really need the answer right now.

Unknown 1:09:57
I think, but I still think the The basic questions of game conditions, wood is still relevant. You know, getting from here to there, there's certain game theories that are going to be more likely to

Unknown 1:10:11
go from here to there is a whole different thing, right? I mean, having having a society of abundance will radically alter human endeavor individual and group psychology in my view for the better and will lead to all sorts of amazing new things. Getting and presumably game theory is currently conceive Linda, pretty irrelevant to that sort of condition, which is very different to what we've seen so far. How how to get from here to there is a much much much thrown, you're in more annoying problem, right? Like I keep asking Google give universal basic income to the average citizens of the Congo, right? I mean, you, you've got the world with ridiculous, military conflagrations with half of the children in Ethiopia brain stunted due to malnutrition. I mean, you've got radical inequality on so many different levels and radical, ethical self delusion among most most, everybody. And then how do we? How do we get from this to a singularity without without having either World War Three or massive amounts of cyber terrorism or without having a world where, you know, 100,000 elite people own all the robot factories, and everyone else is subsistence farming and the garbage for them by the factories? Right? I mean, that's it. How how to navigate through all that, in the minimally terrifying way is a serious problem. And I mean, that that's certainly some sort of game theory could could be relevant there. But we're, we're not primarily dealing with with rational actors, right. It's a very complex system science sort of game theory that's right over there, which is not currently well understood. But you're also probably is his own view that, Monica,

Joscha Bach 1:12:10
I think I need to drop. It's been a long day, and need to attend to local affairs a little bit before the next day starts. And I thank you very much for hosting us. And, Adam, thank you very much for inviting me to this event. I very much enjoyed getting to know Monica's ideas and thoughts and was very glad to pump into Ben even though it was only towards the end. And I hope to see you guys again soon.

Unknown 1:12:41
Thanks, Joshua. How about you, Hugo, what have you got to say?

Unknown 1:12:48
I've not seen your face for a while you go that we've been we've been back and forth. And even

Unknown 1:12:56
your your image position just changed. You went from upper left to cover right. I don't know why they haven't. Yeah, Ben. Yeah. Nice to see. Hey, when you said your son was four years old at that, time has flown?

Unknown 1:13:16
I've never met. Yeah, yeah, we've got a one year old who you've never met. So I'm continuing photos. Yeah, continuing to produce human offspring at a faster rate than human level AGI is but of course, once you get the human level AGI you can include in though I'll paste the number of humans very rapidly.

Unknown 1:13:40
Is there going to be a third one?

Unknown 1:13:44
will know that depends on how quickly the singularity comes.

Unknown 1:13:51
Do you want to have one after the singularity?

Unknown 1:13:56
It becomes a different thing. Right? Yeah, you have genetically evolving populations of humans slash transhuman minds. But yeah, this gets down to what you're talking about before. I mean, you can keep a version of Adam, in roughly human form, to sort of complete complete the journey of becoming an optimal human. And you can lead a version of Adam mindup load with the superhuman God minds. So whether the version of Adam that remains human like, wants to produce more offspring is I mean, that's not that clear, right? We don't we don't know what will be the psychology of reproducing once death is no longer a thing. It'd be interesting to explore.

Unknown 1:14:40
I guess if there was a complete abundance, and there wasn't a lack of resources and you know, plenty of time. I'd say I'd probably give it a go. It's just yeah, they take a lot of time and energy. Right shoulder. That's why I don't Yeah,

Unknown 1:15:03
Any governments anywhere in the world approach to? I mean, is there any sort of governmental interest in the tropics? We're all putting? Well, this group is talking about?

Unknown 1:15:16
Well, there is but what, what is happening, which is interesting is that governments are approaching AGI largely through corporations with cross government alliances. I know you go when you and I were working together in China, we were thinking, what would that be like it? China AGI agency competing with the US AGI agency or Russia AGI agency? And I mean, it's not quite like that. But on the other hand, I mean, you can Google Eric Schmidt and NSA, you can see that there's connections between us big tech companies and US government various levels. Obviously, Chinese government does a huge amount of advanced AI work by the teams and Tencent, Baidu and so forth. I mean, before, before things with Russia started going, going even stranger. I mean, I met with the Fairbanks AI team in Moscow who was reasonably sophisticated in their, you know, they're well aware of work on AGI and interested in so I think that governments are very interested, they've realized that this is technology that requires cutting edge expertise, which is not what government agencies are best at. Pulling in so they're more aligning themselves with, with tech companies, which has various ethical pluses and minuses. I think and but you could see as, as the world reverts into greater tribalism in terms of like, us in Western Europe versus China versus Russia becoming more partitioned rather than more unified, at least temporarily, for the next immediate phase, that corporate slash government, AI alliances in each of these blocks become more distinct. But you would I wouldn't say the focus in all these cases, is on optimizing narrow AI for immediate utilization rather than on AGI like if, if you look at the big tech companies in either China, Russia or us, what you see is the group was most closely aligned with government or not the AGI r&d groups. I mean, I mean, the group is doing pretty specific, sort of data analytics related stuff. So I think that the short termism of most governments, combined with the desire of governments to command and control rather than trade things around predictably slip out of their fingers seem to be keeping AGI on the sidelines of major governmental initiatives now, which has a an anarcho libertarian by nature, I tend to think is, it's probably for the best and it leaves. It leaves an opening for AGI to be rolled out more like Linux or the internet sort of in a in a more global and fundamentally decentralized way. At least that is, is my own take on it. There may be opposing views in the in the room.

Unknown 1:18:31
I don't mean so much on the Yeah, community government on the commercial side. I'm thinking more on the AI threat side. Are you getting any feelers?

Unknown 1:18:47
I think the the AI threat, interest of government is really more on the on the practical side of man. You'll get a lot of feelers on that but people are concerned about drones and autonomous weapons. People are very concerned about bias in AI models of various sorts. And I feel like around the time of Nick Bostrom, his book super intelligence there was a flare up of explicit concern about Super AI killing everybody. And people still understand those same issues. But the focus of attention for those concerned about ethics of AI seems to have gone to like drones murdering the wrong people or AI is discriminating against people in ways that aren't realized that attention is shifted a little bit more to to more immediate, practical aspects

Unknown 1:19:53
of government worry about AI. AI is taking over hacking, duties or creed In a whole heap of fake news, text generation is being used for malicious purposes.

Unknown 1:20:08
Yeah, yeah. Absolutely. And there's also government interest in using text generation for malicious.

Unknown 1:20:16
Yes. And I'm

Unknown 1:20:18
sure activities in that regard from all over the place.

Unknown 1:20:23
Those apps don't require AI in mind.

Unknown 1:20:27
Well, for that transforming neural nets are heavily used for that works better than Markov models.

Unknown 1:20:35
Now, you just templates, they're not gonna compare from person to person. Mail templates. Yeah, well,

Unknown 1:20:43
you can use templates. I mean, transformers are being used for that heavily. No. I mean, but it's, anyway, it's it's very practical, near term concerns compared to the higher end ethics concerns that both from wrote about, Hugo wrote about in his previous books.

Unknown 1:21:09
I mean, it's pretty like to generate images. It's been pretty interesting to watch.

Unknown 1:21:16
Why is that a weird impact on global discourse? Right. So like, in, in Russia now, in Russia now, where many people are fed, sort of fake news about what's happening in the conflict with Ukraine? If you show them real videos of people been tortured in Ukraine, they're just like, Well, how do I know that's deep fake otherwise? Right? So I mean, the ease of generating stuff with no technology has created a situation where people they know they can't tell illusion from reality. So in many cases, that can make people easier to to brainwash, and certainly make them more, make them to a greater extent just fall into into mental ruts. Because if you throw them something that will jump them out of their way of thinking, they'll just say, well, there might be a fee right now that, yeah, that that problem can be solved by various sorts of authentication technology. But what's interesting is the lack of motivation to solve it, right? Because the powers that be May, they want people to be able to tell reality from from illusion, because it makes it easy, it makes it easier to program so so the rollout of technologies to validate truth is not as aggressive as this as it could or should be.

Unknown 1:22:41
So they, it sounds to me, although I don't haven't really researched this myself that you'd be able to tell the difference between an image by looking at the actual structure that of a,

Unknown 1:22:53
you know, not very well, to a

Unknown 1:22:56
real person. Whether it's a real photo,

Unknown 1:23:02
it's gotten beyond that. That was the case three years ago. It's not, it's there's been a whole spy versus spy thing of adversarially, trying to generate models that elude the models that try to distinguish real from fake and so forth, I mean, that the correct solution is to use blockchain so that like when I take a picture with my phone, I authenticate with with my biometrics and my own private information, it's instant with a hash code saying like Ben verifies this picture was taken on this hardware device at that this place in time, and then that, that, that hash code is put as a watermark into the image, right? And so then then, then it can then it can be validated back back back to me. So I mean, they're, they're pretty clear blockchain based solutions to authenticating images and texts and so forth. And I mean, these have been presented Interpol these are well understood, but they're rolled out very slowly, because this is whose parody is it? I mean, who who benefits more forever being able to do what's accurate versus from being able to make people think whenever you can pay to make them thing. So, I mean, this, this is more about trustworthiness of information dissemination, as modulated by narrow AI among a lot of other technologies all right, but it is, it is an interesting fact that I mean, if, if the powers that be, are more interested in deception than trustworthiness, even at the current level with with narrow, narrow AI around with law with other non AI tools, like when you get into an AGI if it's in the hands of the same powers that be how much will they care about it being trustworthy versus care about being useful as a tool for them to increase their power by deceiving people right. So this, this again, leads back to my thinking that we better have AGI be like Linux or the internet rather than be under centralized government slash corporate control. Because the combination of government and corporate control I mean is, is oriented other things rather than trustworthiness.

Unknown 1:25:11
Can you use? Are you still keeping tabs on what's happening in AI research in China? Like, are they rapidly catching up to the US? So what's your general feeling?

Unknown 1:25:24
I mean, in No, I absolutely. And there's more and more creative and original stuff coming out of China all the time. I mean, I think that the vibe, there used to be the China copied stuff and scaled it up. Whereas the West came up with original ideas. I think that's much less true in the last five, five years or so there's more creative, original innovation coming up, perhaps as to the generational shifts in China. On the other hand, for AGI in particular, there's much less support among Chinese institutions for general intelligence r&d, than there is in the West select that people aren't doing in Beijing involved with AGI they just start a journal called the Journal of intelligence science, because general AI is not as popular there with funding sources. So I think I think there's more worry in China about AI is that you might not be able to control what they do. And AGI, by its nature, it's gonna be harder to control what it can do. Whereas in America, we're a lot crazier. And I mean, what we're, we're more willing to entertain research on things where we may not be able to control what happens once we've launched onto the world. So I think that there is a fundamental disharmony between a governmental system is focused on command and control. And the creation of human level AGI which anyone with any sense can see is going to be harder to command and control. So I think for that, that's my very, very crude, high level reader. The reason why you're seeing a bunch of AGI r&d In the West, even though a narrow AI China is picking up incredibly well.

Unknown 1:27:29
Kazakhstan come through

Unknown 1:27:32
Kazakhstan is is in a screwy situation, here. They they have not really followed through on their their attempts to do advanced science, probably because oil prices have not. I mean, you go, how has your own thinking about the future of AGI and sort of the future of global human organization? And politics is related to it? Yeah. How was? How has that evolved? Since you wrote your books on these topics?

Unknown 1:28:07
Well, to be honest, after I retired, well, particularly, the last what, three, four years I've been in Australia, I split my mental effort into two one is largely pure math. And you can see that so I'm multitasking. So I'm meditating volume nine, which I believe is particular for you. Personally, you notice then that for the rest, I believe this particular theorem, which is known as the gigantic theorem in pure mathematics, is literally humanity's greatest intellectual achievement. So in other words, what I'm trying to say is, I haven't given thought to this.

Unknown 1:28:58
Yeah, well, the world the world AI has changed a lot the world The world has changed a lot. So yeah, I'd be I would be, I'd be curious how your perspective was updated after a few months of study and thought, but maybe we save that for Adams next conference. And

Unknown 1:29:14
well, this just these last two days are actually two nights that have been illuminating for me and sort of listen to some speakers and they make me feel incredibly ignorant. Definitely fallen behind. But that's, that's what happens if you sort of branch in a different direction, I guess.

Unknown 1:29:32
Yeah, I was. I was thinking about your old work on FPGAs lately, because one of the things I'm doing is designing, designing an AGI chip to massively speed up opencog Hyper owns sort of meta graph, pattern matching and so just just working with FPGA technology for prototyping chip designs, you see like how, how far the Xilinx FPGAs have come since you were working with before like, there wasn't much on aboard RAM on FPGAs back then and that now there's huge amounts. Right? So I mean, there's, we see improvements in so many different supporting technologies and the world has changed a lot since since you did all your pioneering technical work as well as your futures books. But, yeah, we started a few years for the singularity so that there's time for you to jump back into

Unknown 1:30:25
speaking of hardware, can you help Monica? Oh, sorry, can you sort of give a one minute Spiel to begin with your your hardware money problem.

Unknown 1:30:42
All the work that I've done so far has been done on a Macintosh, late 2013, I can as I call it, the cam for Macintosh. That's basically that's since 2013. That's what nine years of computing or something like that. And today, what I really need to move basically, the language is language competences that I'm creating, in order to move them beyond this simple proof of concept stage, I need to basically learn a lot more language. And the only constraint I have is actually RAM. I mean, I don't use GPUs at all, which means the DRAM is my only constraint. But I want to keep everything in RAM. So a three terabyte machine from Super micro with 220 cores cost about $1,000. And that's my next target machine. Of course, by the time I get the money together, they will have a new one, that's going to be even better. So but hey, it's that's the game I'm playing.

Unknown 1:31:37
Yeah. We build our own server farm combining CPUs and GPUs to run our own systems. I think if you we've managed to do hopefully maximize processing and minimize costs. But this this is, this is an issue that ties back to the corporate dominance of of no AI and AGI r&d that I mentioned, like I mean, 10s of millions of dollars to train all these transformer, the neural net models, right, so while the work we're doing because of its more intelligent design will be less wasteful of processor power than something like training a transformer. Even if you're less wasteful of processing power by having better algorithms, you're, you're still right, you're still using a lot of processing power, right, and you still need a lot of a lot of memory. And yeah, that's, that's becoming a large advantage that big tech guys. And this is part of why they're able to open source all their AI algorithms, because they know that where the magic happens when the algorithms meet the data and the processing power, they have the day to day the processing power. So they open source the algorithms together and improve their algorithms for free. But then they're the only ones that can put them to any use, right. And so that's, that's a big issue, which I mean, in the short run, I'm just taking a traditional approach to that and pretty much spending money to buy processors and build custom server farms for projects use. But I mean, this, we have, in the singularity net ecosystem, we have a project called new net new, which is, is an infrastructure for marshaling together, processing resources from from all over the place, instead of protein folding and homestyle, to pull them pull them together for use in this, this, this can be a route to make processing power available more, more cheaply to researchers aren't in big tech companies. But that project is just getting started isn't isn't. It's not up to the chassis you have yet. We're still only useful for simpler things at this moment, maybe in a year from now. But yeah, it's a quite frustrating issue, indeed. And I, I mean, while we have our own server farm, which is okay, I still faced the same issue. I mean, if I had, if I had a half billion dollars a year to spend on having opencog systems learn things, you know, how much further how much further along would be, we'd be right. But, of course, I could have gone to work for a big tech company and got all that processing power, but then then they would own exactly not just the code ideas, but they would own the team that I that I that I brought in and I don't I don't want things to become centralized. To that extent it's worth it's worth eating a small delay in my work to return the chain freedom.

Unknown 1:34:51
Thank you, everybody for coming along. And thanks hates Ben and Monica and Hugo as well. So just jumping on the panel there at the end. So good. Yeah. So I haven't spent much time outside. As I said to you before, Ben, I just got covered. But I think I'm on here. I think I'm on the recovery. I'm going to do a rat test today. And I'm hoping that if it that I'll be negative. I should be. Yeah, I'm pretty positive, that would be a negative.

Unknown 1:35:24
Yeah, well, I mean, here in the US, essentially, everyone says, Zed COVID. By knowing you sort of a freak, if you haven't had it yet. I haven't

Unknown 1:35:33
really knocked me around, because I like, usually pretty healthy person. But this one knocked me around. But it wasn't just

Unknown 1:35:41
I got COVID After every three vaccinations it was I was essentially, essentially asymptomatic, which was fortunate, I can tell you this. My, my, I mean, it's terrible. What COVID has done to people who, who got severe infections, especially the elderly or immunocompromised. But my, my, my worry about that is more seeing how badly the global political system dealt with this relatively mild pandemic, like what I mean, the gentleman that vaccines was fast, it was sort of the bright spot, but what will how will the world react if there's a really bad pandemic, like five, five years from now, right? So this is, what if we get something as infectious as Omicron. But as as deadly as SARS was, or something, whether it's human engineered, or just spontaneous mutation based or say, right. I mean, if we don't so badly, with a relatively mild pandemic, we'll be doing something much more serious happens, which is just just one more motive to create the singularity so that we have the the robotic, helping hand superhuman AGI to help us out before something like that comes about.

Unknown 1:37:05
Alright, thanks, everybody, for coming along.

Unknown 1:37:07
Yeah, thanks. Thanks for organizing organizing this event in spite of your your temporary medical condition. I think it's it's it's good fun to keep revisiting these issues. And the nuances are a little different each time as, as technology advances in culture evolves. So that's, it's kind of cool to see how the nature the discourse changes over the years.

Unknown 1:37:30
Okay. Yeah, true. True that I've seen back in Australia from time to events.

Unknown 1:37:37
Oh, am I allowed in?

Unknown 1:37:39
Yep. I Yeah.

Unknown 1:37:44
Cool. Well, we'll talk about that I'm eager, eager to get back as well. Actually. It's been it's been a while I've been mostly in the US. So that's two and a half years since COVID. broke out, but the there's been a few voices out. So yeah, we should definitely do one of these things. Face to face next year something.

Unknown 1:38:10
Yeah. Cool, man. I look forward to it. I'll speak to you about it offline. Anyway.

This transcript was generated by https://otter.ai