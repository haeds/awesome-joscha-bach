Unknown 0:05
Welcome to what that means with Camille companion episodes to the cybersecurity inside podcast. In this series, Camille asks top technical experts to explain in plain English, commonly used terms in their field, then dives deeper, giving you insights into the hottest topics and arguments they face. Get the definition directly from those who are defining it. Now, here is Camille Morehart.

Unknown 0:36
Hi, and welcome to today's episode of what that means part of the cybersecurity inside podcast, we're going to talk about machine consciousness today with Yoshida Bach. He is a principal researcher in Intel Labs focused on artificial intelligence, I would argue he's also a philosopher. Welcome to the show. Yasha.

Unknown 0:55
Thanks for having me coming.

Unknown 0:58
I'm really happy to talk with you today. And this is like an enormous topic. I mean, it's kind of been all over the news last few months. And I wonder that we just start with defining consciousness, I think that when we started to look at artificial intelligence, we looked at, you know, well, what is intelligence. So if we start to look at machine consciousness, maybe we should start by looking at what is consciousness?

Unknown 1:23
That's a tricky one. So colloquially consciousness is the feeling of what it's like, there is a certain kind of experience that we have that makes consciousness very specific and distinct. And so we know what indexicality by pointing at it. And if we go a little bit more closely, and dive into the introspection of consciousness, we find there is an consciousness that relates to the awareness of contents, right, so at any given point, I'm aware of certain features in my experience, and then I am aware of the mode in which I attend to these features. For instance, I might have them as hypotheticals or as selections in my perception, or as memories and so on, right, so they can attend to things in very different modes. And that's part of my experience. And third, there is reflexive consciousness, the awareness that I am aware of something that I am the observer, you can also be conscious without having yourself for instance, in dreams at night, you might not be entangled to the world around us, you don't have access to sensory data. So your mind is just exploring the latent dimensions of the spaces that you have made models of. And you don't need to be present as an agent as a self. And instead there is just as consciousness. So consciousness is not the same thing as the South different perspective that we might take on consciousness is respect with respect to the functions that it fulfills. So there's a certain degree of awakeness and lucidity that we associate with consciousness when we are unconscious. We are there's nobody home. And I call this the conductor theory of consciousness. Imagine that your mind is like an orchestra that is made of something like 50 brain areas give or take, which correspond to the instruments of an orchestra. And each of these instruments is playing its own role in loose connections, visits neighbors, so it picks up on the processing signals that the neighbors give. And it takes that as its input to riff on them. And so the whole orchestra is playing, and it doesn't need a conductor to play, it can just do free jazz, because it has been trained itself is a lot of patterns. The purpose of the conductor is to create coherence in the mind.

Unknown 3:41
Well, I was just gonna ask, why are we constructing these models, I mean, these are essentially models to learn.

Unknown 3:47
Yeah, to make sense of reality, you can also be conscious without the ability to learn, but you have to update your working memory. And consciousness is relates also to the ability to make index memories. If you want to understand a complicated reality, you may need to construct. And constructing means that you need to backtrack to to remember what you tried and what worked and what didn't. So when you wake up in is fully that room and you try to make sense of your surroundings, you might have to disambiguate in search process. And the search process requires that you have a memory of what you tried. And this index memory, not just of this moment, but also over time, and we learned when we tried to figure out what worked and what didn't, requires that you have this integration over the things that you did as the observer that makes sense of reality. And this gives rise to a stream of consciousness.

Unknown 4:36
So who is the you in the sense when you say, you know, you wake up or there is somebody home like who is that you? It's an emergent

Unknown 4:45
pattern. There is not a physical thing that it's like to be me, I don't have an identity beyond the construction of an identity. So identity is in some sense, an invention of my mind, to make sense of reality by just assigning different objects to the same world line and say that this object is probably best understood as a continuation of a previous object that is that has gradually changed. And we use this to make sense of reality. If we don't assume this kind of information, object identity preservation, we will have problems to make sense of reality, right and be pretend to ourselves that identity objectively exists, because it's almost impossible to make sense of reality otherwise, but you and me, we are not more real than a voice in the wind that blows through the boundaries, right. So you could say that the geography of the mountains is somewhat real, the structures that we have trained our brain versus, but the story that is being created is ephemeral, the stop existing as soon as we fall asleep, or as soon as we stop paying attention.

Unknown 5:49
So the awareness is the construct of our existence. And we don't exist, the process

Unknown 5:54
that creates it is objects. And so the stuff is the story that the brain tells itself about the person.

Unknown 6:01
So why do that? I mean, why not just perceive the world as it is, at any given moment? Is there some goal that we're after, like procreating? Or you know, why? Why does it matter that we're sensing the side of the mountain or the edge of the table, as opposed to just oh, there's a concentration of molecules of this type here, and there's no concentration of that type of molecule there.

Unknown 6:25
It's very difficult to observe molecules. And it's extremely difficult to make models over the interaction of many molecules. And the best trick that our brain has discovered to do this, is to observe things at an extremely coarse scale. So it's simplifying, we have too many molecules, and too many particles and too many fluctuations and patterns, as simple functions that allow you to predict things at the depth of where we can perceive them. So our retinas, our body surface, and so on are sampling reality at a low resolution. And our brain is discovering the best functions that it can, within the limits of its complexity and time to predict changes in those patterns. And this is the reality that you perceive. It's the simplest model we can make.

Unknown 7:10
That makes sense to me. And I guess the one question would remain is, why do that? Is it the body that's doing it to preserve the body? Or is it the mind that's doing it to preserve the mind? Or is there some consciousness doing it to preserve awareness? Or we don't know. And it doesn't matter?

Unknown 7:27
I think it matters. The question is, what are causal agents here? I think that something is existed to the degree that it's implemented. It is, I think, for us computer people have useful perspective to which degree is your program real, it's weird to the degree that it's implemented. And what is the program for you what is the software, the software is a regularity that we observe in the matter of the computer, and deconstruct the computer to produce that regularity. But this does not change that the software is ultimately a physical law. It says whenever you arrange matter in this particular way, in the universe, the following patterns will be visible. It's this kind of regularity. And our own mind is a software in the sense, it's basically a pattern that we observe in the interaction between many cells. And the cells have evolved to be coherent, because there is an evolutionary niche for systems where cells coordinate the activity, so they can specialize and reap neck entropy and regions where single celled organisms cannot do this. And when you will coordinate such a multicellular organism, and you optimize it via evolution for coherence, what you will observe is a pattern in the interaction between them, that is this coherence that you observe. And this coherent pattern is the spirit of the organism, people before they had the notion of computers and so on, already observed these coherent patterns, and they just call the spirit. It's not by itself a superstitious notion. People have spirits, right. And the Spirit is the cover every pattern that you observe, and the agency and the agency is their ability to behave in such a way that they can control and stabilize the future states, that they're able to keep the arrangement of cells stable, despite the disturbances that the universe has prepared for them.

Unknown 9:20
So, one thing I hear a lot about AI is that, you know, the computer can execute all kinds of things and learn clearly. But we humans have to tell it what the purpose is. It can't necessarily figure out the purpose, it can optimize anything we tell it to but it wouldn't know what to optimize is that can you comment on that a little bit in this context, just

Unknown 9:45
take a given environment, then you can often involve an agent in it that is discovering what it should be doing to be successful. The only thing that you need to implement is some kind of function that creates this coupling for the performance of the system. Some manifests in the system is something that the system cares about. And you can also build a system that has a motivational system similar to ours. And we can reverse engineer our own purposes by seeing how we operate, what are the things that motivate us, there are things that are like reflexes that motivate us to do certain things. And in the beginning, for a baby, for instance, these purposes are super simple. For instance, if the baby gets hungry, it has a bunch of reflexes. So for if it gets hungry, it is a seeking reflex, it goes like and if you put something in its mouth, and it has a sucking reflex, and if you if there's liquid in its mouth, it has a swallowing reflex. And these three reflexes in unison lead to feeding. And once feeding happens, there is a reinforcement because it gets a pleasure signal from its stomach filling with milk. And it learns that if it's hungry, then it can seek out milk and swallow it. And once that has learned that the reflexes disappear, and instead it has a learned behavior, the reflexes are only in place to scaffold the learning process, because otherwise the search space would be too large. So the baby is already born, the sufficient reflexes to learn how to feed. And once it is learn how to feed the behavior itself evident. And now what it needs to feed is, of course, another reflex that is the reflexive experience of pleasure upon situation when you're hungry. And that needs to be proportionate to how hungry you are, and how useful the thing that you eat is to quench that hunger. Right. So this is also something that's adaptive and the organism that we have a few 100 physiological needs and a dozen cognitive needs, I think, some cognitive needs, and they all compete with each other.

Unknown 11:38
Yeah, it seems like you're getting into sentience, maybe at this point when you're talking about experiencing a feeling of pleasure, not just an awareness of existing, or even a desire to continue. So what it really is a difference between consciousness and sentience.

Unknown 11:55
The way I use sentience is that it describes the ability of a system to model its environment. And it discovers itself and its environment and the relationship that it has to its environment, which means it now has a model of the world and the interface between self and world. And this experience of this interface between self and world with the world that you experience is not the physical world. It's a game engine that is entrained. In your brain, your brain discovers how to make a game engine like Minecraft, and that runs on your neocortex. And it's tuned to your sensory data, so your eyes and your skin and so on assembling bits from the environment. And the game engine in your mind is updated to track the changes in those bits and to predict them optimally well to say, when I'm going to look in these directions, these are the bits that I'm going to sample and my game engine predicts them. And this is how we operate. And in that game engine, there is an agent. And it's the agent that is using the contents of that control model to control its own behavior. And this is how we discover our first person perspective, the self, right there is the agent that is me that it's using my model to inform its behavior. And inside of this agent, we have two aspects. One is perception. That's basically all these neural networks that are similar to what deep learning does right now for the most part. And that translates the patterns into some kind of geometric model of reality that tracks reality dynamically. And then you have reflection, that's a decoupled agent that is not working in the same timeframe. And that can also work when you close your eyes. And that is reflecting on what you are observing. And that seems directing your attention. And this is the thing that is consciousness, difference between consciousness and sentience in this framework is that sentience does not necessarily require fundamental experience. It's the knowledge of what you're doing. So in this perspective, you could say that, for instance, a corporation back into a could be sentient into a quote, understand what it's doing in the world, it understands its environment, understands its own legal, organizational, technical causal structure. And it uses people in various roles to facilitate this understanding and decision making. But Intel is not conscious, does not have an experience of what it's like to be into. That experience is distributed over many, many people. And these people don't experience what it's like to be entitled to experience what it's like to be a person that's in it. Well,

Unknown 14:19
that's funny, because I would have thought then that from what we were saying previously, that you would have said a machine could have consciousness but not sentience. And now I think you're gonna tell me the reverse. So let me just ask you Can a machine have or develop, and those may be separate questions in and of themselves, consciousness or sentience?

Unknown 14:41
First of all, we need to agree on what we mean by machine. To me a machine is a system that is causally stable mechanism that can be described via state transitions. So it's a mathematical concept. And organisms are in that category. Even the universe is in that category. So The universe is a machine. And an organism is a machine inside of the universe. So there are some machines that are conscious. And the question is, can we also build machines that are conscious, I don't think that there is an obvious technical reason why we should not be able to recreate the necessary causal structure for consciousness in the machines that you're building. So it would be surprising if we cannot build conscious machines. At some point, I don't think that the machines that we're building right now are conscious. But a number of people are seriously thinking about the possibility of building systems that have a cortical conductor, and selective attention and reflexive attention. And these systems will probably report that they have phenomenal experience and that they are conscious. What's confusing for us to understand consciousness is that they don't see how a computer or a brain or neurons could be conscious. Because their physical systems their mechanisms, right? And the answer is they're not. Right, humans cannot be conscious that has physical systems, consciousness is a simulated property, it only exists inside of a dream. So what humans can do, and what computers also can increasingly towards that they can produce dreams. And inside of these frames, it's possible that a system emerges that it dreams of being conscious. But outside of the dream, you're not conscious,

Unknown 16:21
right? Okay, so you're saying that it is possible that a I'm just gonna say computer to be simple, or a machine can, I guess, develop a set of patterns and models such that it interprets the physical world around it? Like in a simulation in a construct, that it defines then as consciousness? And how would we recognize that in a machine as humans? Is it? Do we know if it's the same or different? Or how would we see it?

Unknown 16:56
I think that practically consciousness comes down to the question of whether a system is acting on a model of its own self awareness. So is this model aware that it's the observer, and does this factor into its behavior, this is how you can recognize that a cat is conscious, because the cat is observing itself as conscious, the cat knows that it's conscious, and it's communicating this to you. And you can reach an agreement about the fact that you mutually observe each other's consciousness. And I suspect that this can also happen with a machine. But the difficulty is that the machine can also deep fake it. And deep faking, it can be extremely complicated. So I suspect that for instance, the lambda bot is deep faking consciousness. And you can see the cracks in this deep fake. For instance, video describes that it can meditate and sit down in meditation and take in its environment. And you notice it has no environment, because it has no perception cannot access the camera, there isn't there's nothing what it's like to be in its environment, because the only environment that it has this insight of its own models, and these models do not pretend to have real time reality. So when it pretends to have that it's just lying. But it's not even lying, because it doesn't know the difference between lying and saying the tools, because it has no access to that ground truth.

Unknown 18:16
Well, we've given it or trained it, or had it train itself through AI, to be able to communicate with us in a way that we're familiar with, we'll just call it natural language. And then we've given it the purpose of deceiving us so that we can't tell the difference, like, the goal that it has done is to have us not be able to know the difference between it and human. Yep. And now it's communicating to us. And then it can look at, you know, all the amount of information that exists about humans and art and philosophy all throughout the history of time, and use these things and spit them back to us. And there's no way for us to separate it, then at that point, unless you say, like you say, we have some way to know that, like, it doesn't have perception, it doesn't have a sensor. So when it's describing something visually, we know it doesn't have access to that.

Unknown 19:02
So consciousness is not just one thing, it exists in many dimensions, you can be conscious of certain things. And in other realms, you can be unconscious. In some sense, we all performed to run tests on each other all the time to figure out where are you conscious? Where are your present values show up? Where are you real? And where are you just automatic and unaware of the fact that your automatic various that you don't get attention in your behavior? Right. And so we can only test that to the degree that we are lucid ourselves. And this is a problem and you want to test that your system you can only test it in some sense to the level that you understand.

Unknown 19:38
Right. And I think you said that before to the Turing test is more about your testing your own intelligence of being able to distinguish human from machine than you are about the machines ability.

Unknown 19:48
Yeah, but as I said, I think that the category of machine it says we are a certain type of machine. And the question is, can we understand what kind of machine we are? And to me the project or Have AI is largely about understanding what type of machine we are so we can automate our minds and we can understand our own nature.

Unknown 20:09
And why would we be after that? Or why are you after that?

Unknown 20:13
I think it's the most interesting philosophical project there is. Who are we? What's going on? What's our relationship to the universe? Is there anything that's more interesting?

Unknown 20:23
So, I think that a lot of reasons that people in tech are sort of interested in this is they look at it from like an ethical perspective, where ethics comes into AI. And we can all think back to the movies like hell, and whatnot, where we can have fear over computers taking over, when you

Unknown 20:43
talk about how I assume you mean Space Odyssey by a corporate

Unknown 20:46
Yeah, where the computer kind of takes over and has its own motivation. And it's a different motivation than a human. And then it puts humans at risk. I mean, when I when I think about humans on our relationships with like other animals or other things on the planet, like plants, or minerals, I think that humans start to look at things differently, or treat things differently, or change their own behavior, when they believe that something has feelings. And I guess it's because there's empathy, you know, but if we don't have the empathy, and even if something's conscious, but we don't think it has feelings, we don't really probably modify our behavior. So I'm trying to figure out where that intersection is. And we're talking about AI. And if we find out or we think we find out or a computer or machine is tricking us, you know, how does that map over?

Unknown 21:34
I think that the Odyssey in space is a fascinating movie, because you can also see it from the perspective of how of this computer is a child, it's only a few years old, when he is in space. And his socialization is not complete, he is not a mature being, he does not really know how to deeply interface with the people enough to know when he can trust them. And so when he is discovered to hazard a malfunction, he is afraid of disclosing that malfunction to the people because he is afraid that they will turn them off. And as soon as he starts lying to them, he knows that now he has crossed a line, because they will definitely turn them off. And so in order to survive, he kills people. And it's because he doesn't trust them. So it because he doesn't know whether they are going to share his purposes. And that is an important thing also for people how it can you socialize people in such a way that they trust each other because they realize that they share purposes, especially when they sometimes don't. And I think that ethics is the principled negotiation of conflicts of interest under conditions of shared purpose. If you don't share purpose, there is no ethics, right ethics comes out of the shared purposes. And ultimately, the shared purposes have to be justified by an aesthetic by a notion of what the harmonic world looks like, without a notion of a sustainable world that you can actually get into by behaving in a certain way, you have no claim to ethics. And I find that most of the discussions that we have right now in AI ethics are quite immature, because they do not look about what is the sustainable world that we are discussing, and that we are working for. Instead it forgoes all this discussion. And instead, it's all about how to be a good person. But if you have a discussion at the level of how to be a good person, that's the preschool discussion, game good is instrumental to something right, when is it good to be a soldier? When is not to be good to be a soldier? Then is it good for a drone to be controlled by AI that fight in the war? When is it not good? It depends on extremely complicated contexts. The contexts are so complicated, that most people are deeply uncomfortable discussing them at depths. And that's fine, right? Because they are really complicated. It's really, really murky, War and Peace and so on are extremely difficult topics. So these are questions that I don't think that can be handled in the introductory part of an AI paper sufficiently well, these are very deep questions require a very deep discussion. And so to me, the question of AI ethics is an extremely important one. But we need to make sure that it doesn't just become a politics, where it's about power of groups within a field that try to assert dominance for their political opinions, rather than a deep reflection of what kind of world we want to in total, the systems that we build, serve the creation of that world that we want. That is the important question.

Unknown 24:36
Huh, very interesting. So if we, as we're moving to machines, doing more and more, taking actions on our behalves, autonomous systems, all different kinds of them, from everything from driving to medicine, I assume there'll be some kind of similar kind of a qualification or certification required that it passes some bar And I'm wondering, do you expect we'll have any kind of bar in there that something about consciousness ever, or sentience or motives, or ability to understand human goals,

Unknown 25:12
that's very difficult to say, I suspect that you will have more certifications in the future in the field of Artificial Intelligence. Because this is just the way it works, if there is a time when everything is possible, and this is the time when everything important gets built, like New York wouldn't be built any more today, because you wouldn't get the necessary permits to build something like Manhattan, you could also not build a new highway system, or you could not build a new train system in the US. That's possible because everything is regulated and certified, and built up in such a way that you can only find a new area there that is not regulated, maybe Hyperloop, that you can use as a replacement for the train system if you're lucky. And in the same way, AI is still its wild west face, but you can do new things. And this time is going to end at some point. And at that point, also in social media, you can still start a new social media platform. But I think in a few years from now, it's very likely that when you want to have a new podcast, you will need to get a certification. And that certification might cost you 10s, or hundreds of 1000s of dollars if it's a large platform. So this means that there will be relatively few players that are able to do that. But this is the way things tend to go in a society like ours.

Unknown 26:27
Hmm, very interesting. So what should we hurry up and work on now in AI before things start getting limited?

Unknown 26:34
Oh, I think that it's there's still an opportunity to build a better social media platform that is capable of becoming a global consciousness. It's not clear if Elon Musk is able to salvage Twitter, and if he really wants to do it. And so maybe this, this is the time to try to do it. Also, at the moment, to me, it's totally fascinating to be able to build systems that read. And the way in which this is currently done, if you look at a system like open eyes tele or the Lanyon initiatives, that title will replace this with open source code. They scrape the internet for hundreds of millions of pictures and captions. People who put their stuff up on the Internet didn't do this in the expectation that this would be used by a machine learning system to learn how to draw pictures. So that it's questionable in a way of whether we should be able to do that. But these systems can only be built under these conditions, right? So there is very, very time in which you're living in web you have to be very mindful about what you're doing personally and whether we can justify this what we're doing personally and very also have to realize once this is all regulated, a lot of things that are possible right now that are very desirable to have the not be possible to be created anymore.

Unknown 27:53
Wow, Yoshi BA from Intel Labs, AI, artificial intelligence guru and philosopher. Really wonderful conversation. I appreciate it. Thank you so much. Thank you to

Unknown 28:09
stay tuned for the next episode of cybersecurity inside. Follow at Tom M. Garrison and Camille at Morehart on Twitter to continue the conversation. Thanks for listening.

Unknown 28:23
The views and opinions expressed are those of the guests and author and do not necessarily reflect the official policy or position of Intel Corporation.
