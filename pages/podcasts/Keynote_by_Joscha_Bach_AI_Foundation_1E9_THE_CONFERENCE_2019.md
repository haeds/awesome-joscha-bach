Joscha Bach 0:06
Good morning, I joined the field of AI because I want to understand our own nature. I thought that after studying philosophy and psychology and a few other things that we have, can you continue where psychology left off? Or can we test theories that we can implement that we can run in machines. And I've found that by teaching machines, how to think we learn how confused our notions about what thinking is are. And this allows us to make progress. What is intelligence, it's a my view, the purpose of intelligence in a species like us is to create models of the universe, that is of our local neighborhood of the universe to make it predictable. And the idea of data compression is that when you have a bunch of bits, and you find an order in them in such a way that you can predict what the next parents have, which is you only need to encode the difference. And when you minimize the difference, when you find an efficient encoding, you have a model. So data compression and the notion of understanding are intimately related. intelligence is the ability to make models, then intelligence in humans is a multi generational property. Individuals are not that intelligent, they are often smarter than teams, because they're not beholden to the same constraints as a team that means that they have to force a consensus or something. So they often can understand things that a team cannot understand. But across generations, we have a smarter an individual is not smart enough to discover a whole natural language by itself, or to discover the nature of Turing complete languages or to discover mathematics itself or epistemology, of question of what is true, and how we can tell two things from things that are not probably not, or how we can go on a path for truth. These are things that take 1000 years to discover in an unbroken intellectual tradition. So in some sense, what we need to build as a civilization intellect, something that goes way beyond the individual, what does the civilization intellect look like? The individual the mind is the thing that observes the universe, and the neurons and neurotransmitters act as a substrate. And our working memory is the current binding state of that model. And our self is what we think we are what's on the other side of attention, that identification was what we think we should be doing at any moment. And what we think we are stable in this world, and consciousness is the contents of our attention, it makes our knowledge available to our mind. And very similarly to this we have a civilization intellect, our society is the thing that observes the universe, and the people and resources are its substrate. And degeneration is the current binding state of a society and the culture is the identification is what we think and what we want to happen is our self, as a civilization in the media, or the contents of our attention. It makes knowledge available, so our society, and in some sense, I think that our media are a little bit like Twitter, it's like a global brain, completely addled by dopamine, which Mr. X our attention towards engagement, that away from relevance. So it's a problem that we probably need to solve. And maybe there's a coincidence that the end game of social media is going to be a global brain.

Joscha Bach 3:23
So the culture is itself of a civilization and the media is its consciousness. And in some sense, the idea that the construction of a cohesive model of the world, the focus of intellect, is not possible. It's the founding myth of our civilization, this tower which reaches the heavens, and not just the earthly ones, but that is able to basically bridge all the gaps between where we are and what you want to understand that this field is one of the very old stories that we have. And it failed. Because it did not find a common language with the people in the different areas of building the Tower started to speak different languages. And they diverge, they were no longer able to put this thing into cohesive whole. And this whole edifice came tumbling down. And in some sense, you're trying to do this again, right now with artificial intelligence, you're trying to recreate this tower, we are going to create a machine that is going to discover truth civilizational intellect, what is truth, truth is systematic property of a statement. And we require that we have a language in which we can express things. And the domain of all languages is mathematics. Mathematics is an intellectual tradition that did exist for a few 1000 years, but not in an unbroken form. And in some sense, it's like a code library that has been maintained over the centuries. It's very elegant, there is not really versioning in it, and there isn't a unified namespace. So they use lots of Unicode. And it doesn't even have one central maintainers. But there is a problem at the foundations of the mathematics that we've been using this timeless thing and mathematics that we used to have. This turned out to be wrong. And it's one of the biggest discoveries in philosophy of the last century is the one Have quote Google that we cannot build a mathematical machine that runs mathematics without breaking. So this proof that Google made and that Turing edit tool is one that is often understood in philosophy, philosophers often think that what Google could prove was that mathematics is impotent into capturing reality. So instead, we need something that is much more vague and less formal, like the philosopher and question has to offer. And of course, this is not working, but girdle discovered is that he had a mistake in mathematics, we need to define tools in a different way. In order for something to be true, it needs to be proven in mathematics. And what goodwill and during quick discover is, if the proof takes infinitely many steps, it's not a proof, you can only have a proof if there is a machine that computes it. And that's basically made it big switch in mathematics to constructive mathematics and constructive mathematics is computation. So it turns out that every system that is able to model a reality and think about it has to be a computational system. And the next thing that Turing in church could show is that all the computational machines basically have the same power, that's a mapping between all of them, all of them are finite automata. And so we can make a machine that can think if we are able to build an automaton that is able to produce the necessary operations to approximate functions. And the universal computer is easy to make all the universal computers have the same power. But the question is, how can we get them converge to compute the right function. And this is what we are mostly concerned about in AI, discover the functions that do the right thing. So mathematics starts out with very narrow and formal languages, usually, languages in which we can hope to prove everything. But when we want to talk about the real world, the languages of mathematics that we're currently using, there are too narrow, which means you have to extend mathematics, you have to come from a different direction. And can we make a programming language for thought. And this project of making a programming language of thought I think, in our culture was first attacked by this guy by Vidkun, Stein, he, during the war started thinking about this problem very hard. And when he came back, he wrote this book, The Tractatus, which is one of the most beautiful most poetic books I know from the last century, it's a single thought on 75 pages. And the single source tries to explain how we can think in a way that makes language mean something. And the tragic beauty of this project is that it failed. Because it ran into difficulties, where it comes into describing images and things that go beyond images, how to deal with the ambiguity of the world. So we basically need to have languages that can deal with probabilities. The statistics was ambiguity, and Vidkun, Stein was not able to do this. And his project basically preempted the origins program of artificial intelligence that was started by Marvin Minsky, and a few others in the late 50s. So Vidkun, Stein could see very far unfortunately, it was lost, and most of the philosophers at the time, and sort of what Minsky set off, and others was what we could call symbolic AI, classical AI. So you identify a problem like chess, and then become a there's an algorithm that solves chess for us. And now you're currently in the second wave of AI in the sense. And in this second wave of AI, V,

Joscha Bach 8:22
which we use to call deep learning, we take a problem, and then we let the computer come up with the solution. So if we turn this into a learning problem, we give the computer the rules of chess, and we let the computer play against itself. And the computer comes up with patterns that explain chess to it, and to us, and dimensionally outperforms human ability to play chess. And it's tempting to think that the next age of MIP meta learning so instead of coming up with an algorithm that discovers an algorithm to solve a problem, why not come up with an algorithm that discovers an algorithm to discover algorithms? Right. And arguably, evolution is the search for such learning systems, meta learning systems, and our own brain is a meta learning machine. It's not that we are born with one algorithm to learn the world, but that there are many, many ways in which we can learn things and our brain discovers these things. So what How Does Learning work? It's based on information and information is the most basic thing there is a discernible difference, a change, and the meaning of information is the relationship that we discover to change and other information. So if you see a blip on your retina, that blip this blue change on your retina, the meaning of that blip, is the relationship you discover to other blips on your retina. And when your brain has discovered a lot of these relationships, what they what you hallucinate as an explanation is that there are people in the room that are shown by light and these people exchange ideas and they talk to each other and so on. And this is the relationship you discover between those blips. It's the best one you you come up with. This relationship is a computable function. So What we don't do an AI right now is that we put this all into a unified model. And I think this is the most important task that we have to solve, put everything into a single function. Biological neurons do this in a different way from our machines, biological neurons are basically little animals that have to cooperate with each other, to get fed to fire in the right moment. And eventually, your neurons will link up together in a structure that learns whatever the organism feeds them for. It's a very general paradigm that is quite different from what our machines are currently doing, which are weighted sums of real numbers. And so the general form for perceptual model is that encodes patterns to predict other patterns in the present and in the future. And it starts out with constraints that we observe in the world and that we try to map on variables, A variable is a set of possible values. And then there are connections between those which explain which variable is possible in the presence of other variables, you don't see these variables directly, the only thing that you perceive are the patterns. But the fact that there's a person, for instance, behind the patterns on your retina, that's a discovery that your own mind has to make. There are no mother minds, other minds, unless you create them, there is no model of your own mind unless you create it right. And the relationship between them as a relationship of possibility. There's a second class of links, that is the probability and the probability tells you, when you see a particular set of patterns, what is the probable state of the world so you can converge in one of the possible states. And the probabilistic links are the reason why you see optical illusions and things like this. This enables you to draw in a particular direction, your perception when you see a particular kind of pattern. So you know which one of the possible models you're looking at. And then we have the preferences. The preferences are what makes us care that give basically color and tools to our perception. This is what is the difference between an empty pattern, which is just a bunch of rubbish, a bunch of structure, a bunch of stuff, and things that actually matter to us that we want to be embedded in, that are connected to what drives us the feedback loop, which the staffers inside this is relevance. And relevance depends on our preferences. And we have physiological, social and cognitive preferences that make us ask that direct us on interacting with the world in a particular way. And then we have normative preferences, which are basically beliefs or priors. And they're part of the social hive mind that can befall us and direct our thinking in a particular way. So be thinking compatible based. And the way this is what makes homosapiens very, very powerful. But it's also what makes us limited. I think that homosapiens was not the smartphone minute, you were the programmer, the four minute, the reason why we were able to eradicate all the other hominids on the planet, was because we were able to walk in lockstep because we could be programmed with norms. And in a way, this is something that we have to bear in mind, because it's something that makes us very powerful, but it also makes us very blind. So models are constrained by possibility, what fits together by probability, how should we converge by a reward function, it's tells us what is important to us, and by normativity by how should we behave based on what our environment tells us.

Joscha Bach 13:21
Our human perception starts with patterns. And these patterns be generalized into presets, then then into simulations, which have in our mind a perceptual space. And this perceptual space is better self happens and varied. The current world state is modeled in our mental stage models, possible worlds. And you generalize over this perceptual space into global maps and a unified world model of the universe. And then we have an attentional system, a conductor that enables an attentional self and selects features in the environment that we attend to. And if we control this attentional system using the self and we have a protocol memory, that turns us into a person by our biographical memory that unifies our different modes. So this attentional system, and that was our consciousness, it's a model of our the contents of our attention. And the phenomenal consciousness is the memory of all binding state. The Access Consciousness is the memory of using attention because we need to train the attentional system because itself and the reflexive consciousness is the result of training. The attentional system is itself the memory of using attention on attention. So consciousness is something that is very specific to us in a crisis over disagreements that you have with the way the universe works. And there are three types of models in our mind. Perceptual models which optimize for coherence knowledge models, which optimize for tools and repair the perceptual models. And there are agents there are programs on us that do things that revive our memories, and that enable us to rewrite our reality and interact with each other and change our world. Agency is a result of our ability to have disagreements with the universe and do things better Based on this, but agency doesn't mean that you are deterministic freewill is the ability to do what's right. The opposite to freewill is not determinism or coercion. It's compulsion. Right? So the more you know what's right, the more it matters, what you do, the more meaning has what you do. And so paradox, saying that the more you know the right thing, the fewer degrees of freedom you actually have. And what the right thing is depends on our identifications. It depends on which timescale we operate, we identify this the future a future of humanity, do we identify with intelligent life on this planet to be entitled to identify with life in general? Or do we just identify this the next generation, and all these things are rational choices that you can make, but to determine in which way you decide what the right thing to do is, and ultimately decide our ethics. I'd like to end this with what I call the de Bosque theory. If we build a machine that is intelligent, and you make it work for us, like we will do with our artificial intelligences, or as we do with our corporations and nation states, which are machines that are meant to work for us, or as the organism does with its own mind, it's a machine that make generates meaning to work for the organism, how can you ensure that this thing is working for you. And the problem is, if that thing gets too smart, it might figure out that it doesn't want to. So no super intelligent system is going to do anything that's harder than taking its own reward function. And I suspect, this is also the reason why humans are not that smart, you can be very intelligent. But our motivational system is often wrapped into a big ball of stupid, because if we would really understand what we're doing, we might not want to serve the monkeys, you might not serve the monkey that conditioned our mind is the side effect of its regulation needs. And it's going to be very interesting question of how we can build machines that decide to serve humanity and us. Thanks.

Joscha Bach 17:04
Do we have time for questions? People ask questions.

Unknown 17:11
Yes. It was yesterday. But again, we My question is, are we intelligent enough to build intelligent machines?

Joscha Bach 17:32
The question is basically, to me can be, are there any limits that would stop us from doing that? So for instance, is the brain so complicated in terms of hardware that you cannot build a machine with similar complexity? And at the moment, it seems that these machines that we are building are already in this similar area of complexity? It's our brains respect to the amount of determinism that we need to get out of the substrate. The other question is, are we smart enough to find a way to make these machines do the right thing? And the way that nature did this was evolution, which is basically a blind search. And you certainly can do blind search. And it might take quite some time. And the more interesting thing is, can we do better than blind search? And you probably can do this. And then the question is, how long does it take? So what is the half life of our civilization seen from here? It's hard to say, right? Maybe 50 years, maybe 100 years, maybe 200 years depends when global warming gets us and how much civilization we can maintain when that happens? And are we able to basically perform the right kind of search by putting academics and industry researchers on the problem isn't the timeframe. I think we probably can, but it's very hard to say before, right? In 1900 40, when physicist discuss whether the nuclear bomb would be possible, it was completely impossible to know, because it took three more years before people had the right idea. After people had the right idea, it was obvious that it would happen or more or less obvious. But right before that moment, the opinion of the best informed journalist was not relevant. Even the important opinion of the best informed physicist was not relevant in 1940, because nobody could know whether we would stumble on the right idea. And I think in this sense, AI is very similar. We probably need a couple more ideas, but we don't know what these ideas are, and maybe we'll have them.

Unknown 19:29
We are talking a lot about thinking machines. Do you think humanity humanity will want to manage to also teach other beings to reach a similar level of intelligence as humans? No have?

Joscha Bach 19:43
I wonder if you already would have had the chance and decided against it? I mean, it's not obvious what the limit of the intelligence of a dog is, if you would breed it for having a much longer childhood and a larger head. And there are different kinds of dog breeds that differ very much in intelligence and those that have very close to us. Typically not very smart because they're harder to control. And the dogs that are the smartest ones, the ones that basically understand that incentives very well, but they're much harder to control. And so when we have a machine that is maybe smarter than us is the question is, can we control it? And if you're an animal that is smarter than us, who's going to run the show? It's a very big question. So if that happens, we best not identify as human beings anymore, but we identify as intelligent beings that are collaborating and cooperating with others.

Unknown 20:46
Let me just formulate my question quickly. So assuming that AI becomes more intelligent than humans at some point, and we're unable to control them anymore. It Do you think it will decide to keep humans and humanity on earth? Well, except for you know, global warming and anything before that happens? Do you think they will keep us or side that we're the poison? That?

Joscha Bach 21:20
Yeah, it's very hard to say there's a very big danger in this, right. That's when you build a machine that is more powerful than you. And that is not invested in biological life forms. Maybe it uses antibiotics, because it realizes that most of the problems on this planet go away when we go away. And we can probably make sure that some of the AI that we build a safe, but how can we make sure that all the AI that will be built is safe? Right now, when the machines that we are building, the corporations that we are building and so on, then not all of them are safe for us, right? They often do things, that very much defect against humanity. And I think it's an important and unsolved problem so far, on how to make AI safe. And it's also something that you probably probably cannot prevent from happening, because the technologies economically and militarily so useful, that somebody somewhere will build it, the only way to stop us from building AI, well, I believe, would be to outlaw large computers worldwide. And I don't see that happening. So it's an issue that we have to deal with. And I think in a way, the best possible near term outcome is going to be personal artificial intelligence, where you no longer managed by an AI that is owned by a corporation, but where everybody has their own AI, that is acting in their behalf, that is helping them to understand who we are, that helps us to understand how AI works, that helps us to understand how reality works, how society works, and basically levels epistemological deficits, that tells us how every one of us can become their own civilization, and cooperate with each other based on that understanding. That is, I think, a very fascinating perspective, one in which we can interact with each other based on a fully enlightened self interest of you know, truth. And we do not interact based on fake news as our society. So right now, but we interact with each other based on what can be known and how we should act based on that knowledge. And how we can play the longest game in there. Thank you.

Unknown 23:18
So two questions of his okay. One is, as a follow on, would that mean that you talk about this human brain interface where you like, where AI is, like sort of merged or merges with humans? Like in a physical way? That's one question. And the other one is also linking to your question that AI could eradicate us if they see us as a threat to to the world. I mean, well, we, as humans, or as mammals are animals in general have is empathy. And my question is, in your research, is empathy, something which is inherent in in animals, or is empathy, just a function that nature gave us in order to better survive, and if a AI doesn't need empathy, to survive in its environment, then it will lack empathy, and therefore, like just eradicate humans.

Joscha Bach 24:10
Most people are very selective with the empathy. They reserve it for the most cuddly members of the groups. And the fact that we have empathy doesn't stop us from having slaughterhouses. Empathy, I think, is the ability to read and understand the interstates of others. And we are better at this when these states are very similar to our own. And empathy is not the same thing as compassion. There are people which have very little compassion, and a lot of empathy, and people which have a lot of compassion and every little empathy. And so the more interesting question is, how can we build AI? That is not just going to be empathetic because they're going to be really able to read our minds at some point. And you could use this with neural interfaces. As you probably can also deduce most of these things from camera images. The limit of that can easily be seen by the most capable people. And there are people who track capable enough to basically watch you was the eyes and be able to deduce most of your mental state from this based on the context and on your actions. There are only so many ways that you can make decisions. And it's possible to out model you in the same way as I can usually out model my own children, AI is going to be able to model us and see what you're thinking at any given moment. And the interesting question is, then, how can we have the AI have compassion for us and decide that it wants us to be around? And I have don't have an easy answer to this, I don't think there's an easy answer to this

Joscha Bach 25:36
question you having,

Unknown 25:38
I personally think that we have a very slim chance of controlling an AI that is more smart than us. So um, but humanity has been harnessing technology for 1000s of years enhancing us with power. And so do you think there is a path into the future that we can enhance our brain with AI? So let what it means to be a human might be different in the future, because we are somehow merged with an AI or something like that. So it's not a thing about control because we are them now.

Joscha Bach 26:14
So I think we should separate our models from our hopes, then we want to model and we want to hope to be truthful with respect to this, you should completely disentangle the fate of chimpanzees species on this planet like us from what's probably true. And maybe humanity is just Gaia as a way to get all this carbon back into the atmosphere that Gaia could not get at this microbes. So Gaia came up with scribbles that she prodded into developing combustion tech. And when we're done, we burn ourselves out for 1000s of years been using technology. But we are the first civilization that has this kind of technology that uses combustion technology and technology of automation. at that scale, right? We are this first civilization that makes it happen. It's incidentally also the first civilization that went over 400 million people, it will billions of people. Before that, we've always hovered around 300 million or something times 100 million people and before religions, more like a few 10s of million people on this planet. So the technologies that we have right now, might very well something that is intrinsically unstable. And maybe the right solution would have been to go the biotech route and create something like a queen bee organism that lives for 1000s of years and depends on us. But instead, we now go down the route of creating a Crystaline intelligence that doesn't actually need us. And while this might incentivize us to play for a long game, our governments play short games, because we have not found a way to incentivize them for long games. AI might help with this. But yes, there's this problem that the interests of the actual machine, of the author of this industrial society machine are poorly aligned with our own interests. And to regulate this alignment is very hard, and we just don't know how to do it yet.

Unknown 28:07
Is there a solution to control how AI is interact with each other, for example, I mean, a very simple example for that would be the Facebook API a few years ago, where they tried to teach the AI to speak in English to each other, and they come up with a totally random language, improving the English language in a more efficient way. And I mean, there are other systems, for example, for financial markets or stuff like that, where an AI maybe in the future will kinda control the system a little bit, and the other AI who has a totally different purpose, but control systems or in other way, are there some solutions for this, so the AI will not totally count or kind of try to overwhelm the other AI or stuff like that.

Joscha Bach 28:57
I think if you want to prevent a system from taking over, you need to have another system which keeps us in check. And power, this freedom of choice remains in a system where you have a balance of power, not an absence of power, because something will always get to the position of power, right? So what would be the thing that keeps AI in check? In some sense, humans are AIS, if you are very much that it's an AI in our mind creates a virtual world in which a game character the self interacts with simulated universe. And this is what we subjectively experience. And this is allows us to negotiate with the world and the degrees of freedom in this AI in our own brain are tremendous. But what basically limits our own power is, of course, the size of our brain. And the other thing is our mortality if you're not that around for that long. So even if a single individual can completely reprogram itself to be uninterested in the fate of the species and of their children and children's children and their neighbors If this individual will die after a certain amount of time, and the new thing is that AI can live much, much longer than individual candidates. So the the technologies that we can use to automate governments, corporations and nation states, they're not just going to be disappearing after a certain amount of time, they're only going to disappear when the universe forces them to. And I agree this is a it's a big problem. But we don't yet know how to solve. Okay, on that optimistic note. I do think that a topic that is not hopeless, I mean, we are right now in a bind our situation is dire, we feel that our societies have largely lost the plot. And the biggest problem that we need to solve right now is governance. And I think that AI can help as best as it can help us to figure out how we should interact with each other, and what platforms we can create to negotiate the conditions of human survival. And in this sense, I think it's a very hopeful thing that we are now developing these technologies. But I just feel that there are dangers that come with this technology. And some of these dangers are going to be difficult to keep in check. But we probably will not have a choice we can have don't have an option, there is no button that you can press and this button says this is not going to happen. Instead, what we have to do is we probably have to see what's going to happen anyway. What what are the ways in which these things that are going to happen anyway are going to be dealt with and how can you get a head start on this? Thank you very much.
