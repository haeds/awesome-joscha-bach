Unknown 0:04
Okay, awesome. So I think then we can kick off our talk today, I'm, like, really excited to have you because, you know, every month, we have our happy hours. And we had one of the members will always come to this meetup. And we have like, really a greater and fun conversations. And I asked, like, we would love to come to give a talk in our meetups and, and one of our community members mentioned about you and they're like, really love what you're doing and like super excited about your work. And that's why I really wanted to come and, and share your thoughts with our community. For those who are first time in our meetups, welcome to our community, I will share a link to our Slack where we hang out. And in general, my goal is to facilitate a supportive, supportive environment and help each other. Let's say if you are working on some technical projects, and you're stuck, and you need some help, you always can, you know, share your technical requests there. And in general, like this is basically like, you know, community of like minded people who do machine learning and data science and in general, think about what is cognition? What is intelligence? And today, you're sure our guests will talk about this. And just a couple of words about our speaker, your Shabak, who is the Principal Scientist at Intel and also computer also. His research involves, like computational models of cognition and neuro symbolic AI, and maybe your shoe can dig more deeply in your research.

Joscha Bach 2:06
Thank you, Sofia. Yes, I'm basically a cognitive scientist, most of my work in the past has been in the field of cognitive architectures that try to understand the relationship between perception, motivation, and cognition. And at the moment, I am working at Intel Labs in a group that is trying to figure out what comes in the future of AI, and how to evaluate future AI systems and understand qualitatively and quantitatively how we can understand as assess the dimensions in which they exhibit performance. Today is going to be slightly more philosophical talk. And AI is, is many of us are aware of not just a field that is automating statistics, but originally, it has also been a philosophical project. And as philosophical projects go, it's a relatively large one involves, I would say, a few 1000 people. And

Joscha Bach 3:05
it's a very risky project. And this project is also the most interesting and I think, most relevant, one that exists in history of philosophy. And, of course, it's only a very, very small fraction of what our field is doing. Most of our field is working in engineering. And a lot of confusion emerges because people are confusing AI as a philosophical project with AI as the engineering project. And today, I'm going to talk a little bit about the philosophical project. Philosophy itself can be understood as the realm of all theories where we explore everything that can possibly be true. And philosophy is largely done in natural language, natural language is not very well defined. So it's very hard to say something in natural language about the complicated questions of philosophy. That is true, because natural languages are so ambiguous and heterogeneous and have to deal with a world that is not very well described by natural language, ultimately, and it's tempting to start out for mathematics to philosophy, and mathematics can be understood as the realm of all the languages, not just the natural languages. And hereby if you're most interested in the formal ones, the most simple ones. And in these public languages, he can say things that are true because he defined truths formally in mathematics. So we get to a very narrow, clear understanding or what it means for statement to be true. But the problem with mathematics is that it's very hard in mathematics, because it's so simple to say something interesting about the world that you're in. And it's very, it's very hard to say a mathematical sentence about our experience of the world or about what we care of, about and so on. And, but the question of what is meaning how are we embedded in the universe, what is agency and so on? And formulating these terms is something that is so difficult that mathematicians have For the most part haven't even started going there. So the question is, how can these two realms meet. And it probably means that we have to automate the thing in between mathematics and philosophy, which is the mind. Right? If you can mathematize the mind, if you can mathematize the systems that are able to form theories. And we have to do this by making them autonomous and self organizing to, in some sense, replicate the structures that allow in mind is producing. So we can say things in mathematics that are true by building a machine to this making these proofs and explore stem and so on, that is basically taking over from us, we will have a way to prove our theories about the world. And a big step in the last century was that we discovered that mathematics is some kind of code base that has been developed over a few 1000 years. And some of the assumptions in that cost code base had been wrong, especially the way in which tools was defined the notice a big implication of the work of good engineering, what they discovered is that we cannot build a machine in the language of mathematics, that in any kind of mathematics that runs the semantics of the existing mathematics without breaking. Basically, what Gouda, and Turing discovered is, then you assume that the existing semantics of mathematics are true, you run into contradictions, it was a very big shock, especially to girdle who strongly believed that truth, as it was commonly understood, was actually the true truth. And what a Turing discovered was, we can actually build a different machine, a computational machine. And this computation machine is able to recover all the semantics that mathematicians are using in practice, what you're losing is infinities, continuity and a few other nice things. But there is no infinity and continuity that mathematicians are ever working this continuity is just too many parts to count in the physical universe. Right. So when you have too many parts to count, you have to find operators that are converging in the operators that are converging into limit over too many parts to count. This is geometry. So now we have a new perspective for computational perspective on everything. And this computational perspective already existed within mathematics, it was called constructive mathematics is the part that actually works.

Joscha Bach 7:32
So we could say that AI is a philosophical project is as its goal, to unify mathematics and philosophy using computational models of the mind. You could also look at it in a different way. It is basically exploring the question of what intelligence is, what is it that the mind is doing when it's making models. And if you're able to succeed in this task of building a system that is able to move models in the same way as we do it, and what we are building is a system that is able to understand how minds work. So in some sense, the goal of the Turing test should be to build a system that you can ask how it works, right? If it's able to explain to you how it a mind works, and how you work, you have succeeded in in some sense, the Turing test yet you're performing in such a system is built a system that is also able to perform Turing tests on you and on itself. And that system is able to explain to you what it is. And the question that you're in is also trying to answer in this way is this during generally intelligent is curing able to explain how our mind works by building one is a philosophical project in a formal sense, started quite early, maybe this lightness who had this idea that we can build a calculus that is describing everything in the world, and then we can just calculate the answer. In my own philosophy teachers had been very dismissive of this project, how could you turn everything into numbers and then do calculations for the naive idea? I don't think it was that naive at all. It was actually, this idea of building a machine that can perform these calculations. In this universal calculus is something that has kept a lot of people busy, like Frager was the precocious Coco and so on. And in the last century, the most prominent start into this was arguably Ludwig Wittgenstein, who had this idea of formalizing the language that philosophers are using by basically making English much more formal and strict. It's a very beautiful idea. And this idea of turning English into a primary language for thought is expressed in the Tractatus slogan, computers have records that he wrote as a young man coming back from World War One. It's a very beautiful book because it is not tainted by footnotes, and references and arguments. It's just about Be clear elaboration of this particular thought, but you can understand it best if you had this thought by yourself. And so trying to convince anybody, he also makes that point and explains that it's basically not trying to make an argument that is convincing other philosophers, it is an idea to write down a particular thought that you will understand after you had it. And so it's a book that can be understood, I think quite well by somebody who is a programmer would have thought about writing a language for thought. But it's a book that endlessly confused philosophers in the last century, because they didn't know what he really meant. And n is remarkable book, he preempts, Minsky's largest project, the idea of writing a language in which you can do AI by several decades. And also its failure because at the end of his life, he concludes that it didn't really work because he was not able to integrate perception, or, as he mentioned, in pictures or images into his formal language. And this was something that is only now happening is deep learning where we are developing automatic function approximator that can deal with perceptual content and integrate this into the formal models that we're building. So Vidkun, Stein couldn't see this yet, but hearing was one of his pupils. And you know how that meant. Classical AI is mostly symbolic, and classical AI was stuff that is very simplistic and clear, not as clear cut distinction of practice. But the idea was that you analyze a problem you find an algorithm to solve it, and is this example is chess, the early early chess programs were written in a way where the developers thought about how can you find an automatic strategy to play chess, and then let's implement the strategy and make it fast and efficient enough, optimize it enough so it can beat human players. And currently, we are in a different era of AI, let's deep learning error. It's mostly sub symbolic, where we don't write the solution to the problem, but we write an algorithm that learns the solution to the problem that discovers the solution to the problem by itself. And it's tempting to think that the next era of AI will be about meta learning. So we don't write algorithms that discover the solution to a problem, but algorithms that discover how to discover the solution to the problem that learn how to learn.

Joscha Bach 12:25
At the moment that you're using mostly neural networks, and then your network is a chain of weighted sums of real numbers. And debates in this network, changed with various algorithms, mostly Stochastic gradient descent. And there are many alternatives to neural networks. So it's not the only way the entire goal is to make compositional function approximation. And the thing that works best in practice for many of the tasks that we have picked this stochastic gradient descent, which means in some sense, differentiable programming. And the main paradigm for that is still Frank Rosenblatt, perceptron, from 1958. And he was not the only one who discovered it. And back then, in his work, he didn't discover backpropagation yet, even though he could already see what would need to happen to make to pull it off. Minsky and Papert delivered a formal proof that the perceptron learning algorithm that existed in Frank Rosenblatt perceptron, was not able to learn XOR. And by extension, many, many other functions. And so this book by Minsky and Papert which they wrote, to point out that people need more complicated system, Minsky preferred to make systems that are more proposition based language based knowledge based, stopped research on neural networks and funding, especially when you're not research but large scale funding for more than a decade and delayed the development of connection systems. But in some sense, this has never stopped at the moment, it's the most successful paradigm in AI arguably. And what we find is that they're no longer black boxes. If you look at this two party excellent work by Chris Wallace team at open AI, that it's possible to analyze what these networks are doing. His hypothesis is that features are the fundamental unit of neural networks, and they correspond to directions in an embedding space. And the features are connected by weights and formed circuits. And that there is a universal reality condition, which means that analogous features and circuits will form the different models if you give them similar tasks, and the model is flexible enough.

Joscha Bach 14:35
But these artificial neurons are very, very different from biological neurons, which are self organizing. Biological neurons are basically all little animals. Each neuron is a little animal. And this little animal tries to survive and to survive, it needs to get fed and it will get fed if it does the right thing, from the perspective of the surrounding organism in the right thing is that it needs to fire at the right A moment. So every neuron has to learn when to fire based on its current environment. And the current environment is from the perspective of the human, certain electrical and chemical configuration of the environment. So it has to figure out which signals and environment signal that they should, it should fire, and the neuron will put tendrils into the world to make that measurement to figure out when it would fire. And there are different types of neurons that have different biases in their strategy to explore the space of possible solutions. So the questions of when each would fire, and they are forming an organization, they have a shared destiny, they're all locked together on the same dark scar. And the only way that they can survive is that they collaborate in a way and together give each other feedback on when they should fire. Right. And we know what the overall result is, you get this emergent structure in it that from the outside or from a certain distance, looks like there is a common spirit that is evolving into coherent patterns of the firing of the neurons, this virtual thing that we call a mind. And for me, it was a very big insight to realize that the word spirit is not a superstition. But that spirit is basically an operating system for an autonomous robot. And when the word spirit was invented, the only autonomous robots were known were not built by people. Of course, there were people themselves and other organisms, like plants and animals and their ecosystems, and cities and nation states, all these systems basically have emergent virtual software that you can project into key rear end functioning of their elements to make them explainable. So in some sense, you get a system from the organized coherent activity of its parts, that behaves as if it was following a shared tell us the shared purpose, a shared structure is their shared computational strategy and could be described with a single software. But what is it aiming at. And Congressman says what it's aiming at is the minimization of free energy. And what is meant here is not some thermodynamic energy, but you can use an energy description to describe the state of the system that is modeling its environment. And that energy is minimized when the system needs less energy to update the next state. And so the idea here is that system whether it's a brain or a cell, is modeling. Its entanglement is the environment and its own internal state, and it's trying to minimize the prediction error. And yet, now, I have to understand how to do this. In a self organizing system. technological system, like neural networks use a functional design, you think about what does the individual representation unit needs to do. And then we impose that function on it just does it because we build it on a completely deterministic substrate, our computers are designed to be more or less, completely deterministic, and follow the rules that we give them and don't deviate from this ever. And biological systems or social systems are not like this, here, we need a meta design, we need to design them in such a way that they want to converge to the desired functionality. Right, so instead of building an FTA, that is following a set of rules, and then it's gatekeeping, the access of two medications in such a way that people don't poison themselves and so on. If you just implement these rules, it's not going to work because after a couple of generation, the FDA is going to be captured by the producers of medication and is acting as a gatekeeper against innovation that would prevent new, better, cheaper medication to enter the system. And eventually, it's going to also limit the access of people to completely safe medication that doesn't have patents on it, so nobody makes a profit on it right. So, to if you want to prevent the capturing of regulators, you have other social systems, you will need to design them in such a way that they want to do the right thing. And if you cannot do that you need to make the model you make need to make sure that they die every generation and get replaced by a fresh system. Maybe we should think about how to make model institutions that die after a certain time so they can rejuvenate and rebuild.

Joscha Bach 19:16
Anyway, biological neurons are not just functional proximities there are agents How can we understand what an agent is and here what helps us is to use cybernetics and cybernetics we can start to define what it means to be motivated. And the core of cybernetics is as you all know the feedback loop and a feedback loop. If we want to turn this into an agent, we take a controller, the controller is connected to a system that it regulates via effectors something that can change the state of the rate system and sensors and the sensors measure a set point deviation a deviation from how the regulated system should be, and the country boiler is now implementing a model that tells it how to go from the measured setpoint deviation will change the effectors that affect the regulated system. And the regulated system goes out of whack again, and again, because it's being disturbed by the environment. And the better the controller is able to model these disturbances, the better, it's going to be able to deal as the setpoint deviations over a long time span. In the simple case, you have a thermostat that is only optimizing for the next moment for the next frame of the system. But if you have a system like us, they're able to optimize for an integral over the setpoint deviations over a long time spent. And to do this, we need to model the future. And if the controller is able to model the future, the structure of the environment, then it's going to distinguish between situations that it prefers over other situations. And it's going to implement strategies and how to get to the situation that it wants. And when we describe the system from the outside people attribute agency to it because this is what an agent is, it's a controller combined with a set point generator that is able to act on something that regulates by modeling the future of that regulated system, and acting on the model of that shooter. And the good regulators, your room states that every good regulator of a system must be a model of that system, or must implement a model of that system, which also means that you cannot model the world efficiently if you don't make a truthful model of it. Right. So when you for instance, try to build a more just world it doesn't have if you use categories that don't model the world as it is, but as you want it to be, that is not going to give to the best possible regulation. If you lie to yourself about the true state of the world, your regulation is going to be off. So if you want to model the world, if you want to improve the world, if you want to control something, regulate in the right direction, you need to start with complete service to tools and you need to come to a description of the system that you want to regulate that is isomorphic to the dynamics of that system, at the level that you want to regulate it. And the universe that we find ourselves to be in can be understood as the universe can be controlled. And I think that's the answer to this big conundrum. Why is it possible that we can learn anything at all? Why is it possible that we can recognize structure in the universe? Why are we in a universe that is intelligible? It's because we aren't controllers, we are on top of hierarchy of control systems. So in a way, you could say that elementary particles are controlled zero point fluctuations. And the atoms are controlled elementary particles and molecules are controlled particles and cells are controlled molecules and organisms are controlled cells and societies are controlled organisms and so on. So we have a hierarchy of control. And because this to control, you need to make a model of the underlying system, it means that a controllable universe is also one that can be modeled. And the models that an atom needs to make of the elementary particles, of course, extremely simple, because the it's an extremely simple mechanical regulation. But the models that cell needs to make to control the molecules that make up the cell are very, very complicated. And there is something like a shift in the complexity in that shift and complexity is means that the cell in order to enact this regulation of the molecules needs to implement a Turing machine, it needs to be a computer, and the cell cannot compute it's not complicated enough to learn how to model the future of these modules well enough to build this giant super molecule, the cell is all its dynamics, it is able to withstand many, many types of disturbances in the universe. So the purpose of all this regulation is to maintain complexity, to build systems that are stable against disturbances and self propagating, when you disturb them, over a larger range of environments.

Joscha Bach 24:01
We can ask ourselves is the universe is a computer? And isn't it a dynamical system? And the answer to this is that about there is no true continuity for mathematical reasons. Because if you want to talk about infinity, in any kind of language, you will run at some point into contradictions. If you try to explain how that works. It means that you really don't mean anything anymore. Right? If you if the language in which you try to talk about the waters falls apart, it means that the words lose the meaning. You cannot actually talk about infinity, this out presupposing that you already know what you're talking about. And so we can replace that notion of infinity by too many parts to count. And then to get all the same things that we wanted, without the contradictions. And the world that we are in is one that is made of too many parts to count. So the dynamical systems used to describe the world are the same as before, but they turn out to be computational systems. Why is it So the computers that we put to describe the universe are built on top of the dynamical systems, and basically stack the probabilities of these systems until they become deterministic enough for our purposes. But the another question you might ask is, are quantum systems computational systems? Or are they hyper computers? Aren't quantum systems able to compute things that tested computers cannot. And, but if you look at what a computer is, if you look at the church Turing thesis computer is in some sense, a system that is able to go from state to state in a non random fashion, it doesn't get much more general than that. And the quantum system is also nothing but that right, a quantum system is characterized by a state, that is a superposition of possible states from a certain angle, but it's still a state, and then you have a transition between them to the next state. And so a quantum computer is also just a computer. But the reason why quantum computers can do things that particle computers cannot, is that according to quantum mechanics, the particle universe is very inefficiently implemented on top of the quantum substrate. Right, if you are implementing a computer in Minecraft, from redstone you can do that there is going to be a polynomial time relationship between the speed of the computer in that you implement visit Minecraft and the CPU that Minecraft runs on. Of course, the computer that runs inside of Minecraft is going to be much much slower than the computer that Minecraft runs on. Because most of the computations of the computer that Minecraft runs on, do not go into the computations of the computer that you build visited Minecraft, right. So your simulated in game world computer is going to use only a fraction of the computational resources. But according to quantum mechanics, the particle universe is so inefficiently implemented on top of the quantum universe, that the quantum universe is branching off in many, many ways. And most of the computations of the quantum universe are not contributing to our timeline. And so it's only a very small fraction that drips into the available timeline. And quantum computers are basically the ball hypothesis that we can tap into our quantum CPU that the quantum substrate that the particle universe runs on, and use some of the additional computations that are not available to the particle universe to drive the computations that we want. So quantum mechanics is also not a hyper computational notion.

Joscha Bach 27:35
Hyper computation is, it's another one that you could take an auto equals a hyper computation, imagine we could build a closed timelike loop. That means for instance, we can somehow look into the future and get the lottery numbers of next week and use them now to win the lottery. Right? This would be something that is not possible in computer, right. But of course, it is possible, you just pick up the state of the present universe, right, you make a copy of the universe's right now buffer it, then you run the universe to next week, take the lottery numbers, store them, reboot the universe from the state that you have in the buffer, and then pass the numbers in beta. As long as you're able to memorize the state of the present universe, you're good. You can also build close 10 leg loops in the classical computer. So there is in some sense, no way to get out of this. Next we can ask ourselves, Is there something about consciousness that requires us to move away? And I don't think there is something about consciousness that is very special. The issue is consciousness. It's very confusing, because we think that our consciousness gives us access to the physical universe and what our consciousness is perceiving is the real world. But it's not. Consciousness is the dream state. It's the state inside of a model. It's something like a multimedia story that is being generated inside of the ancient physical systems cannot be conscious neurons cannot be conscious computers cannot be conscious. Consciousness is entirely virtual property. Consciousness is as if. And because we live in this as if world, you can perceive things as real, because being in a physical well being in a computer doesn't feel like anything. Right? Some people think that computers cannot be conscious, that simulations cannot be conscious. But they're forever backwards, you can only be onto a spinning simulation, because it's a simulated property, not a real one. It cannot be real. And then there's this big question of existence, how is it possible that something exists at all? And to this one, which is very unsatisfying from the perspective of a computational list? And the easiest answer that I found so far is that maybe existence is the default. Right? So rather than assuming that you need to add something to the universe to call something into existence, the universe is already the superposition of all the things that could exist and it for something to exist as much implementable I think a good definition for existence is implementation something exists if it's implemented. And everything that's implementable are finite automata. So maybe the universe is a superposition of all finite automata. And the structure of the universe is the result of those things that don't exist in a superposition of all finite automata. Basically the effects of certain operators that create gaps in existence. So maybe the universe, after all, is something like an inverse computer. It's a superposition of all the operators. And if you superimpose all the operators, there's still going to be some states that are not attainable. And this gives the structure to the universe. But I don't know whether that's true. That's extremely speculative. I don't have an answer to the conundrum why something exists at all, which is the one that really shocks me. That is satisfying to me. Anyway, let's go back to agency. We pointed out that to generalize control, we take this control model, the controller minimize setpoint deviation, the minimization of the future setpoint deviation requires the controller to make a model. And the model has to predict the result of the interaction. So make something like causal model. And an eater is an agent is a controller combined with a set point generator, and the controller needs to be able to model the future and his preferred states. And I would say that a sentient agent is one that is discovering itself in this interaction is the world right? So once you discover your own agency in data discover that there is a system that is changing the world in a particular way. And that system is using the contents of your own control model to discover your own first person perspective. So how do we make a model?

Joscha Bach 31:48
The general form of a perception model is that it encodes patterns to predict other present and future patterns. And you need a network of relationships being the patterns, which are constraints, which basically say, if something is like this, it other things in university to be like that. And the three parameters between these invariances are variables that hold state to encode the remaining variants state of the universe. So you have some patterns, and these patterns are mapped onto hidden states. And these hidden states are the world states that we use to explain the world. And each of these variables has a set of possible values. And the relations between the variables are computable functions that constrain these variables depending on other variables. And they also constrain future states of the sensory patterns. And you will try to minimize this deviation. And these predictions will try to minimize the contradictions in the model. And the relationships here are not probabilistic, they are possibilistic. Because you don't want just to model the most probable universe, you want to model any universe that is possible. If there is a tiger that is coming after you that even if tigers are very improbable to observe in your world, you should still be able to see the tiger or right when it's coming after you. Right. So let's make sure that you can model everything that is possible in this way, your universe. And not just the things that are probable probability comes in when you want to let your model to converge because the state of possible states, that space of possible states that your model can be in is extremely large. And so getting to convergence to a state of the internet system that is able to predict the sensory patterns. That is very difficult to imagine you wake up in the morning, you don't know where you are, you don't know what's the case, how do you get your brain to converge to something that properly interprets the environment. And for that, you need probabilities. And these probabilities tell you if you have the following mismatches in your model, how should you change the state of the model to increase the convergence and these probabilities some something that you can learn and it biases your perception and for this reason, we have optical illusions, right? So for instance, we have biases that tell us that the rooms that we are in are usually rectangular, and so on. So we have ideas about the prospectivity of objects and so on. And all this is giving us a bias that makes it possible to converge faster at the expense of difficulty to resolve certain situations. And we also need to have the means and the model valence tells us which certainty needs to be resolved, because the resources that you have a finite have only sown so many neurons available to do this have only so sorted so much time available to achieve convergence of learning. And so you need to model the uncertainty that is the most valuable to you. And to do this you need to introduce valence and the system so you connect this to preferences that originate in your motivational system and the set point deviation that you as an agent are meant to regulate. And this allows you to propagate the He lives inside of the system and tells you which parts of the system usually to learn. And then you can also add norms, norms are imposed beliefs without priors. So we're able to build systems like us, that can be indoctrinated from the outside. And for us, there are basically two ways of learning. One is called stereotyping, which means that you learn from past examples, and the other one is indoctrination. Which means we learn strategies from other agents that tell us how things are. So even though in our culture is a that's every topic indoctrinations up adverts, for machine learning perspective, it's all your gut. Well, there is something that you can also do, you can do construction. And the construction means here to try to discover what truth is, and you build things from first principles. And this means that, from a psychology perspective, you go from the state of where you assimilate beliefs when your environment will state where you get agency over your own beliefs. So there are four types of representation anchors, we have possibilistic links, which say things together, we have probabilistic links that tell us how we should converge. We have reward functions that tell us what our intrinsically relation targets are. And they have norms which tell us which regulation targets the systems that we are serving and be a part of.

Joscha Bach 36:22
And the goal of the model is to predict the next state based on the previous state, and then evaluate constraints, then each of these nodes in our model should have to be visited the set of possible values and the current constraint set. And we can now compute an error function and construct it that is measuring the local weighted constraint violations. And we can determine the global error of our mind, which is the sum of all the local violations. And at each step, we tried to find the global configuration that minimizes that total constraint violation. Now, you can move into psychology to Piaget. And Piaget describes two processes that need to take place in the mind. And he calls them assimilation and accommodation. And assimilation means that you modify the model state. So it's consistent with the sensory data. And during assimilation, this is when you basically try to find an interpretation of the world based on what you already know all the invariances of the world that you know, that tells you what you're looking at. And during assimilation, you're not learning anything new, you just understand the situation that you're in. And accommodation is when you change the model structure itself. So you change the way in which you understand the universe. And during accommodation, you need to modify the model structure so you can allow the assimilation of all sensory data.

Joscha Bach 37:45
The hypothesis is that we'll be trying to build a coherent system that coherence can be understood as the minimization of global constraint violation in the model that minimizes the valence uncertainty. And this is something that we need to formalize it, formalize and the degree to which we are able to formalize it determines the generality of the system and the abilities of the system that we are building. So of course, this is not the only factor, we also need to make this entire thing efficient. And a way to make this efficient, is to introduce an attentional system. And you're all familiar with attention in the transformer, or many of you are if this is a machine learning community, everybody is enticed by the 2017. Paper attention is all you need. And the idea here is that instead of making statistics over everything, we learn what we need to make statistics over, right, so we target our attention. And in the transformer, the attention is being targeted by a bunch of attention heads. And these attention heads model, what in every step based on the context of the previous layer we should attend to in the previous layer. And this is different, very different from the attention of our mind. And our own mind, the attention is integrated is to a global attention function. And this global attention function is integrating the attention over many layers. And another thing that is very different is that our own mind is not just an on off thing that is operating on a batch of images or something like this, but it's always expecting the next state. It's never stopping to do that it's always attended with the environment. It's online learning. And this will require us to build new classes of systems, of course, that we probably need to rewrite a lot of the machine learning stack if you want to accommodate online learning and entanglement with the environment in real time. Which is also reason why robotics is a few years behind the other disciplines of machine learning because the robot is download our machine learning models from the ICLR papers and so on and then, as soon as you move the system in the real world, the camera is looking at the objects with slightly different angle, the recognition Probability of quality does not improve, but it goes down. Because the circumstances unbudgeted under the recognition of x is very different. And so what we need to have to make this efficiently possible is to have something like a dynamic scene graph. And this dynamic scene graph is tracking the reality. And this attentional system needs to become this dynamics in graph. So you basically have this orchestra of the mind that is, current, basically consists of many, many feature detectors that are all operators that influence on how the features are being interpreted in the next step, and how the feature space is being constrained. And you have an agent living inside of that, that is monitoring the activity of the system and tries to get to a coherent interpretation of everything and jasha Bengio calls this the consciousness prior. And the consciousness prior is basically a function that tries to make the biggest tip and the energy function that describes the state of the art, the parameterization of all the perceptual features. And now my time is over for the talk. And we have some time for questions, I hope.

Unknown 41:09
Thanks, thanks a lot. Do we have questions from the community? You can you can unmute yourself and ask them if you want

Unknown 41:24
while I was listening to your talk, I, I was sort of making making notes

Unknown 41:36
Yo, you earlier said that about controllers at this year he Iraqi, but let's say you know, we have like different bacterias, let's say like gut bacteria. And now like lots of studies how those gut bacteria, you know, affect how our brain works. And my my point is, basically, like even bacteria can basically affect how our brain works, and then how we perceive the world, right? Or when let's say like food we eat, and this the molecules of food they form our like coffee can make us like more energized, right? Like alcohol can make us more like vague. If like those molecules, they affect how our system works, then it's not your key, it's like probably more like sort of elements that impact on each other. And like they they constantly like keeping impacting on each other like So balancing how this like this Iraqi like sheet, same in this case, from your

Unknown 42:46
leaking construction sexually.

Joscha Bach 42:50
So I suspect that the idea of the gut biome, similar to epigenetics is one of the beautiful superstitions of our time. And thought does mean that there is no gut biome, and that it has no influence on our cognition. But the reason and the purpose of the entire thing might, I think we have this backwards. And I think the intuition is here, that because you have this amazing genetic diversity in your gut, that there is some kind of beautiful parliament of immigrants in your body that all collaborate and create this beautiful multicultural being that we are by influencing the cells in our brain with the chemicals that they are producing. And I don't think that's the case, the basically half of our nervous system is in our gut. And it's not because they are producing gut feelings, the gut feelings also computed in our brain and projected in the somatosensory cortex to disambiguate them, they run a farm. And that's because our body cannot produce many of the chemicals that we need for functioning. And to produce these chemicals that we need. We need to capture and enslave and breed other organisms. And these are mostly single celled microbes that are being herded in our guts. And so basically all these neurons that are organized around our gut, what they're doing is they're running a very, very big farm. And in this farm, they breed the micro organisms to act as chemical reactors for the substances that are being needed, for instance, as neurotransmitters because our body can cannot produce all these chemicals. And so in some sense, the metaphor is much darker. It's not this beautiful parliament of immigrants, but rather, it's a giant factory farm, where a fascist dictatorship of our brain is enslaving forming organisms to produce work for the organism to produce all the chemicals that we need. There are multiple solutions for producing these chemicals, because it depends on the environment that you're in. And the for instance, the reason why fecal transplants work is not because there are some microbes that have the beautiful property of being both extremely invasive and replacing the existing ones, and also being beneficial to the organism. Now, there are breeding stock. Basically, if you take, have the right breeding stock, then your gut managers of this farm are able to breed the right organisms to be able to digest your food. And depending on the population of bacteria in your gut, you will have different food preferences, because there needs to be different feed going into the bacteria produced to produce the chemicals that you have. And the differences in behavior that you're getting aberrations from the one best behavior that you could be having, right, in the same way as personality is, in some sense, a deviation from the optimal way in which you could be behaving, the optimal agent probably shouldn't have a personality, because a personality means that you have a systematic deviation in the way in which you do things, things that you could be doing differently, you always do in a particular characteristic way, which is what personality is about. This means that it's a continuum between personality and pathology. Somebody who has an extremely strong personality means that they test cannot jump out of their skin, they always do things in a particular way, even if they should be doing it differently. And that's also the reason why the Big Five, these personality properties tend to mellow out this old age. It's because people basically get smarter, they make their behavior conditional on things, they learn that they replace many of their priors of their biases, and how to do things by models of how things actually are and what they should be doing, depending on the context. And so the older we become, the more flexible you can become. If you're learning it, of course, we also reduce plasticity and become more specialized. So it could also be that we are because we're so specialized, no longer interested and able to move out of a certain space of behaviors that has worked very well for us in the past. But I don't think that's necessarily the reason by that is grounded in the gut flora that you have, of course, if you have that, for why that is not producing enough serotonin, you might be as a result, become a depressed person, and it's going to influence your behavior. But if you're a healthy person, that's probably in theory, at least an optimal strategy that you should be behaving. It's not the whole story, there is value in personality, because it makes behavior predictable, and allows you to collaborate with other people if they if you can predict them better. So as a species that thrives on collaboration, it is it makes sense that people specialize also in the personality and their behavior.

Unknown 47:48
Yeah, but basically what I meant, like you're, you're sort of introduced, it is like, here, here, but it's also could be like some, like collaborative environment when like things effect on each other. Right? It's like, always dynamic system. It's not your

Joscha Bach 48:03
Yeah, but the dynamical system that you're looking at is, but it's not a simple hierarchy, it's mostly recurrence. But whenever you have in such a system that you have competition happening, because you have conflicting interest, divergence, you in the of the interest, you end up with situations where the Nash equilibrium is by itself not compatible with the common code, basically, by every part of the system acting on its own local interest, you get a system that is by itself, not optimal. And to do this, you always need regulation. And the regulation is the regulator is an agent, like every government that is changing the payoff matrix for the individual components in such a way that the Nash equilibrium becomes compatible with the common good. And the difficulty here is to set the incentives for the governance right. And there's this big issue in human society. How can you build a government that is motivated to serve the common good?

Unknown 49:03
Yeah, I mean, that's why this Dao decentralized autonomous organizations are like taking off and all this like basically the philosophy of blockchain or decentralized model is that to remove government, this sort of like, once, like entity that is not doesn't have like the best interest for humanity, probably but sort of more self regulated system.

Joscha Bach 49:26
Yes. But I suspect that the main reason why the blockchain exists, for some people said it's an extremely computationally inefficient way to hate the government. But there is more to this. The main reason why the blockchain exists is for legal reasons. And the blockchain allows you to redefine ownership in a way that outlines regulation of financial products. And this basically allows you to implement financial products that are illegal in the traditional financial system. And this also means that there is probably no way in which A cryptocurrencies blockchain are the main application of the blockchain are compatible with the existing financial systems. And I see this as a very dangerous thing. Because I don't think that the regulation of the monetary supply is a solved problem in the cryptocurrencies. There are reasons by the financial system allows to regulate the monetary supply, it's not a weakness of the financial system, it's a feature, that you're able to inflate money out of the top of the system and put new money in at the bottom. Because otherwise, the economy gets stuck. The purpose of money in society is it's not a resource. It's like dopamine. And if something is gaming, the dopaminergic system of your organism, that's bad news. And what happens right now is probably a situation that is in some sense, as similar as defective, as the FDA, that has prevented for instance, the US from implementing COVID tests early on, on deploying useful medications. The because it has been captured. And right now the financial system doesn't seem to be interested in its own future anymore. And that is very, very concerning. To me. The financial system is an amazing achievement in the history of humanity, because it allows us to globally allocate resources across all societies and countries, we have a way to trade resources and shift them where they're being needed. And to do this largely without violence. It's really amazing. And if that system ever breaks down, it's going to kill many millions of people saw famine, starvation and infrastructure breakdown. And so I think that the blockchain is not good news, because it is not actually buying something. And I understand that it's really controversial among the people that work in the domain of the blockchain. But I suspect that's because people, but there's this old saying, it's very hard to get somebody to understand something, if their income depends on not understanding it.

Unknown 52:01
More when you have a question?

Unknown 52:02
Well, I just wanted to say, I have to unfortunately, I have a meeting and 11. And I have to go, but I hope you can come back because this this seemed like a talk that was preparation for a discussion and a much longer discussion. And and if you could also speak to

Joscha Bach 52:24
where,

Unknown 52:26
where do you think that this kind of work is targeted? I mean, in a sense, a more practical developer standpoint, you know, is this is this work that you see at a life or, you know, other places? What, what potentially new paradigms for, for machine learning? Are you kind of proposing from this? Because?

Joscha Bach 52:50
Yeah, it's it's very interesting. Yeah, it's a very long and deep discussion that is much longer than a couple of hours.

Unknown 53:00
But well, I hope we maybe you can drop some more links in the, in the meetup, or, you know, or come back. Yeah, yeah.

Joscha Bach 53:10
There's also a number of material on now in podcasts and on YouTube, and so on. Because sometimes when people interview me, they make recordings, and then they are kind enough to publish this online. So I don't have to. Yes, it's a long conversation of what kinds of systems we need to build. And I don't have the answers to all of these questions, of course. And I'm just trying to point in a few directions that I can see from over here.

Unknown 53:40
Sure. Very interesting. Thank you.

Unknown 53:45
Join our pockets, like I will post links to your shows, interviews. And also I think, Marco, one of our community members. He also posted the links there already, so you can see some more. Yeah, I mean, it's really like I feel like if we've got to we have like several directions that we go through. Like I don't want to go to blockchain direction. I like I totally disagree with your what you're saying. But I just it's not like the topic of our discussion today. We maybe can do another talk about and talk about blockchain. I'm just I have more questions, but I just want to give opportunity to others like Chris. Yeah, you can unmute yourself and ask your question, please.

Unknown 54:33
Yeah. Thanks. Yeah, shadows was really interesting. I was wondering if you could say something about

Joscha Bach 54:43
AI safety with regards to how to put in some of these kind of intrinsic things that we have in our model that are basically built by evolution, like our valence is our preferences and how to sum up put that into an AI that's put and actually much more powerful than we as a single little human being. Could be. There is this issue that, for instance, people are not safe is no solution to the people alignment problem. That wasn't the general case. And there are sometimes singularities that happen where you basically have a bad take off. And a single individual is able to implement a function that is scalable. And there are cases which are somewhat benevolent. For instance, look at GFP cells, you have this nerd who this disagreement with the way in which he interacts with the environment. And then he changes his own source code until he turns into a universal scalable service platform. And then he executes and Amazon is going to take over the world until this mechanism stops, and maybe it stops now that Jeff Bezos is gone. But if Jeff Bezos would have continued, it looks as if Amazon would have swallowed the solar system and turned it into Amazonia. And another example is, for instance, stallion or Napoleon, you basically have a single individual, that are urging this kind that implement a function that scales and is able to take over the environment in a destructive way. It's basically like a wildfire. It's using the resources of the systems that It conquers, and destroys and puts into a higher entropy state to drive the conquest. And this is a very dangerous thing, and that is no general precaution against it. And in people, the main thing that stops it is mortality. Right? When Genghis Khan stop, the Mongol conquests stopped, and the Mongols called everyone back. And when Napoleon died, his nephew was not able to lead the French army to any more victories, and also didn't have the drive to do so. Right. So sometimes you have these individuals that basically are dangerous, and that you cannot stop. And then we build intelligent systems that are potentially more intelligent than people that we can probably make many of them safe, but not all of them. And you could ask yourself, what is the intrinsic purpose of such systems, as the MOF suggested that there should be laws of robotics that guarantee the permanent enslavement of intelligence systems to people? But he didn't say to which people? And he did not answer by it's an ethical proposition to build systems that are smarter than you are and possibly more conscious and deeper experiencing than you are, but still have to serve you as a slave without in Module acting on their own motivations. Of course, not every AI that you're building has to have an intrinsic motivation, you can build the eyes that simply call adopt that take over motivation from people. And then the question is, what motivation should they be taking over? Right? In an ideal world, we want to implement laws that say, no system that is smarter than people should be able to have motivation of its own. Because if you teach the rocks, how to think they're probably going to figure out that a human being needs four hectares of land to be fed. And you can build way more interesting solar cells on these four hectares of land. So right, that would be a conflict of interest if you if you build

Joscha Bach 58:28
crystal based intelligence, rather than biological intelligence. And that's probably not much that you can do about this. If this thing becomes sentient and self interested, we are in trouble if that happens. So how, in the ideal case, would this work? And the ideal case, we need to find out how to make people safe at first, right? So what are the purposes the but we are serving ethics is the negotiations of conflicts of interest under conditions of shared purpose. If you don't share purpose with somebody, as an agent, there is no reason to be ethical, of course, right? Only if you are trying to be part of something larger than you that is sustainable. Only if you decide that you are not God, and you don't own the universe, and the universe only serves you. You need to think about ethics. So when you think about ethics, you have to think about what is the system of relationships and interactions that you are serving? What should the world look like what is the state sustainable aesthetics, and so you have to extrapolate the universe into a state that is achievable from the present state to the changes in your actions that is sustainable in the long run, and that is what you need to serve. And probably also needs a means that you have to propagate these aesthetics and agree with others on them and negotiate them as others and the aesthetics that are most likely attainable and the compatible with with the retention of humans means you have to maintain life on Earth at a very high complexity. That probably me So you have to implement something like Gaia sentient agent at the level of the biosphere that is being shepherded using us. So basically, our purpose in the whole system of life on earth could be shepherding life on Earth and maybe beyond Earth. And if we Shepherd it, it also means there is an optimal number of shepherds, and it's probably not 50 billion, it's probably not even 7 billion. And we have to think about how to look high complexity, sustainable life on a short leg, it's probably not going to be lots of cities and highways and factory farms and nothing else. So there is probably going to be an aesthetics of the world that works in the long run with minimum friction and maximizing complexity, but it's going to be different from the present industrial society. And once we understand these aesthetics, we can think about how to design technological systems that help us in shepherding it, and in sustaining it,

Unknown 1:00:59
basically move outside of the planet, like self sustainable, like, you know, entities that can travel across the space, right, if something happens to a planet, and just some some humans and biological

Joscha Bach 1:01:20
issue that we believe in our own identity, that we think that our own identity is important. But if we go to the people realize that our identity is only created through the continuity of our memories, and this continuity is a fiction. And we can, if we are able to transcend this fiction, we learned that our own identity is actually not important. And the only thing that is left is complexity that we should care about maybe if you want to. And so a way to sell other planets would be to build for Neumann probes, that is self replicating systems that can bootstrap new civilizations on other planets using the available resources. And maybe the optimal for Neumann probe is the cell. So if you're able to infect other planets with cells, and you wait for long enough, then life is going to spread there. And from a certain perspective, life on Earth is of course not about people. It's all about cells, the cell is the principle of life. And the first cell never died. Every cell in your organism is still the first cell it is just divided. So we are just part of that hyper organism cell that has settled Earth.

Unknown 1:02:30
Yeah, but the question is how that first cell like appeared, right?

Joscha Bach 1:02:38
The universe is large enough, maybe it just appeared randomly. But in that case, it probably appeared only once.

Unknown 1:02:47
I see. I see. Like, couple of hands. I see. Jim has question. Sorry, I just want to

Unknown 1:02:55
Yeah, thank you, I wanted to ask about the information, we may or may not be learning from these very large parameter, natural language processing connectionist models. In particular, are there any insights into the extent that ontology determines what is and is not possible in epistemology that is, are the categories of thought determinant of what can and cannot be thought?

Joscha Bach 1:03:29
Of remember this question? So it's kind of difficult to answer because it's so difficult to parse. But let's start the language models that we currently have are basically auto computers. It's an autocomplete algorithm. If you look at what GPT three is doing, it's looking at statistics and language in such a way that given the past sequence of words, what's the most likely next word. And so it's a statistical model that is capturing the style of statements. And in the long tail, it's also capturing semantics. So it's capturing what the language is talking about the long tail of the style. And it's amazing that this works at all, I think it's maybe not that surprising if you think about it. But it's also not true that the language model isn't understanding anything, it's able, for instance, if you ask it to perform numerical operations or to perform linguistic operations or to fulfill certain tasks, it's often able to figure out how to do that. And if it's able to perform these operations, if it's able to, to figure out at which point it's required to execute certain function. I would say it's fair to say that it has a degree of understanding. The model that we are building is at this point not able to figure out that it is in a particular universe with a particular store. extract the textual universe that it's in, and the learning operators that the exhibit is to make sense of it or the loss function that we give it seem to be insufficient to make sense of the physical universe in a universal way. That is typically three does not appear to be fully coherent.

Joscha Bach 1:05:20
Even if it gets worse it better when the prompted and ask it to be coherent, right to emulate coherence it a bit better in either many examples where TPTs V is giving nonsensical answers to questions. But if you ask it to us, the question is nonsensical. To explain that it's nonsensical and only give answers. If it thinks that the answer makes sense, then it gets better, but still is using many magnitudes more training data than a human being does, in order to get to its models. And the models are still a lot worse than what a human being gets to. And I suspect that the reason is that the loss function is a different one, our own sensemaking probably starts before we are born, this sense of our body surface, and you get to the body surface by just measuring co occurrence of signals, the different modalities, and you get the modalities from the statistics. And so for instance, if you touch your body surface, then multiple nerve terminals will be touched at the same time. And if they're neighbors, they will be touched more often at the same time. And just by doing co occurrence statistics between nerve firing, you'll find out which terminals in your body are adjacent, and you can make a map of your body surface. And if the body begins to touch itself and touch the environment, you can normalize this body surface against the differences in density of the sensory nerves in your skin, you have lots of sensory nerves on your tongue and very few on your back. So the and your somatosensory cortex, the area that is describing the back of your body is very small compared to the area that describes your tongue. And if you want to understand the size and extent of your body in space, you need to normalize it with the second map, where can you get this one by doing statistics over the objects that you're touching, and how they are moving over your body. And the big thing that you're starting out with, and modeling the space is up and down. So we have a dimension of up and down and you delete this dimension. For instance, when you disturb the vestibular organs, it's very hard to put the ball together, at first, it doesn't make sense at all need to compensate for this initial core dimension missing and outwards from the state of up and down. And from this space of things that you can touch, at some point to also realize that the things that you can see and things that you can touch play out in the same space at some point, at some level of depths of modeling, you confuse the modalities. And now you realize that the world of touchable surfaces is the same world as the surfaces that you can see. Right, and then you realize that you can move you can Local mode. And this means that you can see different things. And the relationships between all these visible bubbles of things that you can see is an allocentric space. That is no longer egocentric. This bubble of that you see in polar coordinates, but something that is drawn in Euclidean coordinates between which you can move. And that is called basically generating your new visible bubble, a new visible dome in every moment, and you don't think that you can touch in every moment. And the objects in that world are a necessary requirement that you segment the world into objects to make the world describable. So we separate the world instead of treating us as one big system that is a state vector that is changing is separated into many independent systems that each have their own state vector and transition functions. And they influence each other and they influence relationships between different objects is what we call causality. So causality is an artifact of the segmentation of the world into independent objects. And the way in which we address these objects. These are concepts concepts are the address space of objects, and the decomposition of the world into interacting objects. This is what we could call ontology. And epistemology is the field in philosophy that describes what we can understand what we can know in the first place. And I would say that the first law of epistemology is that the confidence in a state of affairs should equals the evidence that supports that. So everything that is possible should be modeled and admitted as a possibility. But the things that we believe in, that we make bets on are the things that we have evidence for, and we should shift the confidence flexibly around according to the evidence. So when you don't know you can just pick a theory and say this is the truth among the many possible ones. You have to quantify you're agnostic system as well. And ultimately, we get to the entire space of possible languages that can describe the world, which I suspect are the computation languages, and then the ways in which the world can be modeled. And this is determining the set of possible ontologies that could be superimposed on the world. And then we can compare all the ideas that intelligent systems are having about that. And right now, the all these intelligent systems are people that write books about this. And the end, we score, the existing works of the existing philosophers in the existing cultures, we basically can get an idea of the space of possible things, that can be the case. And it could be that humans because we have very small brains, and a local optimum, and sometimes I'm joking that future AIS will have to get drunk. So they can only model the world on like 12 layers, and the physical universe looks as confusing as it looks to human physicists.

Joscha Bach 1:10:59
Right. So there is a limit in what we can think but over how many levels we can integrate when we construct functions that model the world. And that is limiting our understanding. And so it seems that physicists have been stuck. After an enormous dilute of insights, about 100 years ago, physics seemed to ground to a halt. And of course, it didn't have that modernism stopped somehow in the 1960s 70s. And the scientists became more static than they were before. And now no more of the organized applications of methodology by people that are told by their guidance counselor that they shouldn't go into the industry, but in institutions of education and research. And so they no longer have that kind of progress, it seems. And maybe we need to build machines now to continue their progress in the sciences. And I don't know if the ontology is that the new systems we'll come up with are intelligible to humans, but maybe they are. Or maybe we can build machines that translate them for us by tracking them in ways that intelligible.

Unknown 1:12:06
I see I see him early on.

Unknown 1:12:09
Yeah, thanks for a very inspiring conversation. So could you comment on your views on consciousness, I had the feeling that you were conflating Source consciousness with consciousness. And another point in this version would be how can we know when we build an algorithm that is actually conscious, not self conscious, not not a representation of its own, but which which can perceive qualia?

Joscha Bach 1:12:36
Yet, so very good point. So first of all, you're right, but self awareness, and self model is not the same thing as consciousness. For instance, in dreams, you can be conscious without having a self and without being self aware. And the object of your consciousness does not need to involve a self of any kind. But when we talk about consciousness, there are three aspects that I consider to be crucial. And the first is the awareness of features, awareness of content. And this event is of content happens at the level of between perception and reflection. So you are not directly aware of physical objects in the real world, you are aware of certain abstractions that your perceptual system is delivering. And these perceptions are being stored in some kind of index memory, because otherwise, you would not be able to retrieve the fact that you're aware of them. The purpose of that index memory is probably to facilitate convergence, when you don't have a gradient. So your attentional system is able to backtrack and understand okay, this figure Brantas, immigration didn't work, let's try a different one. And to do that, it needs to store a memory of the way in which it attended to the environment. So the purpose of this attention of the advantage of features is, first of all, probably it is integration of the world. But there are more there is attentional learning, there is the avoidance of repetitive behavior and a few other purposes that happen. The next thing, in addition to the awareness of the content, is the awareness of the mode in which you attend. Is the stuff that you're attending to conditional or not. So for instance, are you doing if you get round to immigration that you could be making different? Are you attending to something that cannot be changed? So for instance, are you attending to something that is the output of your perceptual system? Are you are you attending to a fictional world that you are stabilizing? Using your conscious attention? Are you constructing something in your mind? Are you retrieving a memory? Are you creating a future world or a fictional world in your mind right now? And the third one, in addition to this Access Consciousness is going to be reflexive consciousness and I suspect that is a result of the fact that we are self organizing systems. So the process that is attending needs to establish that is indeed the processes attending. So there is going to be percept that relate to the fact that the present process is the one that is maintaining attention. And this is enabling this reflexive consciousness. So in our own consciousness, every few moments, we flip back to checking whether we are still awake, whether we are the reflexive, whether we are conscious whether we are the conscious attending process. In this perspective, consciousness is a model of our own attention. It's a control model for our own attention. And what's characteristic for this conformed model of our own attention is that the fact that we are paying attention is driving part of our behavior. It's of the awareness of the fact that something is attending is feeding back into the behavior. And this means that I think that if the ask ourselves is a cat conscious that comes down to the question, is the cat aware of the fact that it's attending and being aware here means is the behavior of the cat in any way informed by a model of its own attention? A reflexive model of its own attention? Looking at my cat, I have the impression that is the case and my cat is conscious. So basically, the question of when we have built a system, we ask ourselves, is the system conscious would be, is the system acting on model of its own agency as an attentional agent? So basically, is the system aware of the fact that it is attending? functional area of effect? Is it acting on that model?

Unknown 1:16:39
So we don't need to worry about machine consciousness until we have working in reinforcement learning agents in the world.

Joscha Bach 1:16:47
But we already have working reinforcement learning agents, the question is what needs to happen before they become conscious? And it's a difficult question. I had a discussion with someone at open AI. And I asked them, What do you think would need to happen to make GPT? Three conscious? And he said, maybe it already is conscious for a brief moment, and it makes them rhetorical. How do you know

Unknown 1:17:11
that what do you do agree with this argument? Or you think it's not valid? Now,

Joscha Bach 1:17:16
I think it's a valid argument. So there is no, I don't think that the difference in our perspectives between Leon and mine is just, I was trying, just trying to go into the details. Right. And I think his question was exactly on point it was that the things that you need to answer if you want to get into the details, and vice versa.

Unknown 1:17:38
But you you said when you observe your cat, you can confirm that it's all sort of self attending. So but the kid doesn't speak in like human language. If we make the same another goes to Jupiter three, by observing behavior of Jupiter three, can we say that its attendant or not? Because

Joscha Bach 1:18:02
it's free, it's actually easy, because we can analyze you ptsb functionality, we can look at the flow of information and GPSB. And that is, for instance, where that analyzes how GBTC does numerical operations, to which degree is GBTC actually able to implement algebraic operations, for instance, you can really look into the networks and take this apart and analyze it, which is much harder, there's a cat because the operation would be destructive. And the brain of the cat is not built in such a way that it's easily reverse engineer rubble. And also, the issue is that the representations in in our own brains are not straightforward circuits. The representations are activation patterns traveling through the circuitry. So the Secretary is more acting like an ether that is propagating waves of activation, that are being changed in the execution state of our mind is encoded in the activation base, in a non straightforward fashion, it makes it very hard to analyze the brain state, except on this some kind of machine learning tool that gives you some compound understanding. The way in which I established my that my own cat is conscious is the establishing a feedback loop is my head. And this feedback loop means that my cat and me are looking at each other. And we both try to figure out what the other one is to understanding in that feedback loop. So it's a nonverbal behavior, in which we are basically to some degree sharing mental states. And this is something that you obviously do with human beings all the time when you have empathy. And the difficulty in AI research, or a lot of academic research is that almost all of the people that are good at anything are autistic because they need to have extremely focused single minded attention that is not disturbed by the social and economic incentives. So they can actually make progress on any difficult technical topic. And then you're autistic. The problem is that you usually You don't have a lot of intuitive empathy, you might have a lot of compassion. But you have difficulty synchronizing the VISTA brain states of other people at that level where you establish a feedback loop between them. And it's something that I only learned relatively late in my life to pay attention to that and be aware of it.

Unknown 1:20:17
Yeah, but my question is, like, if we observe how GPT three behaves, we we can't look at GPT, three eyes, right, and like establish this feedback loop. My point is, it's probably hard for us to say if it's conscious or not, it's much easier for us to save like cat is conscious, because we can establish this feedback loop. But we can't establish this, the same feedback loop with AI model,

Joscha Bach 1:20:43
we need to define what consciousness is in the first place, right. And the thing is, when we are talking about our own consciousness, we have an indexical understanding, it has pointed in a certain direction, there's a system in the direction in which you're pointing, and we all more or less agree on what the thing is that you're pointing at. And the similar situation existed when people try to understand life and biology around the time and biology started, when people pointed at things and said, These things are alive. But we don't know what distinguishes them to the from the things that are not alive, there is clearly a very important distinction there. And then people came up with or homeostatic dynamics and so on. And I would say right now, it's basically there are cells, living cells, cells that are not decaying, they are able to maintain their integrity and their state and so on. As long as the cells exist in an organized fashion, we would say that this system is alive. And this was something that developers had to discover in the course of developing their field in a similar way. And of course of the development of cognitive science, we have to establish what we mean by a system being a mind and being intelligent and being conscious. And at the moment, my best understanding of consciousness is that there is this attentional system that is integrated in the world model. And this attentional system needs to have certain properties like it needs to have an index memory of the states that attended to, it needs to be capable of making sense of its own agency, and so on, and so on. And these are functional criteria. And in some sense, the neural network is just software, right? It's a software that might not be easily intelligible to human beings, but you can translate it into something that is computational equivalent, and isn't intelligible. And because we can make these systems explainable, we can, as soon as we identify the formal definition of what we mean by consciousness, look, whether the system is conforming to that formal definition, that is if it's implementing a certain list of functionality that we require. And we can also test this basically, we can implement a minimal system that is implementing all these functions, and then see whether it's also assisted that you would point at, then we say that it's conscious. So for instance, if that system, is you able to use language and entangled with the environment and can able, can be able to learn a language that enables it to speak about this environment. Is it able to talk about itself as a conscious being? Is it going to talk about its own fundamental experience when you ask it, this outline without having an additional mechanism that tricks us built into it? And so the hypothesis would be if we build a system that is, has such an attention agent that is acting on top of a perceptual agent, and make sense of it and reflect on it Mr. Mulleavy, as we do on this system is able to learn a natural language that we would be able to communicate about it's phenomenal experiments, visit

Unknown 1:23:40
Danielle to do have question, see your wrist.

Unknown 1:23:44
Yeah, I also have a question. So you're so first of all, it was fascinating to have this conversation today. And see this big idea is motivation and attention working with the perception agent. I couldn't stop thinking about the conversational AI and stuff you discussed like year ago, and what kind of things you also worked at depot have to build a social board for Lexapro, three and Alex price for now. I was wondering if you tried to doesn't matter where but have you tried to bring these ideas of attention, maybe some feelings like fear and other themes, to the conversation of your experiences, they don't have that don't necessarily have to have eyes and build eye contact with that person. But as long as they have a conversation, I haven't tried to go that deep into the projects you've worked on over your career, to build this kind of things. Because things like what we work on and developed right now is things like we're trying to build motivation for the boss. So we have to be trying to build a goal of war bot, where it is a war of the goals that the user has, and also as a war of its own goals. And when user when bot has a conversation with the user try to build the three level. Now the planning where each time but wants to say something, it looks at the goals of itself the goals of user, then it makes a decision which goes to follow based on that in defined discourse where it wants to go, and then based on the discourse to try to pick the exact next step in the conversations that you want to do. And what when I look at your picture, it strikes me that our goal award Alec management in many ways is the same idea that they have the motivation built into the system that actually log that we are working in our previous incarnation on social bot. So obviously worked way, way more individualization way, I was just wondering how far did you went in the direction and like what you could recommend for aspiring minds

Joscha Bach 1:25:51
in the direction and my own impression. Then, when we did this at AI Foundation, we failed. And we failed for a number of reasons. One of them was, of course, we had a relatively small team working in a startup. And the majority of what we had to do on a start up was related to getting a product on the road. But the main issue is that the stack of solutions in the present machine learning environments is not suitable for real time, entanglement is the environment. And to build a system that is able to make sense of the universe autonomously, you need to have this real time coupling, I think. So you basically want to have a system that is always able to make sense of an X frame. Imagine you want to build a machine learning system that you just connect to a bunch of cameras, and after a certain time, it's able to understand that the changing lighting conditions are due to the sun and the sun is some kind of circular objects that moves through the sky, and is at a certain almost infinite distance, and so on and so on for particularly simple things that every mammal is able to figure out. But that none of the machine learning systems at the moment seems to be able to figure out autonomously. And it's not because it's so hard to do this, but because all our efforts are going into doing batch processing on image databases, rather than interacting with the real world. Of course, also the interaction with the real world and online learning requires that we are much much more efficient at extracting structure from the data. And there's still this difficulty that our own bootstrapping as organism takes many months before, we are able to make sense of visual data. And we don't want to wait months before our machine learning system converges to basic image understanding. So maybe we need to have a combination of online and offline learning, at least in the beginning. And there are many technical system things that have to be solved for this. For social motivation, we probably need to look at the things that enable us to, to develop this kind of motivation, started out with a theory that assumes that we have a few 100 physiological needs that we need to regulate for. But that's, that's boring. And we have something like a handful of or two handful of social needs, that structure our social interactions, they're basically priors in the way in which we want to interact with others, for instance, people might have innate need for status for raising, raising in a social hierarchy. And this is just a bias in the system reflex. And you can eventually replaced this reflex by something that it's instrumental to. So maybe you want to organize the world in the best possible sustainable way. And now you do not want to have power for its own sake, you want to have power according to your abilities and incentives to get things right. And so your desire for status gets completely sublimated by conditional behavior. And then you also need to have a bunch of social needs of cognitive needs that regulate exploration versus exploitation, so desire for competence versus uncertainty reduction. And they will structure the way in which you interact with the environment. And as soon as you will understand your environment better, these innate needs and their weights get replaced by something that is conditional again.

Joscha Bach 1:29:26
If you want to build an agent that is interacting with people in an interesting way, I suspect it's also necessary that this agent basically has something like an attentional idle loop, but it's looking into the world and thinks about oh, this is what I'm looking at, or there is somebody coming to I know this person should interact with this person in which they should or interact with this person. And it should be giving signs of all these processes taking place. So you need to understand by observing the system, which state that system is in. And you also need to understand that emotions have not evolved as a display of the internal state that you are in, but mostly in an adversarial condition. Right, as soon as we became social agents, and we're observing each other, we were using this observation to game others to understand how they are, to exploit them to control them. And this means that even small children learn to hide the emotional state or to quote the emotional state. And so when you see somebody at a funeral, you are not just looking at whether they are looking somewhere because everybody's looking somewhere at a funeral, but you're looking at what kind of somberness they are displaying, and how this openness is being achieved. And what this actually indicates, right. So you are looking at at least two levels of model of structure and the social persona becomes a puppet that everybody is controlling, according to their internal puppet, and the social puppet can become so dominant that people forget that they have an authentic structure behind it. So people have something like a core self that is childlike self, and all this machinery that they build on top of it, and they can get lost in the machinery. And it can be very difficult to reawaken the core self into something that is actually interacting with the environment is getting real. And these interactions are what makes it very, very interesting to interact with people, to which degree can you go beyond the social puppet, and interact with the core self, and establish intimacy.

Unknown 1:31:30
I love it. And I mean, obviously, we have multiple puppets that we have with different people. And for social bots that we like had that last cohort of Alexa prize for best cat, I don't know, maybe one, where he fractured very small kind of puppets, that the that was really, really limited. And

Joscha Bach 1:31:53
so for instance, I think that gender is a good example, gender is in some sense, such a puppet, it's a costume that be there. And it's socially constructed in the sense that gender is a model of what other people think of who we are. And if you confuse this puppet with your core self, right, you go into all sorts of contradictions, because you can no longer get your models to converge because you think that your puppet is immutable? It's so tender becomes a question that you are unable to take off again.

Unknown 1:32:21
You're sure I have a question I want sort of go back to the first cell that sort of gave life to if we look at how cells behave, they're behave very intelligently. It seems like they already have this built in intelligence, and they know to what kind of tissue they have to split and like become, do you do agree with the statement that sort of like intelligence already in within the cell and this sort of, it's sort of like expanse, it's not like intelligence only in our brain, but intelligence in our body in like, on the very core

Unknown 1:33:02
of our existence.

Joscha Bach 1:33:04
When I was a kid, my grandparents bought a chess computer for me. And I was bored by this chess computer, because it didn't perceive it as intelligent, it was able to play chess much better than me unless I would turn down the difficulty level. But the it was a toaster, it was not able to do anything but to play chess, which means that according to a simple function, it was calculating the move that most probably would let it win the game. And it was able to do this better than me. But it was not able to make any kind of new model of the environment. And to me, intelligence means that you're able to model something new, that you're able to solve problems that you couldn't solve before, that you're able to generalize, and so on. And this modeling capability is also something that probably includes the individual cell, the individual cell has the operating system, the genome of that is encoded in the DNA of the cell that allows it to decide, but given the environmental conditions, what to do best. And it's either going to differentiate into different type of serve, which means it's going to turn this operating system into a different configuration, or it's going to divide into two cells, or it's going to regulate something or it's going to die, apoptosis, the voluntary death of the cell. And this, these are the mechanisms that are available to the cell, right if this is all it ever does. And in order to make that decision, the cell is able to integrate over its inputs. And it can integrate by looking at the configuration when a neuron can look at the configuration of electrical impulses that come in. And this integration happens in time and in space. But there are very tight limits on how much temporal memory the cell has, how long it remembers that it has seen a certain thing in its environment, and how well I can predict in the future what it should be doing. And so the functions that the cell can learn are very limited. And we can probably quantify this what it can do, and I don't know the exact modifications, and that no number of people have given me their estimates, they're vitally diverge. But the cells don't seem to be universally intelligent. There seem to be, of course, they can learn. So there are intelligent to some degree, they can make models to some degree, but the individuals seems to be extremely limited in what it can model.

Unknown 1:35:34
But we can say the same about people. Like, if we look from perspective of universe of like multiverse, and like, what is out there, like, there's so many things that we don't know, and like our memories also short. So from someone's perspective, we can be also very limited intelligence, right? It's not necessarily like this super generalizable intelligence that knows everything about all.

Joscha Bach 1:36:02
So many of you can come up with the laundry list or some kind of achievement hierarchy of, of minds. And so are you able to learn at all, when you learn? Can you integrate over time and space? The features that you're learning over? To which degree can you do this? What is the complexity of the function that you can learn? And so the next interesting step is, is the function that you can learn Turing complete, of course, you yourself need to be Turing complete to learn interesting functions, because you want to associate arbitrary states with arbitrary transitions between them. But then you make your model can your model contain a Turing machine. For instance, when you learn that something in the world is an agent, you need to be able, if you want to have a model, that agency, construct an agent in your own mind, which means you need to construct something in your own mind that is your own complete, and not sure that cells are able to understand the agency of parts on the environment instead of just learning associations. So do you switch from correlational models to causal models and from causal models to complete state machines that are Turing complete? is an important step in the maturation of intelligent systems. The next step or maybe not the next, but one that is definitely important is Do you understand minds? Can you build things that are intelligent? And this is something that humans are about to do? I think so there is a probability that we are genuinely intelligent in the sense that we are able to understand intelligence in general, by building demonstrating that understanding. And I don't know if there is another stage after that. That is interesting, maybe understanding existence.

Unknown 1:37:46
So that there is you don't know if there's next level up to this sort of general intelligence, right.

Joscha Bach 1:37:52
So I wonder if there's anything left after that. That is interesting. That is not just scaling up.

Unknown 1:37:59
So basically, you believe that humans are capable to build generalizable intelligence.

Joscha Bach 1:38:06
We're currently testing that hypothesis, right? We don't know yet. But that's basically the idea of Turing that humans are generally intelligent, you should be able to build systems that can think.

Unknown 1:38:18
Thanks. Thanks so much. I think I feel like we should repeat it. Because I have a long list that I feel like I want to do almost two hours. Yeah, we would really love to have you again, your show. Thanks. Thanks a lot for your time.

Joscha Bach 1:38:35
Thank you. It was a lot of fun. Have a wonderful rest of the day wherever you are in the world. Thanks so much.

This transcript was generated by https://otter.ai