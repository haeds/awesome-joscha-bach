---
source:: https://www.youtube.com/watch?v=uc112kET-i0
---

Unknown 0:01
And we have your Shabak with us. And he's going to be speaking about AGI and agency. Okay, so agency in the age of machines, and you're sure it's a PhD. He's an AI researcher, who's worked and published about cognitive architectures for quite some time now mental representation, emotion, social modeling, and multi agent systems. And he's taught computer science, AI and cognitive science at Humboldt University of Berlin and the Institute of cognitive science at odds in the book. And he's got a book out, which has been around for a while now. It's called Principles of synthetic intelligence. Yes. So I won't belabor the bio, I think, are you working at Intel Labs? At the moment? Is that correct? Yes, that is current. Cool. All right. Yes, Your show is working at Intel Labs. There we go. Thanks so much for joining us. It's great to have you here. It's been a real pleasure to listen to previous talks as yours and interview in the past as well. So I'm, I'm I can tell everybody you're in for a real treat. Yes, so the talk title is agency in the age of machines. And I'll hand you over to your show.

Joscha Bach 1:14
Thank you. I work at Intel Labs is concerned with discovering the future of artificial intelligence and understanding how we can assess the next generation of systems. And what we are currently looking at are the different vectors in which intelligence unfolds and how to basically understand which systems occupy which regions in the resulting space. Today I'm going to talk about a specific thing, which is agency. Does this work? Yes, excellent. So the word spirit is an old word that is no longer common in our culture. But I think that in the context of agency, this has been the word that has been used before our time to describe what we want to have. The spirit is basically the virtual control system that gives control to query into an autonomous agent. So spirit is in some sense, the operating system of an autonomous robot. And when the world was invented, the only autonomous robots that were known are sales and plants and animals and humans and families and cities and ecosystems, nation states, and so on. And they all have spirits and sentience is the ability of an agent to discover itself in the world and its relationship to the world. And it's clear, not all spirits have sentience. But they're all agents at some level. And our own culture is confused about the nature of agency and of consciousness and sentience. Consciousness and sentience are not the same consciousness refers to a particular kind of control model, one that is noticing its own attention in the world. And that's a specific thing that you have, in addition to agency. And it's not clear that all systems that add agency need to have it, it's also not the same thing as heaven itself. The self is basically a model of what kind of agent you are. And it's a very specific one, one that gives you a first person perspective, it's one where you notice that the contents of that model are being changed as a result of turning your intentions into actions. And this is what the first person perspective and the self is about the self is getting an intermediate stage, I think it discovers exists between the discovery of your own agency and the deconstruction of the representations in the self. It's basically at some point, you notice what you're looking at is not you, but it's representations about you. And at this point, it stops being a self model. But most people don't get to that stage. So this first person perspective seems to be something that is almost insurmountable for most people. In our culture, we do not have a very sound understanding that you give to the kids in school about how this whole thing works. And consciousness seems to be so confusing to many people that they turn into materialists, then Mysterion ism is the philosophical position, that something cannot be understood. When when Chomsky doesn't understand it, Noam Chomsky is mysterious with respect to consciousness, for instance. And instead in our culture, a lot of people are instead dualists. And this means that we in some form or other think that there is a separation between the physical domain of things and another ontologically similar or equally privileged domain, in which ideas thoughts, feelings, emotions, and so on, play out. There are many versions and variants of that, and alternatives to this dualism that most people haven't informed The form even Most neuroscientists that I talked to seem to have this at some level is monist idealism, which means everything is a dream. And we live in this dream, and minus physicalism, which means there is a causally closed mechanical layer from which everything emerges in this is all there is.

Joscha Bach 5:18
Right. And I think that our culture has this confusion, the stool is confusion due to the religious indoctrination that we have. And it's something that we do not find in the same form in some other cultures. So for instance, when you talk to people in Japan, or sometimes two people in China or India, and our culture, I think, has known this at some point and how to put this together, and it just got lost. And we found this documented and interesting text, which is Genesis one, Genesis one is being used as part of the Christian don't know tree nation, and typically takes the place of telling pre scientific people about the creation of a physical universe by a supernatural being, at least that's the modern interpretation of what Genesis is about. Right? And the way in which Genesis does that is completely unscientific. And it's also philosophically and sound. Because what's been described is that there isn't God who hovers over the face of the water somehow, and then it creates light. And then it finds that the site is good. And then it creates a firmament and separates the waters above the firmament from the waters below, the firmament really doesn't make any sense at all. And then it creates all the plants and the animals and gives everything their name, all these things, you know, none of these objects, it's a physical object. We know physics, it's quantum mechanics, right? It's VHF, it's math. This is not plants, and animals and names, colors and sounds, colors, sounds, names, plants, animals, people, they are not in the world, they exist inside of your mind, they are creations by your mind to make sense of the patterns at your systemic boundary. So we know that people know this right? Took us very long time to figure this out. Did it need a lot of science to figure this out? Did we actually need to do experiments to figure this out? No, you figure this out. But actually thinking about how the state of affairs works, we realize that the math is not the territory. It's not that hard to see. What if the ancients were able to think straight? What if they had actually good philosophy, so let's reread Genesis one. And I think it might turn out to be a six stage theory of development of consciousness and personal self. By the way, the picture that you see to the right has been generated by Delhi to the new open AI transformer model for the substitution model for generating images based on textual Trump's and the prompt of generates image is the emergence of consciousness from the universe automaton. So in stage one, we have the word Elohim. The word Elohim is the creative spirit. It's something that can create stuff. And what it does that it creates the domains of the world. And it creates the domain of abstractions ideas, and this is other domains that our ancestor Decart describes as wrist extensor and risk aka density, a domain of stuff and space, and the domain in which thoughts and ideas happen. Right, these are two domains that exist in the mind, but out there somehow, at the level of physics. And this whole thing starts this creation process with the creative swearword hovering over the substrate. And the substrate is to vappu. It's without form and void. Right, it's a completely chaotic substrate that the creative spirit hovers over. And we have now discovered that this card acts abstract to our best current understanding is the neural architecture of the brain. And the first thing that the creative spirit manages to do it manages to spark light out of these neurons, basically, it sets them into oscillation, and some of this oscillation creates a measure of intensity that is possible to be separated from its absence. So basically, you draw out this intensity from the darkness and what you get is contrast. And then you associate this intensity with the color of the day. This light is brightness, and the absence of it was blackness. It's the color of the night. This is stage one. One stage one is done.

Joscha Bach 9:40
What you do is you create space from the substrate you do this by arranging the countries along an extension. And you can find discover operations over this extension. You can for instance move mathematically we call this moving alone extension addition. You can zoom in To our to call this multiplication. When you really look very hard, you also can discover the relationship between addition and multiplication, which is the exponentiation. If you are defining extensions in multiple directions, you can also discover another operation which is rotation. So now you have the basic primitives of moving stuff in space, and creating shapes. And what the creative spirit is doing after it is covered multiple dimensions. It it separates spatial regions, and it creates a division between the domain if it's going to describe the world from staff in space, and ideas. And this is stage two. And stage three, it is structuring the space into 2d and 3d, and it creates the ground plane from 2d. And on top of the ground plane, it puts a space in which things can be put, and the things that it puts in are the solids and the liquids and the organic shapes that it discovers how to model and to create and to manipulate. So what you now have is the basics of a game engine that you can use to model the regularities and the patterns of your systemic boundary. And it states for creative spirit discovers the irregularities of illumination shapes remain constant when the light changes, so you need to tell them apart. It just covers the light sources. And it puts the big light in the sky during the day, which is designing and explains by things are visible during the day and not during the night. And weather has changed over time. Stage Five, it starts using this game engine to construct variation of plants and animals and people and starts to name them. And it learns a language to do that. And it's stage six, it discovers the need to control and interact with this world. And in order to control it like with this world, it needs to have a model of an agent that performs that interaction. And so it proceeds to create this agent and it creates this agent in its own image in the image of this creative spirit, but not as the creative spirit. But as men and woman as a person. It's something that thinks it's a human being. And what you observe when you have children that at some point, between the age of about one and two and a half. Its locus of self construction shifts from the creative spirit into thinking that it's a person that it starts to talk from in the first person at some point. And around the same time. But you also notice is a break in its memories. The toddler has perfectly fine memories and is able to report on them after learning to speak. But at some point, they don't remember what had happened before h1 and a half or h2. And I think what might have happened is that they basically construe memories from this new perspective. And the original creators were worried that they had been gets relegated to a perceptual module that helps them with constructing solutions to problems occasionally, but they identify as a human being that lives in this game that is created in the brain. And it takes a long time before you discover again that the world that surrounds you is one that has been originally created by your ancestor by this creative spirit that lives in your own brain. So I think that's a pretty sound theory that is pretty congruent with what we figured out in AI. So far. The pictures that you've seen were mostly not all of them generated by

Joscha Bach 13:37
the Delhi, which is not an AGI yet. But an interesting point in the development of understanding what an agency this strategy of discovering modern agency had to start again from scratch because you broke our civilizational spirit when the Christians destroyed the insights that you had an antiquity and then we basically broke the Christian epistemology was the enlightenment and it was hard at that point. And so modern science had to reinvent all the terms from scratch. And we started I think, the psychophysics disco stuff, thickness and hammer and hammer its work and then this very important experimental psychology and then they discovered cybernetics theory of control and feedback and building systems from them. And then artificial intelligence took over and pushed out cybernetics because Minsky didn't like it, and turn everything into computation. And then we had to replace parts of artificial intelligence with cognitive science, because other people didn't like the idea that we could be computers. And we had to pretend that we are studying the mind without doing computers for a while. So we started annoyed new discipline in which different fields came together. Linguists, neuroscientists, AI researcher, philosophers and anthropologists into turned out to be a shared race for the same kind of funding, but very little overlap between the actual disciplines. What does each agency emerge over? Coffee, Justin has a very beautiful theory that has the benefit of being super elegant because it's a simple single thing. And it's the search for free agency, which basically leads us to building predictive models. And the AI systems that you have seen, like Telly, and so on, and GPT, three and so on are interesting in that what they're doing is they just are systems that are in some sense, taking, for instance idea to the extreme, what they're doing is they minimize the predictive error. They are autocomplete algorithms, they try to identify the future from the past just by minimizing a prediction error on some level. And, of course, there's also contrastive principles. So you basically try to do this in both direction that you try to associate the similarity between a label and an image and the dissimilarity that you have between them. But are these networks slightly conscious, famous open AI researcher tweeted recently that he thinks that it might be the case that today's neural networks are slightly conscious, and it created an uproar of people who mostly disagreed with them. It seems to me that the Noble Eightfold Path to make these transformable slightly more conscious is to do the right feature decomposition, the weight loss function, the right output generation, the right update rule, the right prompt, the right model size, the right attention model, and the right attention control, this is what we're currently doing. But it might be that this is not the right way to do it. For instance, an issue that the current generation of monitors, despite the amazing abilities, is compositionality. So if you ask them to to generate a blue cube on top of red cube presets, smaller yellow sphere, these are some of the things that you get.

Joscha Bach 16:55
Right? I think this one down here maybe comes closest, but it's orange, or not, the yellow one is not always getting it wrong. But interesting thing is that it's getting it wrong most of the time here. And it's fascinating thing has seen 400 million images that are annotated, and has discovered structure, and it has discovered so much structure that you can ask it to draw a bunch of teddy bears that were AGI research on the moon in the 1980s detest that it looks awesome. But why is it failing as these things still. And first of all, taking 400 million pictures is a lot more than what a human artists does, before they learn to do things. Even if you count looking into the world into different scenes in your course of your life as frames that you're looking at. Right, you're not going to look at 1 million scenes in your life, you don't get that old. And so it's not very simple efficient to converge to this. And it also learns in somehow in the wrong order, it starts with learning some basic structures, and it gets to syntax, then it gets to style. And semantics emerge as the long tail of learning style and these models. And our learning is very different. You start learning stuff based on some kind of relevance. And then when you learn language, you learn syntax, also the syntax of visual primers later on, after we already have semantics, and the semantics and syntax cool, we will for a while. And so we start basically just doing semantics and the style is the long tail of the perception that we have. So human beings learn in a completely different way. And I think that's because they're not driven, but just by the minimization of predicting error, but their own agency they are based on control. And the monitoring that had to happen in the service of control. You'll know what the controller is, this is the basic canonical structure of a controller is often being described in the context of cybernetics, the controller is an effector and a sensor the sensor notices a difference from the way how things are, and the system that's being regulated gets manipulated by the controller. So, the effector to minimize this difference between the setpoint deviation and the sensors measurement and the environment exists as something that affects the regulated system by disturbing it and so, the controller has to constantly or regularly mess with a regulated system to account for the disturbances. And to do this, it needs to if it wants to be effective to model the environment to some degree. And a simple controller the canonical example is will be the thermostat. So the thermostat is measuring the difference between what the temperature is and what it should be. And if the temperature is lower than it is or what it should be the thermistor attached turns on the heating and if it's as warm as it should be, it turns off the heating again. And so this is a very simple control circuit, but this control circuit is not an agent. How do you get from this to an agent? Let it melt, control the temperature of the future. Basically what you do is you Give the thermostat the ability to anticipate future differences in temperature based on its current actions. So and then it starts to learn the difference. Now, what thermostat is optimizing for is not the temperature at the given moment, but the integral of the setpoint deviations of the future. And this means that if the thermostat decides to turn the heating now versus five minutes from now, they're going to be different trajectories of temperatures in the future. Now we have a branch and universe, and the thermostat can decide which branch it's going to prefer based on its model. So it will get better. If it starts making such models of the future, it will be more energy efficient, controlling the future and so on. And it will try to have basically beliefs about the current state beliefs about its relationship of its actions to future actions. If the left believes about the future, it will have intentions and it will turn these intentions into actions you have a full blown agent just by giving an agent the ability to control the future. And if an agent becomes very smart, it can start to model the environment in very intricate ways, including itself so it can discover the nature of its sensors and the factors the inaccuracies of its sensors, additional kinds in which the sensor interferes with heating, because it's put next to the heating. And it might even discover that when it makes things too hot and short, short amount of time somebody will open the window and heating will become less efficient and so on and so on. So that it's possible if you give that thing, enough, entanglement was developed and if you give it enough ability to model the world adapts that will discover itself in this world and eventually become sentient.

Joscha Bach 21:42
This idea of cybernetics was thrown out because the people at MIT discovered how to make computers have usually successful and eventually, this notion of agency was rediscovered, for instance, in Minsky Society of mind where he had this idea of basically reconstructing agents from neural networks that form an organization and eventually all sorts of models that interact with each other. The difference between computation and cybernetics is quite interesting computation is built on the idea of a state machine and deterministic transitions between states. And what Turing has shown after goodness failure to save this semantics of classical mathematics. And we had to throw out continuity that the state machine is equivalent to what mathematicians have been doing done on a lot betweens, the state machine gives you all of constructive mathematics and constructive mathematics gives you a domain of all languages in which you can make computable functions. And the dynamical systems that existed in cybernetics are in part built on continuous functions, which is not a big problem, because you can always if they are useful, turn them below the hood into something that the real numbers actually all integers. And you'll get your results after finite number of steps. But the angle in which you look at it is a different one. And state machines look at discrete functions in dynamical systems, you tend to look at geometries and state machines to look at composition dynamical systems, you look at entanglement in state machines, you have logic and dynamic physics, you have feedback, and state machines, you look at modeling and dynamical systems, you look at control. But there is no continuity in nature. If you zoom in, you'll realize that geometry is just too many parts to count. There are no discrete state machines later, your computers are built on some continuous systems, right? That you just tune in such a way that they look discrete to the logical language that runs on the computer. So in some sense, dynamical systems are discrete computation and practice suppose abstractions, and abstractions of the same ontological phenomenon. And controller modeling turned out to be two sides of the same coin, there are just two ways to write down the mathematics. Cybernetics itself was not that one of the most interesting attempts to build a cohesive cybernetic theory of representation that exists today might be Steven voiceworks, adaptive resonance theory spoke conscious mind resonant brain, something I'd recommend if you're interested in that. And basically, he had some ideas on how to neurons compute minds in the aggregate. But I mean, you all do if we look at this idea. What we see here is some neurons that light up we see how the activations are spreading through the axons and are affecting other neurons. And then eventually, they form large scale structure in which somehow computations of mental states take place. And the original interpretation of what's going on what's happening here is that humans are basically circuits and the activations of the circuits are basically the activations that are associated with the nodes in the graph and the links in the graph, transport this activation into other nodes. And they're being summed up there and weighted against functions. And this basic idea is the perceptron by Frank Rosenblatt that is still at the core of our neural networks today. And back then, then Frank Rosenblatt invented the perceptron. He, I think, already anticipated, deep learning, as you can see here, and the organization of a perceptron. And he did this before people had discovered to train these things with Stochastic gradient descent, this rule that is mostly still used today. And because they hadn't discovered that, Minsky could show that the existing algorithm couldn't learn many things. So the algorithm that Frank Rosenblatt and others had at the time, and Rosenblatt actually implemented this with hardware, he did this before computers were available. So they did this really, with mechanical contractions and chemical systems and so on. It was a really mechanical interpretation of this, it could be used for actual computations and for actual learning, but with the existing algorithm, it could not learn for instance, XR was not able to do the decomposition of arbitrary arbitrary functions. And Minsky and Papert wrote a book that basically obliterated funding for connectionist research for more than a decade, and set back neural networks for quite some time. This was not Minsky's fault, arguably, because how would he know that he didn't see something. It was the fault of the funding agencies who interpreted Minsky in this way. But it's not that Minsky later recanted and told the funding agency, they should give the money to the neural network people anyway. So I think there was also sometimes politics involved.

Joscha Bach 26:46
The neural networks today are immensely successful. And this idea that they form circuits and so on has been explored by open AI, they've written an excellent series of articles under the distilled pub, you will arrive at a look at lots of, for instance, Vision neural networks and discover their similarities. And when you read this, you will notice that you will networks are far from being black boxes, that they actually understand what they're doing, and how they are able to decompose the world into features. By representing everything as weighted chains of sums of real numbers, and how these features represent different aspects of the reality that are being super imposed. You can basically replace once these features have been learned by function to construct yourself that have the same property and you get the same outcome, if you can show that this works and why it works. So if you ever get told by somebody that you will networks are a black boxes, this has not been true, I think, for a pretty long time. And so features in the interpretation of Chris olas work are the fundamental unit of neural networks. And they correspond to directions in an embedding space. And they're connected by weights from circuits. And there is a universality condition. That means that if you have a model that is architecture that is powerful enough, then anomalous features, will form circuits that are have none of its functionalities, of course, different models and tasks. There's, of course, a question whether the circuit metaphor is wrong, and whether our new neocortex is actually made of circuits. For instance, when you look at a new representation in your cortex, by measuring the activation of cells, and you look, the next day, sometimes the activation patterns have migrated or they've even rotated. It's not obvious how this would be done in circles, right. And there is some confusion at the interface between many groups at different schools of neuroscientists, and also between neuroscientists and AI researchers, but the relationship between neural networks and the neural mechanisms in the brain art. And I suspect that the correct solution has to do with understanding the self organizational structure in the brain. So it's the difference between technological systems which have a functional design, let your networks have a functional design, and biological and social systems which use a meta design. And biological neurons are self organizing agents, right? If you look at them, they are single celled organisms, each of them tries to survive by itself. The only reason that you can survive is that it gets fed by its environment. And the environment is measuring the performance of the neuron when it fits that and so the new world better behave. And it's linking up with other neurons to solve this problem of survival and discard. And it does that by looking for features patterns in its environment that give it an information about when it should fire. Right, so the individual neuron is looking for a spatial temporal pattern of information, a wavefront that comes in that tells us now you should fire now you should send a signal and if it's successful doing that, it can exchange some of this info Question was other neurons. But it's also collaborating with other neurons in sending these activation patterns force. And the overall active interaction of the neurons is integrated by the brain to distribute reward across the neurons to tell them that they're doing that and keep them on track.

Joscha Bach 30:18
How is the organization in the brain forming? Right, you cannot principle you could put everything into the genome. But the genome is not that very long, it just fits on a CD ROM, and a very tiny fraction of that CD ROM, we don't know how large it is codes actually for the formation of the brain and for the difference between humans and other cells. So there is a very sparse representation for what the brain is doing, how can this space the other structure of the brain that is crucial be represented in such a sparse form, and also such a robust form that it works even when there are developmental defects, or when the environment is quite different that you need a different brain organization. And the solution to this might be Neural Darwinism. The coin, coined by an idea coined by Jerry Edelman, the idea is that what happens is that there is an evolutionary competition between different modes of organization in the brain, something like you have a society that doesn't know what it's doing, the people don't have a government yet. And then there is a different competition between different forms of government that are being invented and compete with each other and fight wars with each other and become more complicated, get one more layer, and prevail and have revolutions and so on. And this is a very efficient way to set up the design, if you will only send up the boundary conditions of the evolution, right. And you can do this arbitrarily tight. So this, this evolutionary competition is going to be rigged, it's going to have basically the same output, basically every person, right, it's the outcome of the evolution is going to be pretty much the same, but you only need to leave a few cues in the system to speed it up. So you can get an entity quite efficient if you basically just create a quick preconditions for an evolution of possible mental organizations that make it necessary for convergence of that thing. And it gives you the benefit of some mutation that affects the way in which your brain is organized, you will just turn out to be inert. You still get to full organization, you even can get to enlightenment, it's just going to be different paths than the normies. So how is the difference? Between this technical design, a technical design works from the outside in biological and social design works from the inside out. And to understand this idea, we have to look at feedback loops. An open loop basically means that there is no loop means you just write in your environment like industrial robot that has no sensors, you take up this part from the conveyor belt, put it somewhere else. And if somebody has moved the bin in which you're supposed to put the part, you're going to drop it on the floor, right? This is what an open loop is about. But if you give the robot arm feedback, then it's able to adapt to slight changes environment, even to quite large changes in its environment. But the type of system that we are, we have an extending loop. Basically, what we do is we build looks into an environment, check, which part of the environment yields to our control, and then we integrate it into us. And that's the reason that why for instance, when we use a car, they use to drive a car, the car gets integrated into our body schema, right? When you drive the car, you notice by that is in motion, that is extension of the dynamical system that you are, you notice the distance between your car and the environment quite effortlessly. You notice how the tires touches the ground, and it feels like part of your body. And if somebody crashes into you, they crash not into your car, they crash into you, right? You feel this, how they crash into you. And it's quite fascinating is what basically happens that your body is inside of your car, and it's becomes they become integrated. Your mind has a resonant model of your environment. And if you can create this instant resonance, then you will have this impression that is part of your body. How can you build these resonant loops? What is the minimal system that can create these loops into the environment? And what your basic need is something like a coercive. And I think it's tempting to think that our consciousness now well, mind itself, is actually simple, that the heart thing is not consciousness, and the observer that is attending to the fact that it's attending. But this notice notion that you are attending to the fact that you're attending and thereby establish that you are an observer is actually algorithmically instructionally. Simple. What's hard is perception. What's hard is learning What's hard is the overall architecture that you have this thing to see that is growing and that imposes more and more order on its environment as it grows is actually quite simple. Right so we are back at our creative spirit is original spark that is taking in the observing the tool bubble around it, that formless void of the neural substrate that it's having codec patterns, and it pulls feedback loops into it and creates order. It's basically like an attention hat in a transformer only. It's integrated with itself all the time. And it lives in an resonator that is dynamically entangled with changing universe. So if you look at the difference between the Inside Out design and the outside in design,

Joscha Bach 35:20
if you design a system technically, you start out visit controlled table Workbench lab, science, scientific discipline. And from this deterministic known environment, you identify a substrate that is still formless void on which you can write in which you can create, and you extend the known world into the substrate. Right, so you basically you close the gap and you vote from the outside in that building this new thing that you add to your lab to your document, or to your science. And biological systems do not work like this around them there is in indeterministic environment there is with liveness. And you know, within our discipline is like, and in order to become a tree, you need to start out with something that is not a tree seed. And the seed is colonizing its environment. Colonization means that you impose an administration on the environment that replicates your own structure into the environment. And in order to make that happen, you need to extract more energy from the environment than it costs to maintain your administration of that environment. But this is what it means to build a colony. And if you exist in nature, you also in competition with all the other colonies that exist, right. So in order to survive, you need to be able to defend the onslaught of the other colonies that are going to try to extract the Nick entropy from the same volume of space as you're occupying. And this is what limits the size of an organism, right, an organism needs to be able to maintain chains of command into the environment to maintain this colonization by the principles that define the organism. And one way of doing this, if you want to grow with the size of the cell that has a clear cut boundary to the world beyond which it knows what is happening, and outside, it doesn't know what is happening is to take multiple cells of the same type. multiple cells have the same type know what they are like, right, so they can link up the cells of the same type, and form some self organization. And together colonize the world by creating more of cells, so they're copying themselves create more of the same kind. And in this way, they can create a branching administration, there's of course, a limit to this administration, most organisms have a certain limit over which they can maintain the logistics chain. So they can coordinate the very few organisms which have cracked the secret of spreading beyond very small organism in which basically, can scale almost indefinitely. For instance, there are some trees which managed to clone themselves and stay connected, where there is a dwarf and species that has been able to colonize a large part of the planet as a single colony. But the price for this is that they stopped at a stop adapting, because if you still evolve, if you still have changes in your genome, it will mean that some parts of your super organism will go out of whack with the others. So the reason why not all organisms have evolved in the shape where they can go indefinitely is because the benefit of remaining at some kind of medium shape is that it's much easier to adapt, because you can form units that are different than the others and you can evolve them and you can select for those that work. So if you look at the causal systems that can exist. The interesting system features here is the agent and the agent is a subset of our control system. It's basically the controller, the sensor and the effector, together with the setpoint generator, and a model of the future that has been generated by the controller. So this is the canonical shape of an agent. And to control the future, you need to be able to make a model of the future. That means you need to create a world that has counterfactual causal structure that is different from this causal environment that you currently live. But you need to create your own hypothetical universe. And to make such a simulation universe, you need a Turing machine. So the simplest agents in nature will have a Turing machine in them, otherwise, you cannot be an agent because you cannot control the future. And the simple system that we know in nature that contains a Turing machine is the set A that literally as read white tape in it. And the cells in this way able to make models that are decoupled from the substrate of the cell that are the variation, some causal closure, that the model doesn't know what the environment outside of the model looks like. And so the agent can use that model to entrain it with causal structure, arbitrary quasi structure that you can use to model counterfactuals like the future.

Joscha Bach 39:55
And so mechanical components are not agents. They're just systems that have to randomly self assembly or that use an outside InDesign an external force. And they have no capacity for adaptation controller are resilient, they have attractor states, but they cannot adapt to new environments. Agents decouple their computation. And they are able to anticipate disturbances in environment. This allows them to adapt, and group events that have an individual motivation. They form the group by usually via reputation systems. So they're basically coordinated attraction, the navigation of getting the world in such a way that the interaction is to the benefit of the individual. And when this is basically to the mutual benefit of the individuals, then the group becomes more powerful. But the problem is this this group agent, that sometimes the local Nash equilibrium of the individuals is not compatible with the optimal good of the group. But for instance, imagine you live in a village and you will always put on your garbage on the street. This is maybe bad for the village. But it's a benefit for you if somebody else takes care of that garbage. So if everybody just acts on their own benefit, the streets will be full of garbage. How do you prevent this you will form a government and the government is an agent that exists to change the payoff matrix for the individual agents in such a way that the Nash equilibrium becomes competitive as the common code, which means basically, if you slow your garbage on the road, that the government is going to punish you, which means it makes throwing the garbage on the road, expensive enough that you will take care of the garbage, it maybe you can pay somebody to collect your garbage for you. Right, this is the benefit of the government, it changes this payoff matrix. And it's in your own best interest to pay for having a government because it's going to create a world that is much better for you, right by also making the other space by imposing common rules on the system. The government is not bad to you, because it forces you to put your garbage today, the government is ideally good, because it creates a world where there's no garbage on the street, which is more useful to you then about which way you can dump your own garbage as the only person. And as a result, there is no time to average and everybody else has to write. So this is the benefits thing, you need to have something like state and the state means that you are defecting against individual units vendors into the interest of the better of the system. The solid state building basically means you have an organization that is also willing to arbitrate against your friends, or against itself in principle, or against parts of the system, by making sure that the global good is maintained of the system, that's the purpose of a state. And so a state is going to have hierarchical governance. And you can derive the nature of the state and its optimal organization from cybernetic principles, basically, there's always a right level for control. And a state will have to have an immune system that defends itself against agents that do not submit to this hierarchical governments. And the state has to limit the autonomy of the sub agents. And if you want to go, for instance, beyond a simple state, a tribe, for the individuals to know each other, and you want to build a state that can stay indefinitely, then you cannot maintain this visit and reputation system. So basically, you need to build something in each individual agent that makes them behave, even if nobody is looking, even if they get into a new context with strangers. And this is going to limit their abilities and evolutionary drift. And it's quite interesting to look at humanity because it's at this boundary between the tribal agents and the infinitely scalable building agents, human humans are to a large degree domesticated they have built something into them that makes it hard for them to defect, but not all of us have and our societies are being built. So that tension in a way. So if you look at the Society of mind structure of the agents in our own mind, that we can also use to understand how to build machine minds. Minsky has looked for instance, at the modularity of the functionality that is implemented by hierarchies of dynamical features which are actually operators. So the operators the change, the interaction was the vote. And, for instance, you suggest if you can create actions by organizing our behaviors into hierarchies that get more and more complex and then combine the sub goals of these actions into long term goals and you basically have alternatives and conjunctions of steps that give you scripts that you can execute. And can obtain these scripts by having self organizing agents that coordinate with coordinate with each other until they form something like a society in your mind in which the agents are being activated and played with each other according to the tasks at hand.

Joscha Bach 44:53
And you will have to have special agents that are Evaluate the behavior of the other agents. And what they have to do is to have to figure out the differences between the current situation and the goal situation. And these differences have been made available to the system, and lead other agents who decide whether they should become active. And the interaction between the agents is also managed by specific agents and Minsky called stem K lines. And these knowledge lines are basically they form their own society of mind. And this is the government. So you have a case society and an S society. And the society of the government and the society of the other agents interact with each other. And eventually, you can basically turn this into introspection into a brain and the brain. And the B brain. This isn't the a brain and it's optimizing the learning and actions of the A brain and the brain is what acts directly on the world. And the self is basically a model that emerges in the brain, it's a model of the own aggregate agency of the system. And it's downstream from the set point deviations from the motivation. So when you look at your own self, you don't model notice for the most part, when you create your own motives. Instead, what you notice is urges, and you channel these urges by directing the attention. And it shapes your own agency via identification, which means the self contains the model of what you care about. There are two ways of modeling reality and measure of reality. One is, you might have an absolute value. And the other one is to measure the difference between how things are and how they should be. And this creates a discrepancy that we could call an identification is where you're saying, This is what I'm supposed to regulate. This is what I'm supposed to take care of. For instance, you might notice a mismatch between how justice society is and how justice should be how balanced it should be in its social equilibrium. And you realize that you get an action urge out of this, and you need to change this, right. But objectively the world is just as it is. And it's the result of many attempts to change it in the past. So this impulse is something that motivates you. But that's not really explained the truth that exists out there in the physical universe. Instead, it's something that exists upstream from yourself inside of your mind. And that changes yourself into something that is serving these purposes. What is the role that consciousness plays in this key idea that, for instance, traditional NGO has is that the purpose of consciousness is to get into this dynamic model of reality that we form and find solutions to problems that cannot be found automatically. The automatic way of finding solutions to perceptual problems is basically a stochastic gradient descent, you follow a gradient until you reach an optimum. But sometimes you cannot find a solution to a problem by just following the gradient. In that case, you need to construct a solution, constructing a solution means you need to create some kind of low dimensional function is that is often compositional, some kind of script. And then you need to modify the parameters of that script until it works. And you will have to remember what you tried and why and what worked and what didn't work. And so you basically to create an index memory. And Vangelis idea is that conscious basically a prior, that there is this low dimensional discrete function that allows you to modify the overall state of your model. So it snaps into a low energy energy state with this basically means is, there is an optimal description or nearly optimal description of the world that requires a relatively small function. And you've discovered this function using construction by consciousness. And consciousness is aware of the task because it needs to have a memory of its own actions. Right. So the consciousness is something that observes itself, performing manipulations on the mental substrate, because some things require such a manipulator. And so you have an attention agent that is basically the conductor of your mental orchestra. And you have a motivation agent that creates what you care about. And then you have lots and lots of perception agents that interact with the world, and coordinate the motor behavior of the system. And if you turn off your attention, that is the integrated attention that tells itself a story about what it's doing, then you become a sleepwalker. And when you will slip talking in that state, the orchestra can still play, you can still play without playing it paying attention, but it's basically just full free jazz. And if you ask the sleepwalker what they're doing, they will give out a goofy answer. But it's not one that is a coordinated physical human story of what this agent is currently doing in the world. Because the character is dreaming. Dreaming means a state that you are so far disentangled from the world around you that the representations that you are acting on, are not representations of the world around you, but the features of the internal system dynamics.

Joscha Bach 49:57
So the role of attention of consciousness This basically to prevent you from dreaming, it's to create a coherent story and make sure that this coherent story is in sync with your perception of what perception is available. If you shut out your perception, then even your conscious mind, even if it creates its own order to drift into dreams, the same circuits that produce dreams during the night produce them during the day, right. And normally, these dreams are tuned to the sensory data. And if you tune out sensory data, for instance, during meditation, you can create arbitrary dreams in your mind and follow them. Dreams are very useful, they can be used to augment the training data that you've seen, they can create new perspectives, they can vary the known parameters, you can see what is coherent, but the data that are generated been in the dream and not observations of the real world, they are extrapolations over the known observations of the real world. Part of the role of consciousness beyond things control model of attention is also that it supports distributing knowledge throughout the mind. To do this, we need to discover something like a language of thought. And this language of thought is enabling our own society of the mind. There is a similarity between the organization of our mind and the organization of our societies. The human intellect, basically, the mind is the thing that observes the universe and creates a structured models of it, and the neurons and neurotransmitters are the substrate. And the working memory is the current binding state of the mind, into a model of the scene that we are in reality that we are in the frame that we are in, and to serve as the identification between what we think we are and what we want to happen. And consciousness is basically the contents of attention that makes knowledge available throughout the mind. The civilization intellect is quite similar in its structure, the society is the thing that observes the universe. It's a subset of the society of people and resources. The current generation is the binding state of the society. And the culture is the identification is what we think we are and what we want to happen. And media are the contents of our attention and media make the knowledge available throughout society. So in some sense, the culture is the self of our civilization, the media is the consciousness of our civilization. And if you look at it like this, it seems that our culture basically currently has a seizure. And because our media is not really helping us to understand society, as it is there is a disconnect between media and the cloud tools. They call that state post modernism, right and modernist society, everybody is obsessed with interacting with the ground tools and having agency based on the way in which the universe is to model the future. And when your models of the future move faster, then you can track them, because the future changes faster than you can model it, then you stop having model of the future, you lose the plot. And you basically live in a society in which you no longer have a model of the future in which you do not plan ahead, plan ahead past the next kind of generation. And this is a very concerning state to be in. It's really one therapy that either precedes our to have one in that precedes a new form of organization in which again, you're able to discover our agency and model the future. And before this happens, what happens is a singularity, it's a state that you cannot predict. It's one day, we don't know what's going to happen next, because the current models do not allow you to see what the state is going to look like and what kind of structures what kind of eigenvectors in the new space of possibilities will emerge. So what we've seen so far is some stages of intelligence agency starting from the regulator, which is a feedback loop and the predictive controller that models the future. When you combine it with its own motivation, its own set point generator, you get an agent that can discover that it has a self and when it discovers itself in the world, then it can also become sentience, which means that it understands what it's doing. There is another thing that is missing here, and this transcendence. Transcendence, I think has to do with leading up to next level agency. You probably are all familiar with Maslow's pyramid of needs. I started to encounter it when I was trying to model cybernetically the need structure of the human mind so I could build artificial agents that mimic human behavior. And what I realized that this pyramid does make a lot of sense because first of all,

Joscha Bach 54:43
these are not needs. These are they don't have a need for self actualization were a need for self esteem. This is too complicated for the organism. These are models that I have about what I'm doing. These are purposes, right? self actualization is a purpose that I can follow. It's not only Basic need the most of these things are actually way too complicated to be direct needs of the organism. Right? So purposes actually are models of our needs. And when we act, we don't act directly on our needs, because our needs often do not create pleasure and pain in the year now, the Act basically on anticipations of pleasure and pain in the future integrated over long time spent. And this gives rise to things like self actualization, right? self actualization means that you can create an environment that optimally serves your needs. So it's a high level abstraction of your purposes. And in this way, if you try to understand our needs, what we see here is you have basically the immediate organismic purposes, you have the relationship purposes, and we have the purposes integrated of our own lifespan, which this is what Maslow talks about here in this pyramid, right? It's either two or three categories of purposes that are organized in this hierarchy. Also, what it turns out in the individual is that it's actually not the pyramid. Because our individual purposes, though relationship purposes and our immediate organismic purposes compete at each point, it's not that you wait until you have eaten until you care about justice, sometimes you start a revolution, while you're still hungry. Right? It's not just because you're angry. Sometimes purposes are of other levels are displacing the purposes of all the other debts, they have different weights. And these different weights give rise to different personality structures, and so on. But they do not form the hierarchy. They coexist these needs, and the purposes that result from them. But there is something that goes over all of them, that's more important than all of them. And that's the purpose is above the ego, the stuff that is sacred to you. And all the other purposes, when you discover the sacredness are instrumental to this. And this is basically the agency that you think you are serving, that you are part of that gives reason to your own existence. And it's not that all people have something that is sacred to them, the people that do not experience sacredness of any kind, call them sociopaths. Some people think that's a pathology. Other thing that's the Free State, where the other one is domesticated, probably depends on whether you're asking a domesticated human being or a sociopath. And then you have something that is sacred built into that basically, its purposes above the ego, things are more important than your own organism, you have possibility to interact with other agents based on that shared sacredness because you can give them resources, if what they do is in the surface of these higher level purposes above the ego, do not expect anything in return, because it's the larger agent that is being creative. So your interaction, the important thing to realize is that this other agent does not exist as a machine independent of your view in the universe. It exists because you implement it together with others. So how does this emergent higher level agency works? There are some principles for emergent social agencies. One obvious principle for rational agency is goal rationality. Goal, rationality means that you are picking the right behaviors to achieve a goal that you have rationally reason to the right you should not act people that irrational behaviors to achieve your goal, because that is not the right way to relation. You also need to have the correct optimization between exploration and exploitation, which means you need to learn the right amount before you can act. And once you have learned enough, you need to be willing to act on your models, right? It's no good if you spend all your life this preparation for learning. And it's no good if you just jump right into action. So this balance is very important that you need to be willing to act on your models, once you have them. You also need to optimize your internal regular regulation. So your homeostatic self regulation of the organism, you don't overeat, you exercise enough and so on but not excessive, you just optimize your your own agency. And you want to optimize the regulation between agents. So create stable social systems between them that are balanced economically and structurally.

Joscha Bach 59:15
And then, if you these principles basic give rise to an individual rational agent. This is basically this applies to any sociopathic agent that doesn't have transcended agency, but it does not give rise to a society or to some kind of structure where society acts as a single coherent agent to do this to create a single coherent agent formula society out of individual social agents. And in some sense, the same principles will also apply to the AI agents and that exists in the mind and Minsky Society of mind. You need to have principles for the formation of next level agency. And first of all, this first principle is you need to be willing to serve it right you need to have a commitment to serving next level agent is out that it's not going to happen and you need to in To create with other agents around you to do that, so shouldn't happen in the abstract. But it must happen in the practical sense that you actually connect with other agents as well to share resources in the service of producing and maintaining this next level agent. And the third one is, you need to be willing to be the seat if it doesn't exist yet, which means you need to be willing to act as if that thing was existing before it can give you a rewards. Right? So you need to be willing to act in the absence of rewards as a precondition for making this new order happen. If you don't do this, it's not going to emerge, because who's going to start? These principles have been discovered in around 1200 70, by Thomas Aquinas, the famous Catholic philosopher. So what if he wants this principle of world rationality of picking the right action and picking the right goals prudence, the optimization between exploration and exploitation, the willingness to act on your models is correct. The optimization of self regulation is temperance, the optimization of regulation between agents vertical disasters, by dirty power these weird, antiquated Christian words for that, well, he invented them, or they use to describe exactly those principles and our society saturated with them. Right, we think these are abstract values. But they're not abstract values, they address principles that you can discover when you think about how to organize agents. And for the next level agencies, he discovers this principle of the need to submit to the next level agent to the higher level agent, and he calls that phase. And we need to be willing to work with the other agent to make that orders to make that happen, that's love. And we need to be willing to do it before there has been work. This is hope. Hope is in the use of Aquinas different from the hope that we have in our everyday language, it doesn't mean that you think something good will happen. Or you think that something good should happen. It means that you're willing to invest into the future, to make it happen. So I think it's very interesting that the Catholics discovered this and use this as design principles for the religion. So no matter what the tech succeeded or not, it's interesting that philosophers discovered principles that we might be able to use to analyze agency. Now, there's an interesting question, how can we align non human agents with our human interest? This gives rise to the alignment problem. The alignment problem means how can we align artificial agents like AIS and organizations with our human interests. And this is not solved in the general case, we already have this problem that we don't really know how to allied Wall Street has human interests. And Wall Street is basically an emergent agency that is formed in our economy. And that is the activity structuring our society and most people feel also economists often that it's not always in the interest of the optimal organization of our societies. So they struggle, this question of how can we take these institutions that emerge in the financial system is extremely useful, but it's one of the big, modern wonders of the world that we build a system that is able to align resources, so every place in society responds to every place in the world, and have a universal way to measure the value of these resources and so on. And it works everywhere. It's completely ubiquitous. This is an achievement that is even bigger than the internet or the Coronavirus, the other big men made miracles of the modern world. And this is totally fascinating to me that we do not have a good theory about this alignment yet, in which we talk and then when we start discussing alignment that we act as if we start from scratch. So typically, when we discuss alignment between people, I think we should start out with identifying our highest level purposes. And our highest level purposes are the world that we want to create and participate in.

Joscha Bach 1:04:04
And what we need to know to do this is we need our preferences and we extrapolate them. And we see what wealthy get by extrapolating our preferences in the available actions. And it usually means that we have to retrace and so out some of our preferences because they are not simultaneous impossible. And other preferences will turn out to be instrumental to the aesthetics, which means you no longer need to have them explicitly as a valuable integral system because they will automatically follow from the harmonies of the world that you that you want to create. Right. So, when you are completely done this extrapolating your preferences into possible reverse, you pick the one that is the most harmonious one, then you can replace all of your preferences, you can just throw them out, you can you will be able to eliminate all your values, you just throw them out. The only thing that remains is this this model of the harmonic world and all of your actions can be evaluated with respect to how instrumental they are to achieving this harmonic world that you're working on. And so the only thing that's left is to negotiate between different harmonic worlds. Right. So this gives rise to second order problems in which you discuss the outcomes of the negotiations and the world that result from the conflicts and the successes of this negotiation of the boat, the most harmonic boat. And this is I think, this domain is ethics. And I find surprising the data ethics in AI alignment discourse, most of it is just politics. And most of this politics is not talking about aesthetics of the harmonic world that you're working from. It's actually about very simple things about values about or even less than that it's about the Civil list of different political groups that are vying for power, and by which they recognize their own group. I think that's a very dangerous situation to be in. Right. So at the moment, our strategy for AI alignment consists in nerds, creating demons to create pretty pictures for us. And we surround these neurons with a troupe of indefatigable college level thinkers that curse at the demons when they are painting nurses, as women and CEOs as men and so on. Because this is foreshadows difficulties with AI alignment. And it does, right, it's right, there is a problem there. But there's a much deeper problem, the deeper problem that we need to discuss goes far, far beyond that. It requires us to understand what kind of world you want to have. And it's a problem that is not adequately being handled in asking AI engineers to put an ethics chapter into their papers. So I think this needs to be taken much, much more seriously. And it needs to be its own discipline. And it needs to be a discipline that operates respondent methods. And that operates very seriously, because I think that we are very soon going to inhabit a world that is populated by sentient agents that are not made from carbon not made from cells, and that will interact with us. And that will form their own opinions about the aesthetics they prefer. So we will probably need to find ways to align our aesthetic to stem and it might not be that much time left. So I think I stopped here, this is a good point to stop. And I hope we still have some time left for questions. Oh, that's good.

Unknown 1:07:32
Now. Thank you so much for that. That was very fascinating, a lot to digest. And a lot there. I mean, you've obviously thought about these things very deeply. Has anybody got any questions?

Unknown 1:07:48
Can you elaborate a little bit on? Do you have any ideas of what a formal method or formal methods would look like for beneficial AI ethics that are actually functional, functional?

Joscha Bach 1:08:01
No, not from scratch, I suspect that what you first of all, probably create need to create are institutions that allow us to do this, at the moment, what blocks us is a lack of honesty, this respect to understanding our own nature. We are very dishonest I find in the field of AI ethics. And we talk about our own psychology and about the structure of our own society, and the sustainability of our own society. We currently do not live in a society that in its present form of organization seems to be sustainable. And it's not clear yet to us how we are going to resolve this. And without being able to create a space in which you can have that discussion, we will not be able to form models about our future, and how about the coordination between people and AI. And at the moment, I do not see the space because I think that if you would want to try to join academia, to seriously honestly discuss about our own future, and about our own psychology and about our own nature, you are at the moment getting into strong conflict with tribal politics. And from all sides. And this makes it very hard to have that discussion. Basically, we first of all need to create the space again in our society, that we can talk about these questions without screaming at each other without sacrificing the outcome to very short games.

Unknown 1:09:28
It's very movements that you see, getting close to this space, where people are shouting at each other over tribal politics or talking about real ethics, or close as close to it as possible.

Joscha Bach 1:09:40
I think that such forums exist, but they know people which do this, but so far, they have relatively little societal influence. So it's nothing that seems to have a very large effect so far. And so while it's I think it's starting to happen, it's not clear yet how to integrate this with our public discourse.

Unknown 1:10:02
How do you integrate it with with public discourse, if it doesn't involve politics? I don't know that had Henry Wayne public discourse off politics, but

Joscha Bach 1:10:13
I think it's a has probably to do with the top level incentives at the moment, there are bad incentives for the people who want to work in the domain of AI ethics. And it's not the fault of these people. It's basically it has to do with the incentives that are operating under. And basically, people are incentivized for bad behavior. And this bad behavior leads people to not producing optimal results, they're sacrificing tools, to the benefits that they get, or to the constraints that they emerge over the span of different incentives. And, again, this isn't nobody's fault. It's the problem of the structure that we are in that is uncovered. And it may be necessary that he basically started a societal project that is, at aims at our core problem is, which is truthful governance, truthful, effective, efficient governance. And we don't yet know how to achieve this feat agree that we want to have, I think most of us at least, a society that is robust, and that is nonviolent. And it seems to me that this means that it should be a liberal society, because the alternatives to liberal society are ones where you get to power or lose power, by using violence. And liberal society requires that you have a space in which you can have discussion that is oriented on truth, without any set of any form of coercion. If you have not figured out how to do this yet, and we have very big debates about how to do this, and I suspect that social media has to play a very big role in this social media, in some sense, is, is not a separate thing that is new to society, it's just the space in which you have started to coordinate and communicate using the internet. And we have not integrated social media in our society, the governance of social media is currently done by small teams of people that are not being elected, that do not have a principled form of governance that they investigate the code of conduct and moderation of words. But this is not the forum in which you can organize all over society. Basically, social media is not no longer just a small part of society. But it's a significant part of society. It's a significant part of our effective government, about our way in which we form the will the attentions of our society. And so what I suspect is that we also need to think about new ways to organize social media to turn it into a global brain that comes gets to sane results, that allows us to become sentient as a society again, and that helps us to support our form of government. That doesn't mean that we necessarily that you have more voting on social media. And I also suspect that the question of free speech or not free speech is the right question. The question is, how can you organize the discourse in such a way that you do not suppress to this ability of the system to get to true opinions about reality?

Unknown 1:13:21
It's hard to incentivize for that sort of thing. People are very interested in signaling, but not so interested in talking about things that expose themselves, because maybe they believe that if they expose themselves too freely, people use that as ammunition Yes.

Joscha Bach 1:13:40
That's a little bit like saying people like to throw garbage on the streets. And nobody wants to have the street without garbage. But this is actually not true. I think that most people want to have the streets without garbage more than they want to saw garbage on the street. It's just in a world where so many, so garbage on the street, they many of them don't see the point to not do it. And this is the same issue with governance. And the question is how can we have self organizing governance on the internet, if you give people the ability to govern the rules under which they communicate with their peers, with their friends with the people that will actually want to be in relationship and you will probably create societies, sub societies on social media that are attractive to enter by submitting to their worlds. And it's very different from saying, we let social media have fun system of rules. But instead, we allow the communities to invent their own rules, and we trust them in doing that, that the rules that will come out of this will be sometimes beneficial, and also work out in such a way that more people are going to join the more productive and beneficial sub communities.

Unknown 1:14:48
Santa Ana, I think you had your hand up first.

Unknown 1:14:52
Yeah. So you talked about person transfer. It's like me, so I was wondering whether your thought process on perceptrons is related. Marvin Minsky is perceptrons, his computational geometry.

Joscha Bach 1:15:05
The perceptron that he describes is a Rosenblatt idea. And it's not just Rosenblatt idea, the basic principle has been invented and discovered by numerous people. Also, the hours that came later with backpropagation has been reinvented and discovered by several people. And Minsky has done some research in this field. And basically, what he added to the discussion, among other things, is a theory in which he could prove that given learning algorithm was unable to work to learn certain things on the perceptron. So it basically had proven a limit of a certain implementation of the perceptron that says contribution. But the perceptron by itself, and its core idea is simply the neural network, it means you represent the computation as a sequence of sums over real numbers, for some modification, like a special value, or a mapping to a sigmoid, or another regularizer. And it turns out that this is general enough to express arbitrary programs. And the benefit of the neural network instead of using other methods to of expressing programs that will be shorter and more useful is that you have discovered some learning algorithms that can tease this neural network structure into an obvious arbitrary function. And if you don't know how to do this at scale for this other representations that might be more efficient. That's why everybody's using neural networks right now. For almost everybody.

Unknown 1:16:36
Do you think any of the current work on achieving causal learning is going to scale in the near term causal representation learning or these influenced causal influence diagrams that DeepMind are working on which somebody spoke on earlier today,

Joscha Bach 1:16:55
and not completely sure about this, my issue is that I don't quite understand the arguments of the causal reasoning community than they are made in a strong case. The strong argument is basically there's a hierarchy of causative models at the lowest rung of that ladder is the discovery of associations between elements in the data. And this would be what neural networks are doing today. And only that, and on the next level of the causal hierarchy is the ability to intervene, and pedals the causal structure. So you can control the world using the causal model, on the highest level is the creation of counterfactuals and the interaction with counterfactual worlds. And I think this is a very good narrative. But it's simply not true that you will networks cannot do this, you can perfectly well as CPT suite to predict the outcome of putting sugar and cook it off, or salt and cook it off, it's going to tell you whether the cookies are going to be salty or sweet. And it you can also connect a robot to that, that make bake the cookies based on what you want to have. And if you ask it by salt is going to affect the cookies in this way, it's going to give you a causal explanation that involves sodium and restates receptors. And this means that it's possible to extract causal structure from the world given the existing learning models. And conversely, if you look at the models that the causal reasoning community is using, it's mostly directed acyclic graphs that are inadequate in many domains, where there is heavy cross correlation between features. And the as a result, a lot of what the causal reasoning community is currently working with, these are toy models that do not scale to the problems that can be solved by massive large language models and vision models. And so, so far, the problems of the deep learning community don't like deep learning, because it's brutalist and simplistic and simple, inefficient, and so on, have always been solved by using more deep learning, not less. If you take a step back and look at what deep learning actually is, it's differentiable programming. It's a particular way to express things in the program. So, you can often find a solution by Stochastic gradient descent because you represent everything somewhat continuously. But it's not like deep learning people can you can do every single deep learning are religious about this, they are completely fine with mixing in conceptual graph and using propositional representations as a layer and so on. Writing and reading from propositional layers with an end to end learn system. There is no problem was that so I not sure if you need to extend the existing paradigms, even if you should look for different algorithms. I'm much more interested as a woman in trying to find out why the deep learning systems are so incredibly simple and efficient, what can be done about this? So I suspect that the more important things are about attention, how to build better models of attention.

Unknown 1:19:57
Right. I mean, you reference somebody is working on causal representation, learning how it was Yoshua Bengio. Up interested, interesting to get you to in conversation at some stage. Look, there's a few people asking about blockchain and how it can solve this through Dows. I don't know very much about blockchain or dowels. So I'm just

Joscha Bach 1:20:24
but as they say, We don't know what was that for you will be built for us but for those sticks and stones and not quite convinced yet about this ecosystem, it seems to me that the blockchain system ecosystem has been developed entirely in the shadow of cryptocurrencies. It's not independent of cryptocurrencies, if he would outlaw cryptocurrencies, I suspect that the majority of blockchain applications and blockchain companies would disappear. And I don't think that cryptocurrencies by themselves, sustainable cryptocurrencies seem to be the control group to the economy. So, in the same way, as Bitcoin is the control group to Dortch How does a serious cryptocurrency perform compared to one that says on the packet that it's a joke, you can basically look at the stock market and compare it to cryptocurrencies to understand how to real stocks perform compared to something that, obviously is, is a joke. And so to me, the fact that cryptocurrencies performs almost as well as the stock market is very disturbing sign that is very concerning to me about the state of our economy. And so I do think it's very important to study these principles and study these ideas. I also think this idea of having executable contracts is a beautiful idea. But blockchain goes beyond that. Blockchain is not about changing contracts into something that is auto executable. And it also for the most part does not solve problems in this regard in the practical world yet. What I see on the other hand, is that there is a very big danger in such technologies. One is, if we currently look at the financial system, there are bugs in it. For instance, when you follow the robin motor fair, and there was this issue with the game's stop stock, but basically what happened was that the private investors, retail investors has this app discovered back in the day in which the value of the stocks are determined, it's actually an undefined value. And the reason why Orban would have stopped from trading with these things at scale was that some adults higher up in the food chain discovered that there was a danger that the retail investors would set forth the market. And if our financial markets seg faults, that would be a very bad outcome for all of us, right? Because we don't have a second financial market that we can just put in, instead of the first one. So far, we don't know even how to reboot the first one.

Unknown 1:23:15
Right? Segmentation fault, okay. Yes. So that means so

Joscha Bach 1:23:18
basically, it creates a system back that could create a critical error, that brings down most of the functionality of the system. And that would have been super bad if that happens. And imagine that all the decisions in that thing were not be done by people. But dividend by machines based on the hope that everything in the algorithm is correct. And our system would have SEC faulted because some automatic trading hours would have just gone to the end. I think this is one of the biggest dangers of AI in the near term, that somebody is going to build. A financial stock market transformer is the same resources as open AI is froze on Valley. And this thing is going to model the shit out of the stock market and figures out that all the actions on the stock market are produced by less than 7 billion people who are only alive for a very limited amount of seconds and that you can basically figure out what they think in the seconds. And you're not going to do this exhaustively. But you can just reinvest most of your gains on the stock market into collecting more data and buying more compute to get as close as possible. And I think it's going to give the shit out of us before it suffocates all of us, right? This this terrifying to me this financial singularity illustrating singularity and the Dow's and blockchains are basically to me the attempts of children to play with the financial system because the adults are no longer at home.

Unknown 1:24:49
Yes, that was awesome. By the way, Joshua, that was

Joscha Bach 1:24:52
I don't want to infer of it. Anybody who works in the blockchain. I'm sure that there is a lot of work being done. Very interesting. Cool. insets I've listened to people who get JavaScript on the blockchain, so it becomes more democratic and even people who can only do front and can to serve on the blockchain.

Unknown 1:25:11
Yeah, well, you'll be sharing a panel later with Ben, who's working on the blockchain. Yeah. Yeah. He's working on his singularity net stuff. But yeah, that'll be fun.

Joscha Bach 1:25:25
Yeah, I think it's great that Ben has discovered ways to fund his research. But I wish that he was still working on ATI. I think he does. But I wish he had more attention for it.

Unknown 1:25:35
Yeah, that's, yeah. So I mean, he I agree, he does seem to have a very distributed attention. It's great if you can manage it. And I'm sure he can. But yes, some of us fishy. federate his attention on the most important things. But

Joscha Bach 1:25:55
yeah, basically, if somebody else would do the blockchain for him, so he could focus again on the AI. But maybe he finds a way to combine it in ways that I'm not seeing it's probably the case.

Unknown 1:26:06
Yes, you can, you can drill him on that.

Unknown 1:26:10
How I understand is that the whole point of the blockchain is to coordinate the AIS to have a marketplace of AIS that query each other and each other

Joscha Bach 1:26:20
services. But that case, you should just leave the design of the blockchain to the AI because I think that the design of safe and extensible and scalable protocols is a problem. That is it's not very interesting, but heart. So it's really an IoT problem.

Unknown 1:26:37
But you need to bootstrap it, right?

Joscha Bach 1:26:39
Yeah. But you can bootstrap it without a blockchain. I don't see like, why would you need a blockchain to bootstrap AI that invents blockchains?

Unknown 1:26:49
Yeah, that's fair. Actually. System.

Unknown 1:26:53
Yeah. Yeah. Like we had a Stuart Armstrong earlier today talking about his new research. He's left the future of humanity Institute, and it's now heading up Alliance AI, if you didn't know. Yeah, concept, extrapolation is the thing that is working on AI. That's not all that they're going to be working on in the near future. But that's what aligned AI is working on right now.

Joscha Bach 1:27:18
So interesting to reason about the space of aesthetics that you could get. By bio statics probably culminate in Gaia, some coordinated agent at the level of all the organisms together, combined into one big hyper agent, maybe at the planetary level, maybe beyond, right. It's interesting, what happens when we teach the rocks how to think and if that is compatible?

Unknown 1:27:49
Yeah, I'd like singing rocks. I used to put like little eyes on them to make them look sentient when I was a kid and paint them and things like that. But we

Joscha Bach 1:27:59
could also take the opposite stance. I mean, it looks like to me is that Gaia built these trees 100 million years ago, and the guy didn't invent insects yet to take them down again. So many of them fell into swamps, or microorganisms couldn't eat them, and they fossilized. So all this delicious carbon was done from the circulation. And since then, Gaia has been working on a way to reactivate that carbon. And the other solution, we take it out of the ground, put it back into the air, and it can get gets recirculated. And, of course, we will burn ourselves out in the process. But we would have been useful to Gaia. The alternative is the standard story that this was unintentional. And he just destroyed Gaia, because we're so stupid. But I suspect in this case, Gaia had come in because she didn't see us coming as it's obvious that she should have seen this, this would happen and she would have prevented us if she didn't want this from happening. So Lisa has to be angry at guy and he can retaliate by teaching the Vauxhall to think,

Unknown 1:29:03
yeah. We've got a minutes before the next talk is scheduled to stop. So I give people five minutes from this point in time to have a comfort break. And then we'll start the next talk, which is Monica Anderson. And Monique is going to be speaking on the red pill of machine learning. So stick her out or take that blue pill and go away. Now he should join us and take the red pill.

Joscha Bach 1:29:36
Right solution is always to take two red pills, one blue, one red to blue, and then one rhetoric and then one based reality everything else gets you on the wrong one.

Unknown 1:29:46
Right. I'm gonna have to trust you on that one. I haven't worked it out. Let me think about that. I have to do some more compute. I think I think I went through a segment Integration falls. To that to have it maybe a purple pill
