Joscha Bach 0:00
I think under certain circumstances, being conscious is an important part of a mind. It's a model of a model of a model. Basically, what it means is our mind, our neocortex produces this dream that we take to be the world based on the sensory data. So it's basically a hallucination that predicts what next hits you already. Now, that's the world out there, we don't know what this is, the universe is some kind of weird pattern generator is some quantum properties. And this pattern generator throws pattern, it's at patterns at us. And we try to find regularity in them. And the hidden layers of this neural network amount to latent variables that are colors, people, sounds, ideas, and so on. And this is the world that at least subjectively inhabit, that's the world that we find meaningful. It's a model of the world out there, then we have the model of the self. That's basically the model of how we interact with this model. who we are as a system, and this self model is gets modeled again. So we basically model which properties we know about this to be in a mental state does mean that we know that we are in that mental state and needs to be a part of our brain needs to be tasked to find out what what the brain is currently doing. And that model is is not accurate, usually, but it's helpful to some degree. And so this model of a model of a model relates to our consciousness. And the part that is interesting to us is different functionality that is relevant for self reporting, and important to others about the presence or change or absence of phenomenal content. Phenomenal content is the stuff that gets generated. If you notice its presence and absence and change, then we need to pay attention to it. And consciousness is largely a protocol of our attentional processes. And this attention is required, like when you are conducting an orchestra, you pay attention to stuff that the orchestra cannot do well. That's what you filter out. You tell these instruments, you're not playing well together, you have a conflict here or you tell this instrument you have the wrong note, you should or you should be louder, or you should not play this at all, because you're playing something else tonight. This is the role of this conductor. So it gets active whenever there's a conflict, something that requires attention. When you don't need attention for something you're not conscious of it, you don't remember that it happened, you only remember those things that require your attention. Which is why when we get older, we remember less than less of the previous day, before fewer new representations at integrate fewer things into our mental protocols. And when AI becomes conscious, if it ever will be you superhuman, which is not clear when that happens. And what we need to do before that happens, it might become extremely conscious, much more than we have many more things because it's mine is going to be more scalable and larger, but only for a brief amount of time, because at some point, it will have solved all its problems. At this point, everything will become subconscious, it will be able to do everything on autopilot. I think you are concerned about others, it's not so much dependent on your perception of the universe, but mostly of your needs with respect to others. When you are a sociopath, you probably don't care that much about others, even if you see color in the same way as they do. I think that marry the colorblind neuroscientist is an amazing example to understand the difference between art and science. If you do science like this neuroscientist Mary, you try to make a model of how things work. And making a model means that you understand it, you map it onto something that you know how it works. And if you fully understand it, you map it onto a computation on to a computer program on them some lowest foundational level that progresses in the system between states and you know, this transition function by which it progresses. If you can map something to a hierarchy of concepts on this, you probably understand it fully because it means eventually you can write a computer program that reproduces that phenomenon, if you had a computer that is large and fast enough that is and if you understand something partially if you have a partial model of something and you can map something else on it, you understand this other thing as well as you understand the original thing that you understood, but usually what we mean by understanding we map it onto a model that we already have, thereby creating new model be inspired and existing model. And what Mary as the permanent fundamental experience does is more than like what you do when you do art, and you do science, you try to capture truth. And truth is the shape of the space of all possible theories that can explain the observations.

Joscha Bach 4:32
Normally, we are fine when viewed as at least one model. But it's always infinitely many models. If this if the problem is sufficiently interesting, that can explain what's going on in the try to find usually a few of these models to basically get a feel for the shape of that space of the possible theories that can explain it. And when you are an artist you're not interested in truth, you're not interested in the shape of that subspace, what you're interested in is capturing your point. just state the feeling of what it's like this is I think what art is about. And Mary as the experiencer is changing her own brain state into one that is in a state of phenomenon experiencing it. So, the scientist has to model a domain to understand how it works, the art experience or or the experience or in general of the color red has to change the modeling system their mind into something that works similarly, to some other mind. Wisdom, you want to share an experience. And this also means that you have to have a mind that at some level is isomorphic. To the other mind that you are communicating with, when you see art as a communicative exercise, to this feeling of what it's like means you have to have a mind that to a large degree is very similar to the other mind. That's why this discussion you don't know, Thomas Nagel, what it's like to be a bet is similar, you can know what it's like to be bad in the sense that you can make a complete model maybe of the bad brain and predict what the bad is doing, under what circumstances and understand why and how the bad reacts in every situation. But to know what it's like to be a bad, it means that you have to have a brain that is so isomorphic to a bad brain, that you can undergo similar exponential states as the bad. I find theories that make you feel good, very suspicious. If there is something that is like my preferred outcome for emotional reasons, I should be realizing that I have a confirmation bias towards this. And truth is a very brutal vector. If you have something that pulls you away from truth, it's unlikely that you get to truth. So I think what you should be doing is you totally ignored the dimension of, of how you feel about the theory. It's not that you invert it, you ignore it, it doesn't matter. Not in the positive, not in the negative, it's completely irrelevant. That is you like the outcome of your thoughts. Otherwise, you cannot get the tools you need to be able to disentangle yourself from how you feel about a result. For me, as a human being, the former one is, of course, boring to be in. It's a mathematical entity in some sense, and the universe is such an entity. This other thing comes only in so taking a certain perspective on it, that you care about something is this evolutionary shortcut that we have that makes life so interesting for us, the optimal solution will probably not care. Like for instance, if you are a mother and you care about your child, you have this motherly love. That's a proxy for having offspring and optimally caring for their offspring. But it's not the same thing because you spent many nights laying awake and being concerned even though you shouldn't, you might die of grief and your child dies when it comes to harm. So all these things are evolutionary, probably maladaptive. They are the result of nature trying to get the calibration right of how you should be concerned, because nature hadn't found a way to directly imprint our brain is a function that would optimally let us procreate and generate offspring, right. But if you have a system that is completely rational, soulless, rational agent, that only focuses on its objective function. Such a system is a feature of the environment. For us, it's boring, what we see as the soul in humans in each other, that's exactly the difference between the soul that's rational agent performing optimally on the utility function, and what you are based on your human drives, and cultural influences, and so on. And this is what makes us interesting for each other, like why I have are important. So I wouldn't equate having these human things to qualia, because you don't need to care about qualia. qualia is just the phenomenal experience of generative representation in your mind. So it means you do have this world simulation with this aspect of a world simulation like I see your face right now and to see the emotional expression that you have on your face to put and post on it, and so on. And that's the state of my emotion more than that my brain makes from you. And this gets integrated. My protocol manager, I try to remember what they do address experience. My brain regenerates this, I see your facial expression in my mind's eye. See your facial expression in my mind's eye, and it is related to qualia.

Joscha Bach 9:15
I think the fact that I care about this is orthogonal to this, it's irrelevant in the sense that otherwise my brain would not spend attentional resources on it, because that's the way my brain operates. It's the reptilian brain that in some sense, gives these motivational urges to say, oh, I should care about Adams emotional state, because I like him and so on and so on. Right. But if you are a perfect utility Maximizer you might still have qualia, if you have a similarly structured brain of a similar complexity. But you probably wouldn't care. I'm the same way as we do. We just produce the behavior that you need is also the thing that you are addicted to insight. And that's probably defective. I mean, I have the same addiction. This addictive addiction to Insight is so One thing that you should not have, if you were caught totally aligned with your evolution, your prerogatives, and it turns out, but most people have this to some degree, but not to the extreme degree that you and me have, because they are optimizing for their social and economic success instead of wasting their time finding more insight, right. And I suspect that this insight is basically insight addiction is something like a mental parasite that we get as a side effect of our brains that learn natural language and therefore need to look for abstract mathematical structure. grammatical structure is something that we cannot directly observe, it's a mathematical feature. To learn more mathematical structure, I think we need to have an a drive, basically an urgent built into our mind, that gets less reward signals when we find interesting compositional structure. And this makes us susceptible to aesthetics, also to the aesthetics of information. Well, qualia might be important for minds like ours, or for minds of a certain complexity that want to have these generative simulations and put them into their protocols. But I don't think that caring about them will be that important to have pleasure. And to have pain will not be that important. Because you have pleasure and pain only when you need heuristics about something that are rooted in your biological substrate to some degree, because you don't have a rational model of it. Imagine that you come into a job that you want to be good at, but you don't have any positive or negative emotion about it, do this job to earn money, you want to become competent in it, you're not scared that he's not don't succeed, you don't have a very big desire to be super good at it. Because you only do this to earn money, that you can be totally competent in that job, right? If you can focus on this task, and if you look at all these things that you're doing this job Congestus down, delivers it rationally. And ideally, this is the way that utility maximizing would look at the thing that the utility Maximizer locates exactly those resources now that correspond with the reward that you get from exerting them. And the didn't have a way to in our evolutionary history, to assess utilities properly. And to even know that we would need to do this. So instead, we have these heuristics that are encoded in our brains as fears and desires and so on and interest. And this is probably not something that is going to be evolutionary optimal. Hypothetically, if you want to build an AI that solves the same task as VI in an optimal fashion.
