Joscha Bach 0:03
get this to work. Amazing. Okay, I think the last, or the best contribution of the last century to the sciences was the discovery of the concept of computation. Computation treats arbitrary systems as sets of discernible differences. The fancy name for discernible differences in science is information. And we can group these discernible differences in states and then can describe an arbitrary systems by how these states change. And the way they change is usually described for some kind of transition function. And the description of a system using a transition function and states is the most general way to capture the notion of computation. Digital computation is a very special case, and literally computation, the states are discrete. And the rules for the state change are deterministic or probabilistic. We can use this language of description to describe any system that presents itself to an observer as a set of discernible differences, for instance, the universe, or the observer itself. So if we look at these different computers, what kind of computer would we need to run the universe? At the moment, you don't know that. But imagine I describe the universe as a set of assembled differences, observations that I can make and how these change into the next state. And I would want to build a computer that can run our universe and I want to buy the cheapest computer that I can get the Weber's, what would I need to look at an order, the cheapest one I could buy is probably something like a very big computer like this one was much, much more memory, it's a finite state machine discrete and so on. Way more memory speed doesn't matter if you're inside of it, of course, because you won't notice if it's slow or fast when you are inside, it could be probabilistic, which means you can branch between the states with some probability function. Or it could be a quantum computer in which each state can be a superposition of states. Or maybe this is not enough. And maybe we need something like the one of the three versions that I just mentioned, with infinite amounts of memory, this is going to be infinitely more expensive. But it could get even worse, if it has infinite memory. And number one, that's your Turing machine. As we know it from these usual definitions of computation. It could also be a geometric hyper computer, which is what physicists believe for the longest time that is that the universe is something that is continuous and is able to perform continuous operations, it's something that you cannot do with a digital computer can only approximate it with a digital one. Or it could be in a causal hyper computer, one where you can take information and calculate it and send it back in time and use these observations which break causality in a closed timelike loop. That's also a computer, it's just one that is not, cannot be simulated on a discrete state machine. So compute computation is the description of an arbitrary system as states and the transition function. And computational ism is the notion that computation is necessary and sufficient for describing all conceivable observables including, of course, consciousness. By the way, number one, and two are the only ones that we know how to build in this universe. And one of the speakers after me hardmode is going to talk about that he's working on building number three. And one of the speakers after me James is going to talk about that the universe is possibly in a causal hyper computer. Rogers thesis is more radical, he says that there's something going on the universe possibly that is not computational, that is not cannot be described as states and transitions between states, something that is not afforded by current physics. So we would need to go beyond physics. And something that boggles the mind, because it's very hard to imagine what that could be. What is not computational? Well, there is stuff that is not computational, for instance, most of mathematics, how can that be, but mathematics is not computation. Mathematics is the domain of all formal languages. It's the domain of specification languages, you can specify arbitrary things, most of which won't work, right? And language, you can say stuff that doesn't pan out. And most of mathematics is not computable. For us, computation is the domain of possible implementations. So it computation itself can never run into a paradox or be impossible or anything like this computation is just what it is a state transition function, then comes the next state transition function comes the next state. That's it. So if we try to describe the universe we do this with physics, if we try to describe minds as information processing systems, you can do this for instance, using artificial intelligence. This was largely why this field was started in the 1950s after people realized that psychology isn't going anywhere. How can AI reveal the nature of our minds?

Joscha Bach 5:05
Current AI, probably not. Current AI is in a strong movement, which is called deep learning. It's super successful and ugly we started pretty much, five years ago, five years ago, a group of engineers led by Andrew Yang, from Stanford and Google built, you will network with nine layers on 16,000 computer cores. And they showed it randomly selected frames from YouTube 10 million of these frames, so order of magnitude more than a baby is going to see in the first month of its life. And after that, it was able to recognize the staff from an image database, this 15.8% recognition accuracy, which means you could show it an arbitrary picture, and it could tell you in 16% of the cases, what's on that picture, and it hadn't been trained to know what it was seeing on these YouTube frames, it was just looking for structure for anything that makes it a suitable structure in there. And it got internet famous because one of these things it could recognize very well was cats, which is not surprising when you remember that it was trained on YouTube frames. So deep learning is basically using stacked hierarchies of layers, and which are feature detectors. And that uses a method largely gradient descent to approximate functions that allows us to classify objects and learn policies. And this means that in 2015, these things outperformed humans in recognizing images in that database that didn't stay at 16% Really outperform people at doing this. They also got better at that, than people are playing PacMan being trained from scratch or not being told what to do. In last year, they outperform people at the game of Go. And maybe 2020, we will have the first self driving cars, at the moment, they are already outperforming the average humans in driving a car, which we for some reason, don't think is good enough because they sometimes make accidents. But on average, they make fewer accidents than people. And there is an argument to be made. If it's really about saving lives, we should switch right now. Of course, this is very far from a mind when you think about building artificial minds, we often think about the Turing test a system that convinces people that it's intelligent. I don't think that is the actual challenge. The actual challenge is a system that performs a Turing test on us. Right, it's what we do with each other, we talk to each other to find out what the other person is conscious of. Right? We don't you don't try to convince others just that I am intelligent. I already know that. I want to find out what you know, I want to find out what you think about what you're aware of. And once the system does this with us, we can recognize that this system has an idea what it is that it starts to become a mind that recognizes us as minds that it makes models of the world in which minds figure prominently. How long does it take to get there? We don't really know Marvin Minsky said, if I believed in realism, if we work really hard, we will get there in between four to 400 years. And we are pretty much on track, I think. But we could say that IT systems that we need to build cognitive AI is different from the narrow eyes that we have right now, the narrow is that we have largely pattern recognizers and object classifiers. So for instance, if you want to get such a system to recognize Alyssa one of the dancers and that image, it means that the system has to filter out everything in the image that is not another variant to find the invariant, the invariant would be would be Elesa. So it treats to filter out the posts, the lighting, the dress, the facial expression, and so on, until only Alyssa is left and all the images that contain her. But this is not quite what people do. What people do is they take all these different concepts, and don't filter them out, there is no background, there's only partially occluded other objects and images, right? What what we do is we create a whole world from what we look at. And we relate the features of that whole world with each other. And we do this starting from patterns, then we go to percepts. And then we go to mental simulations. And then we go to concepts which are abstractions over these mental simulations and then to linguistic symbols that allow us to synchronize our conceptual domains and to some degree, our mental simulations. So minds are not classifiers. They are simulators and experiences. And they start out this feedback loops because they are in the service of control of the needs of a social primate. Most of the feedback loops that we have are not they're available to our conscious interaction with them because they don't need to, for instance, regulate our body temperature, our heart rate, our breathing patterns and so on. Autonomous leave is a lot of feedback loops, many of which are in the brainstem. But for some of the things we need to be able to control what we are doing and adapt our actions. And for that we have pleasure and pain. They are residing in the limbic system largely and pain tells us please do less of what you're currently doing.

Joscha Bach 10:05
Pleasure tells us do more of what you're currently doing. And then we have impulses, and they are directed on future pleasure and pain. And we have a whole motivational system that gives us impulses, many of which are physiological, some are social, and handful of cognitive. My own work is related with discovering these motivations and building computational models of them. To associate these motivations with situations in the world. Many animals have a structure that is called the hippocampus, that mattress patterns and the environment with the needs of that organism. So it knows what impulses should lead to which world situations in which world situations in to try to get to. But of course, it's very difficult to make sense of the world if it's just a bunch of patterns. So we have something more than in the hippocampus, we have the neocortex. And the neocortex allows us to generalize over situations to say for instance, what do all the restaurants have in common? How can I recognize all the situations where I can get food and so on? Or how can I recognize Elissa in many, many situations, and to do this, our brain needs to do something very tricky. It needs to generalize. And a good metaphor to understand what it's doing is a synthesizer. If you've ever played around with this and decides that has a bunch of knobs and buttons, and you can twiddle these until the synthesizer makes exactly the sound you want. Now imagine you take that synthesizer and you connect it to the output of your cochlea. This is the organ in your ear that basically performs a hardware Fourier transform on sound waves. And this is then piped to the cortex. And the cortex takes these signals, and it fiddles, knobs, and buttons until it's able to predict what signals are going to come next from the cochlea, which means it understands the current sound. And once it has done this for a number of sounds, it generalizes over those sounds, and looks, what do all these sounds have in common. So for instance, all these different sounds have one property and that makes them different, and it's the pitch. If he wants you to filter out the Patriot recognize, you can now take a whole class of different sounds and lump them into one into one function that makes it predictable. And then once you discovered the nature of pitch and the different sounds that you can generate, you find another feature, for instance, loudness. And once you've done this, you get to the next level, and you can build something like a sequencer or something that is able to assemble sounds into rooms, and then you go a level above that. And you can build harmonies and melodies and so on, you go level above that, you can build songs, and you go level above that you can build musical styles, and so on. And then you can go across the modalities and do this for different modalities. You do the same things for color for spatial frequencies and so on. You take your synthesizers, plug them into high level synthesizers. And you do this until you can create a whole mental simulation, a dream, because this is what the world is to us. It's a dream. We don't live in the world out there. We live in the dream in a simulation generated by our neocortex, the same circuitry that will choose his dreams at night, but use his dreams during the day, which gets loosely synchronized. Because our sensory input, right? I think most of us understand that, that we don't live in the world out there. The interesting thing to realize is, there is no colors in the world or there are no sounds no people, there is a weird quantum graph out there probably, we don't know what it is, but colors, sounds, people and so on exists in our neocortex as mental representations that allow us to predict the next set of bits that is going to hit our retina. And above these mental simulations, we create concepts as abstractions over what elements in the simulations have in common select the address space of our sensory motor scripts. And then on top of those, we can create linguistic symbols to talk about them and self report and so on control our self reports. For this, we need different types of learning. One is function approximation, which is Bayesian neural parallelizable. And it's exhaustive. So we use this, for instance, to learn the laws of perspective in the first month of our life. And then we have scripts and schemas, which are narratives, that stories that we use to describe, for instance, what happens in a restaurant or what happens on the financial markets or in society or in politics. And this is largely what we are talking about talking to each other, right. And we can use these conjunctions of those to direct our mental simulations.

Joscha Bach 14:13
So this is what the neocortex is doing. It's simulating a dynamic world. And at the moment, we don't have all the elements for that in place and our machine learning models. I think the basic units, probably not a neuron, because it needs to be compositional, it needs to be able to read arranged in different things. The basic unit is probably a cortical column, we have something on the order of 100 million of those. Each of them is probably a state machine made from something like 100 to four neurons, which has a function approximator and combines it with different learning modes and different ways. It can link up to its environment. And these cortical columns are linked up into cortical areas. And these cortical areas are linked with each other into processing streams and form these hierarchies of synthesizers, so to speak, and together. They are something like an orchestra we have something like roughly 50 brain areas that are made II From an order of 100 million of these units, and they talk to the neighborhood, and therefore I'm processing streams, right, and starts out from sensory patterns and motor patterns and gets integrated into more and more abstract things. And there's also a conductor. And the conductor is not the homunculus. The conductor is not something that has super powers, it's something it's a brain area like others, in the same way as a conductor and orchestra is not a Superman, but it's, he's similar to the musicians, he just has a different task. And the task of the conductor is very similar to the conductor and orchestra, its task is to resolve conflicts and tell individual instruments to deliberately loud orbital laser or to make something slightly different, or to listen more to this guy to his other side and so on. And conductor is in this way, controlling what's being played tonight, if you take the conductor of a the person becomes a sleepwalker, there's people can still do interesting things like get up from bed, make dinner, open doors, walk on the street, answers questions in random ways, but there's nobody home, nobody is able to integrate this motor with the motivation of a consistent agent and create a consistent protocol of what's happening and learn based on that protocol. Right. So this is what this conductor is doing. It gives you potential advantages and integrates experiences. And the brain is probably the it's all done largely by the dorsolateral, prefrontal cortex, and some neighboring areas that provide infrastructure for that. But important thing is to realize, of course, you're not your brain, you're a story that your brain tells itself, largely, so the protocol of the conductor. So you have the sensory perception, you build distributed hierarchical representations from them, which are your mental simulations. And then you have your conductor that is able in the secondary process, to scan part of these mental representations basically see what your brain has decoded from the world. And most of this stuff is doing done completely subconsciously, autonomously, in parallel on the background by your brain, your conductor is really paying attention to only very few things there. And well, actually not in real time. And on your model, you have your mental state, world model, self model, procedural memory, object, memory, and so on. So all this is part of your virtual presentation, including a model of yourself of how you interact with the world. And the model of that model, which we usually call the self. Your conductor also keeps a protocol of what it attended to in this protocol is the only place where experiences integrated is something like an equivalent of experiences in many parts of your brain and distributed fashions. But these different parts don't know about each other. And as they talk to each other. And when you serve, report or report to other people, you need to have an integrated place where this information comes from this exactly that protocol of what you attended to you can only remember of what you what you attended to when put in that protocol. Right. You can also then use this protocol to get activation into it to recreate a mental simulation or partial mental simulation that is very similar to the simulation that happened when you looked at a particular thing or were in a particular situation. So you can use this to recreate situations, you can recreate memories, you can recreate particular dreams, and learn from them. And they're sufficiently similar to the situation that you had and then situation that you can learn from them. And then you can also put the fact that you accessed that protocol into your protocol, which means it now becomes somewhat recursive doesn't mean that you are truly recursive, you just become aware of the fact that you did access your protocol. And this created a particular mental state inside of you. So basically, you take the model of your own recollection that you model at some point in your mind, because it's part of your mental landscape, and put that into protocol itself. So this conductor maintains an attention protocol, which is a narrative about yourself, and it has access to the protocol which gets integrated into the protocol itself. I think that remembering having been conscious of something is both necessary and sufficient for consciousness. That is, I don't need to be actually conscious of something it's necessary that I remember having been conscious of something so I can report about it to myself and to others.

Joscha Bach 19:04
The availability of my mental simulations to my conductor is necessary and sufficient, I think, for Access Consciousness, and the integration of the access into the accessible protocol as necessary and sufficient for myself report about the awareness. And the self report in turn about the My memory of awareness is interpreted as my awareness and actuality. What does it mean? It means for instance, if you are drinking a cup of coffee, and you feel how the coffee goes down your throat, this is of course not what's actually happening. What's actually happening is that you have some sensory neurons and your throat, and your larynx and so on that gets stimulated by the coffee. And some of that information gets integrated and transmitted to your brain. And then it gives rise to a simulation in your somatosensory cortex, which happens, something between I think 700 milliseconds on one a half seconds after the fact and is totally different from what physically happened with the coffee and you throat, right? But you perceive it at the same time as which you subjectively take the cup and compare it to your mouth and you hear the clinking of the cup to your teeth, and so on. All these things happen simultaneously, even though the processing for the of the sound took a very different amount of time and the processing of your movement took a very different amount of time, because they're very different sensory modalities with different parts and mental representations. How's it possible that it's objectively happened at the same time, because they are patched together and the protocol and you only get access to them long after the fact. They're patched together and the protocol evade which they didn't happen together. Which means you're not conscious in in the right here, right now, when you drink that coffee, you're not aware of that coffee while you drink it. You're only aware of it a long time later invalid when you actually navigate the cup. You do this with reactive subsystems that have been primed by previous conscious interactions, right? So I think that you're probably not conscious and actuality of things like that. You're probably having a memory of having conscious of something of experiencing something that didn't take place in exactly the same way. As you remember it. That's the easiest explanation of how to make a machine conscious is to give it a memory of angry be conscious of something. That's it from me

This transcript was generated by https://otter.ai